% Copyright (c) 2017, Gabriel Hjort Blindell <ghb@kth.se>
%
% This work is licensed under a Creative Commons 4.0 International License (see
% LICENSE file or visit <http://creativecommons.org/licenses/by/4.0/> for a copy
% of the license).

\chapter{Macro Expansion}
\labelAppendix{macro-expansion}

\todo{write outline}

This appendix is based on material presented in
\cite[Chap.\thinspace2]{HjortBlindell:2016:Survey} that has been adapted for
this dissertation.

The selection of techniques in this appendix includes all those discussed in
earlier surveys by \textcite{Cattell:1977} and
\textcite{GanapathiEtAl:1982:Survey}.
%
In the latter, this \gls{principle} is called \gls!{interpretative.cg}[
  \gls{code generation}].
%
Several of these techniques are also discussed in depth by
\textcite{Lunell:1983}.


\section{The Principle}

\begin{inParFigure}{54mm}[r]
  \centering%
  \input{figures/macro-expansion/principle-example}%

  % LAYOUT FIX:
  % Allow for some more space underneath the figure
  \vspace*{.5\baselineskip}%
\end{inParFigure}%
\noindent%
The first \gls{principle} to emerge was \gls!{macro expansion}, with
applications starting to appear in the 1960s.
%
In \gls{macro expansion}, the \glspl{instruction} are expressed as
\glspl!{macro} which consist of two parts: a \gls!{template} to be matched over
the \gls{function}, and an \gls!{expand procedure} to be executed upon the part
of the \gls{function} that was matched (see \refFigure{macro-example} on
\refPageOfFigure{macro-example} for an example).
%
A \gls!{macro expander} traverses the \gls{function} under compilation and
attempts one \gls{macro} after another, typically in the order they are declared
in the \gls{machine description}.
%
Upon a \gls{match} it executes the corresponding \gls{expand procedure} and then
resumes the traversal with the next, unmatched part until the entire
\gls{function} has been expanded.
%
Consequently, \gls{matching} and \gls{selection} is combined into a single task
as the first \gls{macro} matched is the \gls{macro} to be selected.

The main benefit of \gls{macro expansion} is that it is intuitive and
straightforward to apply.
%
Because the \gls{macro expander} is implemented separately from the
\glspl{macro}, the former can be kept generic and simple while the latter can be
made as customized as needed for the \gls{target machine}.
%
This also allows the \gls{macro expander} to be void of any \glsshort{target
  machine}-specific details, thus necessiting only the \glspl{macro} to be
rewritten when retargeting the \gls{compiler} to another machine.
%
To this end, the \glspl{macro} are typically written in some dedicated language
in order to simplify this task by raising the level of abstraction.


\section{Naive Macro Expansion}

\subsection{Early Applications}

We will refer to \glspl{instruction selector} that directly apply the
\gls{principle} just described as \gls!{naive.me}[ \glspl{macro expander}], for
reasons that will soon become apparent.
%
In the first such implementations, the \glspl{macro} were either written by hand
-- like in the \gls{Pascal} \gls{compiler} developed by
\citeauthor{AmmannEtAl:1974}~\cite{AmmannEtAl:1974, AmmannEtAl:1977} -- or
generated automatically from a \gls{machine description}, which was typically
written in some dedicated language.
%
Consequently, many such languages and related tools have appeared -- and then
disappeared -- over the years (see for example \cite{Brown:1969} for an early
survey).

One such example is \gls{Simcmp}, a \gls{macro expander} developed in 1969 by
\textcite{OrgassWaite:1969}.
%
Designed to facilitate \gls{bootstrapping},\!%
%
\footnote{%
  \Gls!{bootstrapping} is the process of writing a \gls{compiler} in the
  programming language it is intended to compile.%
}
%
\gls{Simcmp} read its input line by line, compared the line against the
\glspl{template} of the available \glspl{macro} (see \refFigure{simcmp-example}
for an example), and then executed the first macro that matched.

\begin{filecontents*}{simcmp-example-macro.c}
* = CAR.*.
    I = CDR('21)
    CDR('11) = CAR(I).
.X
\end{filecontents*}
%
\begin{filecontents*}{simcmp-example-input.c}
A = CAR B.
\end{filecontents*}
%
\begin{filecontents*}{simcmp-example-result.c}
I = CDR(38)
CDR(36) = CAR(I)
\end{filecontents*}
%
\begin{figure}[b]
  \figureFont\figureFontSize%
  \centering%

  \subcaptionbox{%
                  A macro definition%
                  \labelFigure{sicmp-example-macro}%
                }%
                [40mm]%
                {%
                  \lstinputlisting{simcmp-example-macro.c}
                }%
  \hfill%
  \subcaptionbox{%
                  String that matches the template%
                  \labelFigure{sicmp-example-input}%
                }%
                [32mm]%
                {%
                  \lstinputlisting{simcmp-example-input.c}
                }%
  \hfill%
  \subcaptionbox{%
                  After macro expansion%
                  \labelFigure{sicmp-example-result}%
                }%
                [36mm]%
                {%
                  \lstinputlisting{simcmp-example-result.c}
                }

  \caption[Example of macro expansion using \glsentrytext{Simcmp}]%
          {%
            Example of macro expansion using \glsentrytext{Simcmp}
            \cite{OrgassWaite:1969}%
          }
  \labelFigure{simcmp-example}
\end{figure}

Another example is the \gls{GCL}, developed by \textcite{ElsonRake:1970}, which
was used in a \gls{PL/1} \gls{compiler} for generating \gls{assembly code} from
\glspl!{AST}, which are \gls{graph}-based representations of the source code
that are always shaped like \glspl{tree}.
%
The most important feature of these \glspl{tree} is that only a syntactically
valid \gls{program} can be transformed into an~\gls{AST}, which simplifies the
task of the \gls{instruction selector}. However, the basic \gls{principle} of
\gls{macro expansion} remains the same.


\subsubsection{Using IR Instead of ASTs}

\glsreset{IR}

Performing \gls{instruction selection} directly on the source code, either in
its textual form or on the \gls{AST}, carries the disadvantage of tightly
coupling the \gls{backend} to a particular programming language.
%
Most \gls{compiler} infrastructures therefore rely on some lower-level,
machine-independent \gls!{IR} which isolates the subsequent
\glsshort{target machine}-independent optimizations and the \gls{backend} from
the details of the programming language.
%
The \gls{IR} code is often represented as an \gls!{expression tree}, which is a
\gls{tree}-shaped \gls{data-flow graph} (see
\refFigure{expression-tree-example}).
%
\begin{filecontents*}{ir-code-example.c}
$\irTemp{t}$ = a + b
c = $\irTemp{1}$ * 2
\end{filecontents*}
%
\begin{figure}[b]%
  \centering%

  \mbox{}%
  \hfill%
  \subcaptionbox{%
                  IR code%
                  \labelFigure{expression-tree-example-ir-code}%
                }{%
                  \lstinputlisting[mathescape]{ir-code-example.c}
                }%
  \hfill%
  \subcaptionbox{%
                  Expression tree%
                  \labelFigure{expression-tree-example-tree}%
                }{%
                  \input{figures/macro-expansion/expression-tree-example}%
                }%
  \hfill%
  \mbox{}

  \caption{Example of an expression tree}
  \labelFigure{expression-tree-example}
\end{figure}
%
It is common to omit any intermediate variables from the \gls{expression tree}
and only keep those signifying the input and output values of the expression, as
shown in the example.
%
This also means that an \gls{expression tree} can only represent a set of
computations performed within the same \gls{block}, which thus may contain more
than one \gls{expression tree}.
%
Since these representations only capture data flow, the \gls{function}'s control
flow is represented separately as a \gls{control-flow graph}.

One of the first \gls{IR}-based schemes was developed by \textcite{Wilcox:1971}.
%
Implemented in a \gls{PL/C} \gls{compiler}, the \gls{AST} is first transformed
into machine-independent code consisting of \glspl!{SLM instruction}.
%
The \gls{instruction selector} then maps each \gls{SLM instruction} into one or
more target-specific \glspl{instruction} using \glspl{macro} defined in a
language called \gls!{ICL} (see \refFigure{icl-example} for an example).
%
%\begin{figure}[t]%
%  \newcommand{\commentize}[1]{\textit{#1}}
%  \centering%
%
%  \begin{minipage}{9.9cm}
%    \lstset{escapechar=|}
%    \begin{plainCode}
%ADDB  BR   A,ADDB1      |\commentize{If A is in a register, jump to ADDB1}|
%      BR   B,ADDB2      |\commentize{If B is in a register, jump to ADDB2}|
%      LGPR A            |\commentize{Generate code to load A into register}|
%
%ADDB1 BR  B,ADDB3       |\commentize{If B is in a register, jump to ADDB3}|
%      GRX A,A,B         |\commentize{Generate A+B}|
%      B   ADDB4         |\commentize{Merge}|
%
%ADDB3 GRR  AR,A,B       |\commentize{Generate A+B}|
%ADDB4 FREE B            |\commentize{Release resources assigned to B}|
%ADDB5 POP  1            |\commentize{Remove B descriptor from stack}|
%      EXIT
%
%ADDB2 GRI  A,B,A        |\commentize{Generate A+B}|
%      FREE A            |\commentize{Release resources assigned to A}|
%      SET  A,B          |\commentize{A now designates result location}|
%      B    ADDB5        |\commentize{Merge}|
%    \end{plainCode}
%  \end{minipage}
%
%  \figCaption{A binary addition macro in \glsentrytext{ICL}}[Wilcox1971]
%  \labelFigure{icl-example}
%
In practice, these \glspl{macro} turned out to be tedious and difficult to
write.
%
Many details, such as addressing modes and data locations, had to be dealt with
manually from within the \glspl{macro}.
%
In the case of \gls{ICL}, the macro writer also had to keep track of which
variables were part of the final \gls{assembly code}, and which variables were
auxiliary and only used to aid the \gls{code generation} process.
%
In an attempt to simplify this task, \textcite{Young:1974} proposed (but never
implemented) a higher-level language called \gls!{TEL} that would abstract away
some of the implementation-oriented details.
%
The idea was to first express the \glspl{macro} as \gls{TEL}~code and then to
automatically generate the lower-level \gls{ICL}~\glspl{macro} from the
\gls{machine description}.


%\subsection{Generating the Macros from a Machine Description}
%\labelSection{separating-macros-and-machine-description}
%
%As with \citeauthor{Wilcox1971}'s design, many of the early
%\gls{macro}-expanding \glspl{instruction selector} depended on \glspl{macro}
%that were intricate and difficult to write. In addition, many \gls{compiler}
%developers often incorporated \gls{register allocation} into these
%\glspl{macro}, which further exacerbated the problem. For example, if the
%\gls{target machine} exhibits multiple \glspl{register class} and has special
%\glspl{instruction} to move data from one \gls{register class} to another, a
%record must be kept of which \gls{program} values reside in which
%\glspl{register}. Then, depending on the \gls{register} assignment, the
%\gls{instruction selector} needs to emit the appropriate data-transfer
%\glspl{instruction} in addition to the rest of the \gls{assembly code}. Due to
%the exponential number of possible situations, the complexity that the macro
%designer has to manage can be immense.
%
%
%\subsubsection{Automatically Inferring the Necessary Data Transfers}
%
%The first attempt to address this problem was made by \textcite{Miller1971}. In
%his master's thesis from~1971, \citeauthor{Miller1971} introduces a \gls{code
%  generation} system called \gls{DMACS} that automatically infers the necessary
%data transfers between memory and different \glspl{register class}. By
%encapsulating this information in a separate \gls{machine description},
%\gls{DMACS} was also the first system to allow the details of the \gls{target
%  machine} to be declared separately instead of being implicitly embedded into
%the \glspl{macro}.
%
%\gls{DMACS} relies on two proprietary languages: \gls{MIML}, which declares a
%set of procedural two-argument commands that serves as the \tIRformat (see
%\refFigure{miml-example} for an example); and a declarative language called
%\gls{OMML} for implementing the \glspl{macro} that will transform each
%\gls{MIML} command into \gls{assembly code}. So far this scheme is similar to
%the one applied by \citeauthor{Wilcox1971}.
%
%When adding support for a new \gls{target machine}, a macro designer first
%specifies the set of available \glspl{register class} (including memory) as well
%as the permissible transfer paths between these classes. The macro designer then
%defines the \gls{OMML} \glspl{macro} by providing, for each macro, a list of
%\glspl{instruction} that implements the corresponding \gls{MIML} command on the
%\gls{target machine}. If necessary, a sequence of \glspl{instruction} can be
%given to emulate the effect of a single \gls{MIML} command. Lastly, constraints
%are added that force the input and output data to reside in the locations
%expected of the \gls{instruction}. \refFigure{omml-example} shows excerpts of an
%\gls{OMML} specification for an \gls{IBM}~machine.
%
%\begin{figure}[b]%
%  \centering%
%  \begin{minipage}{3.5cm}%
%    \begin{plainCode}
%1:    SS      C,J
%2:    IMUL    1,D
%3:    IADD    2,B
%4:    SS      A,I
%5:    ASSG    4,3
%    \end{plainCode}
%  \end{minipage}
%  \hfill%
%  \begin{minipage}{7.4cm}
%    \figCaption[\glsentrytext{MIML} example]%
%      {An example on how an arithmetic expression
%        \dataTerm{A[I] = B + C[J] * D}
%        can be represented as \glsentrytext{MIML} commands. The \dataTerm{SS}
%        command is used
%        for data referencing and the \dataTerm{ASSG} command assigns a value to
%        a variable. The arguments to the \glsentrytext{MIML} commands are
%        referred to either by a variable symbol or by line number}[Miller1971]
%      \labelFigure{miml-example}
%  \end{minipage}
%\end{figure}
%
%\begin{figure}[t]%
%  \centering%
%  \begin{minipage}{9cm}%
%    \centering%
%    \begin{plainCode}
%rclass REG:r2,r3,r4,r5,r6
%rclass FREG:fr0,fr2,fr4,fr6
%...
%rpath WORD->REG: L REG,WORD
%rpath REG->WORD: ST REG,WORD
%rpath FREG-WORD: LE FREG,WORD
%rpath WORD->FREG: STE FREG,WORD
%...
%ISUB s1,s2
%from REG(s1),REG(s2) emit SR s1,s2  result REG(s1)
%from REG(s1),WORD(s2) emit S s1,s2  result REG(s2)
%
%FMUL m1,m2 (commutative)
%from FREG(m1),FREG(m2) emit MER m1,m2  result FREG(m1)
%from FREG(m1),WORD(m2) emit ME m1,m2   result FREG(m1)
%    \end{plainCode}
%  \end{minipage}
%
%  \figCaption[\glsentrytext{OMML} example]%
%    {Partial \glsentrytext{machine description} for \glsentrytext{IBM-360} in
%      \glsentrytext{OMML}. The \dataTerm{rclass} command declares a
%      \glsentrytext{register class}, and the \dataTerm{rpath} command declares a
%      permissible transfer between a \glsentrytext{register class} and memory
%      (or vice versa) along with the \glsentrytext{instruction} that
%      implements the transfer}[Miller1971]
%  \labelFigure{omml-example}
%\end{figure}
%
%\gls{DMACS} uses this information to generate a collection of \glspl{finite
%  state automaton} (or \glspl{state machine}, as they are also called) to
%determine how a given set of input values can be transferred into locations that
%are permissible for a given \gls{OMML} \gls{macro}. Each \gls{state machine}
%consists of a \gls{directed gr} \gls{graph} where a \gls{node} represents a
%specific configuration of \glspl{register class} and memory, some of which are
%marked as permissible. The edges indicate how to transition from one state to
%another, and are labeled with the machine instruction that will enable the
%transition when executed on a particular input value. During compilation the
%\gls{instruction selector} consults the appropriate \gls{state machine} as it
%traverses from one \gls{MIML} command to the next, using the input values of the
%former to initialize the \gls{state machine}. As the \gls{state machine}
%transitions from one state to another, the machine instructions appearing on the
%edges are emitted until the \gls{state machine} reaches a permissible state.
%
%The work by \citeauthor{Miller1971} was pioneering but limited: \gls{DMACS} only
%handled arithmetic expressions consisting of integer and floating-point values,
%its addressing mode support was limited, and it could not model other
%\gls{target machine} classes such as stack-based architectures. In his 1973
%doctoral dissertation, \textcite{Donegan1973} extended \citeauthor{Miller1971}'s
%ideas by proposing a new language called \gls{CGPL}. \citeauthor{Donegan1973}'s
%proposal was put to the test in the 1978 master's thesis by
%\textcite{Maltz1978}, and was later extended by \textcite{Donegan1979}. Similar
%techniques have also been developed by \textcite{Tirrell1973} and
%\textcite{Simoneaux1975}, and in their survey \textcite{Ganapathi1982a} describe
%another \gls{state machine}-based \gls{compiler} called \gls{UGEN}, which was
%derived from a virtual machine called \gls{Ucode}~\cite{Perkins1979}.
%
%
%\subsubsection{Further Improvements}
%
%\glsunset{PCC}
%
%In 1975, \textcite{Snyder1975} implemented one of the first fully operational
%and portable \gls{C}~\glspl{compiler}, where the \gls{target machine}-dependent
%parts could be automatically generated from a \gls{machine description}. The
%design is similar to \citeauthor{Miller1971}'s in that the \gls{frontend} first
%transforms the \gls{program} into an equivalent representation for an abstract
%machine. In \citeauthor{Snyder1975}'s design this representation consists of
%\glspl{AMOP}, which are then expanded into target-specific \glspl{instruction}
%via \glspl{macro}. The abstract machine and \glspl{macro} are specified in a
%\gls{machine description} language which is also similar to
%\citeauthor{Miller1971}'s, but handles more complex data types, addressing
%modes, alignment, as well as branching and function calls. If needed, more
%complicated \glspl{macro} can be defined as customized \gls{C}~functions. We
%mention \citeauthor{Snyder1975}'s work primarily because it was later adapted by
%\textcite{Johnson1978} in his implementation of \gls{PCC}, which we will discuss
%in \refChapter{tree-covering}.
%
%\glsreset{PCC}
%
%\citeauthor{Fraser1977a}~\cite{Fraser1977a, Fraser1977b} also recognized the
%need for human knowledge to guide the \gls{code generation} process, and
%implemented a system with the aim of facilitating the addition of handwritten
%rules when these are required. First the \gls{program} is transformed into a
%representation based on a \gls{programming language} called \gls{XL}, which is
%akin to high-level \gls{assembly language}. For example, \gls{XL} provides
%primitives for array accesses and \techTerm{for} loops. As in the cases of
%\citeauthor{Miller1971} and \citeauthor{Snyder1975}, the \glspl{instruction} are
%provided via a separate description that maps directly to a distinct
%\gls{XL}~primitive. If some portion of the \gls{program} cannot be implemented
%by any of the available \glspl{instruction}, the \gls{instruction selector} will
%invoke a set of rules to rewrite the \gls{XL}~code until a solution is
%found. For example, array accesses are broken down into simpler primitives, and
%the same rule base can also be used to improve the code quality of the generated
%\gls{assembly code}. Since these rules are provided as a separate \gls{machine
%  description}, they can be customized and augmented as needed to fit a
%particular \gls{target machine}.
%
%As we will see, this idea of ``massaging'' the \gls{program} until a solution
%can be found has been applied, in one form or another, by many
%\glspl{instruction selector} that both predate and succeed
%\citeauthor{Fraser1977a}'s design. Although they represent a popular approach, a
%significant drawback of such schemes is that the \gls{instruction selector} may
%get stuck in an infinite loop if the set of rules is incomplete for a particular
%\gls{target machine}, and determining if this is the case is often far from
%trivial. Moreover, such rules tend to be hard to reuse for other \glspl{target
%  machine}.
%
%
%\subsection{Reducing Compilation Time with Tables}
%
%Despite their already simplistic nature, \gls{macro}-expanding
%\glspl{instruction selector} can be made even more so by representing the
%\mbox{1-to-1} or \mbox{1-to-$n$} mappings as sets of tables. This further
%emphasizes the separation between the machine-independent core of the
%\gls{instruction selector} from the machine-dependent mappings, as well as
%allows for denser implementations that require less memory and potentially
%reduce the \gls{compilation time}, which is the time it takes to compile a given
%\gls{program}.
%
%
%\subsubsection{Representing Instructions as Coding Skeletons}
%
%\begin{inParFigure}{6cm}[p][2]
%  \centering%
%
%  % LAYOUT FIX:
%  % Create a bit of space above
%  \vspace{0.9\baselineskip}
%
%  % Table cannot be full textwidth or it will overflow by 5-6 pt
%  \begin{framedBoxWI}{0.98\textwidth}
%    \centering%
%    \figureFont% The usual patch doesn't work for inParFigure
%    \begin{tabular}{lll}
%      \dataTerm{L}  & \dataTerm{B2,D(0,BD)} & \dataTerm{XXXXXXXX00000000}\\
%      \dataTerm{LH} & \dataTerm{B2,D(0,B2)} & \dataTerm{0000111100000000}\\
%      \dataTerm{LR} & \dataTerm{R1,R2}      & \dataTerm{0000110100001101}\\
%    \end{tabular}
%  \end{framedBoxWI}
%
%  % LAYOUT FIX:
%  % Create a bit of space below
%  \vspace{\baselineskip}
%\end{inParFigure}%
%\noindent
%In 1969 \textcite{Lowry1969} introduced one of the first table-driven methods
%for \gls{code generation}. In their implementation of \gls{FHC},
%\citeauthor{Lowry1969} used a bit string, called a \gls{coding skeleton}, for
%each \gls{instruction}. The bits represent the restrictions of the
%\glspl{instruction}, such as the modes permitted for the operands and the result
%(for example, ``load from memory,'' ``load from \gls{register},'' ``do not
%store,'' ``use this or that base \gls{register}''). These \glspl{coding
%  skeleton} are then matched against the bit strings corresponding to the
%\gls{program} under compilation. An `\dataTerm{X}' appearing in the \gls{coding
%  skeleton} means that it will always match any bit.
%
%The main disadvantage of \citeauthor{Lowry1969}'s design was that the tables
%could only be used for the most basic of \glspl{instruction}, and had to be
%written by hand in the case of \gls{FHC}. More extensive designs were later
%developed by \textcite{Tirrell1973} and \textcite{Donegan1973}, but these also
%suffered from similar disadvantages of making too many assumptions about the
%\gls{target machine}, which hindered \gls{compiler} retargetability.
%
%
%\subsubsection{Expanding Macros Top-Down}
%
%Later \textcite{Krumme1982} introduced a table-driven design which, unlike the
%earlier techniques, exhaustively enumerates all valid combinations of selectable
%\glspl{instruction}, schedules, and \glspl{register allocation} for a given
%\gls{program tree}. Implemented in a \gls{C}~\gls{compiler} targeting
%\gls{DEC-10} machines, the technique also allows code size to be factored in as
%an optimization goal, which was an uncommon feature at the
%time. \citeauthor{Krumme1982}'s \gls{backend} applies a recursive algorithm that
%begins by selecting \glspl{instruction} for the \gls{root} in the \gls{program
%  tree}, and then working its way down. In comparison, the bottom-up techniques
%we have examined so far all start at the leaves and then traverse upwards. We
%settle with this distinction for now as we will resume and deepen the discussion
%of bottom-up \vs top-down \gls{instruction selection} in
%\refChapter{tree-covering}.
%
%\labelPage{branch-and-bound}
%
%Enumerating all valid combinations in code generation leads to a combinatorial
%explosion, thus making it impossible to actually produce and check each and
%every one of them. To curb this immense complexity, \citeauthor{Krumme1982}
%applied a strategy known as \gls{branch-and-bound search}. The idea behind
%\gls{branch-and-bound search} is straightforward: during search, always remember
%the best solution found so far and then prune away all parts of the search space
%which can be proven to yield a worse solution\footnote{In their paper,
%  \citeauthor{Krumme1982} actually call this \gls{AB pruning}, which is an
%  entirely different search strategy, but their description of it fits more the
%  \glsshort{branch-and-bound search} approach. Both are well explained
%  in~\cite{Norvig2010}.}. The problem is how to prove that a given branch in the
%search space will definitely lead to solutions that are worse than what we
%already have (and can thus be skipped). \citeauthor{Krumme1982} only partially
%tackled this problem by pruning away branches that for sure will eventually lead
%to failure and thus yield no solution whatsoever. Without going into too much
%detail, this is done by using not just a single \gls{instruction} table but
%several---one for each so-called \emph{mode}---which are constructed in a
%hierarchical manner. In this context, a mode is oriented around the result of an
%expression, for example whether it is to be stored in a \gls{register} or in
%memory. Using these tables, the \gls{instruction selector} can look ahead and
%detect whether the current set of already-selected \glspl{instruction} will lead
%to a dead end. With this as the only method of branch pruning, however, the
%\gls{instruction selector} will make many needless revisits in the search space,
%and consequently does not scale to larger \glspl{program tree}.
%
%
%\subsection{Falling Out of Fashion}
%
%Despite the improvements we have just discussed, they still do not resolve the
%main disadvantage of \gls{macro}-expanding \glspl{instruction selector}, namely
%that they can only handle \glspl{macro} that expand a single \gls{AST}
%or \tIRnode at a time. The limitation can be somewhat circumvented by allowing
%information about the visited \glspl{node} to be forwarded from one macro to the
%next, thereby postponing \gls{assembly code} emission in the hopes that more
%efficient
%\glspl{instruction} can be used. However, if done manually---which was often
%the case---this quickly becomes an unmanageable task for the \gls{macro}
%writer, in particular if backtracking becomes necessary due to faulty decisions
%made in prior macro invocations.
%
%Thus \gls{naive macro-exp} \glspl{macro expander} are effectively limited to
%supporting only \glspl{single-output instruction}\footnote{This is a truth with
%  modification: a \gls{macro expander} can emit \glspl{multi-output
%    instruction}, but only one of its output values will be retained in the
%  \gls{assembly code}.}. As this has a detrimental effect on code quality for
%\glspl{target machine} exhibiting more complicated features, such as
%\glspl{multi-output instruction}, \glspl{instruction selector} based solely on
%\gls{naive macro-exp} \gls{macro expansion} were quickly replaced by newer, more
%powerful techniques when these started to appear in the late 1970s. One of these
%we will discuss later in this chapter.
%
%
%
%\subsubsection{Rekindled Application in the First Dynamic Code Generation
%  Systems}
%
%Having fallen out of fashion, \gls{naive macro-exp}[ly \gls{macro}-expanding]
%\glspl{instruction selector} later made a brief reappearance in the first
%dynamic \gls{code generation} systems that were developed in the 1980s and
%1990s. In such systems the \gls{program} is first compiled into \gls{byte code},
%which is a kind of target-independent \gls{machine code} that can be interpreted
%by an underlying runtime environment. By providing an identical environment on
%every \gls{target machine}, the same \gls{byte code} can be executed on multiple
%systems without having to be recompiled.
%
%The cost of this portability is that running a \gls{program} in interpretive
%mode is typically much slower than executing native \gls{machine code}. This
%performance loss can be mitigated by incorporating a \gls{compiler} into the
%runtime environment. First, the \gls{byte code} is profiled as it is
%executed. Frequently executed segments, such as inner loops, are then compiled
%into native \gls{machine code}. Since the code segments are compiled at runtime,
%this scheme is called \gls{JIT compilation}, which allows performance to be
%increased while retaining the benefits of the \gls{byte code}. If the
%performance gap between running \gls{byte code} instead of native \gls{machine
%  code} is large, then the \gls{compiler} can afford to produce \gls{assembly
%  code} of low quality in order to decrease the overhead in the runtime
%environment. This was of great importance for the earliest dynamic runtime
%systems where hardware resources were typically scarce, which made
%\gls{macro}-expanding \gls{instruction selection} a reasonable option. A few
%examples include interpreters for \gls{Smalltalk80}~\cite{Deutsch1984} and
%\gls{Omniware}~\cite{Adl-Tabatabai1996} (a predecessor to \gls{Java}), and code
%generation systems, such as \gls{VCODE}~\cite{Engler1996},
%\gls{GBURG}~\cite{Fraser1999} (which was used within a small virtual machine),
%and \gls{GNU Lightning}~\cite{GNUlightning} (which was directly inspired by
%\gls{VCODE}).
%
%\glsunset{tree covering}
%
%As technology progressed, however, dynamic \gls{code generation} systems also
%began to transition to more powerful techniques for \gls{instruction selection}
%such as \gls{tree covering}, which will be described
%in \refChapter{tree-covering}.
%
%\glsreset{tree covering}
%
%
%\section{Improving Code Quality with Peephole Optimization}
%
%An early but still applied method of improving the quality of generated
%\gls{assembly code} is to perform a subsequent \gls{program optimization} step
%that attempts to combine and replace several \glspl{instruction} with shorter,
%more efficient alternatives. These routines are known as \glspl{peephole
%  optimizer} for reasons which will soon become apparent.
%
%
%\subsection{What Is Peephole Optimization?}
%
%In 1965, \textcite{McKeeman1965} advocated the use of a simple but often
%neglected \gls{program optimization} procedure which, as a post-step to
%\gls{code generation}, inspects a small sequence of \glspl{instruction} in the
%\gls{assembly code} and attempts to combine two or more adjacent
%\glspl{instruction} with a single instruction. Similar ideas were also suggested
%by \textcite{Lowry1969} around the same time. Doing this reduces code size and
%also improves performance as using complex \glspl{instruction} is often more
%efficient than using several simpler \glspl{instruction} to implement the same
%functionality\footnote{On a related note, this idea was applied by
%  \textcite{Cho2006} for reselecting instructions in order to improve iterative
%  modulo schedules for \glspl{DSP}.}. Because of its narrow window of
%observation, this technique became known as \gls{peephole optimization}.
%
%
%\subsubsection{Modeling Instructions with Register Transfer Lists}
%\labelSection{register-transfer-lists}
%\labelPage{register-transfer-lists}
%
%Since this kind of optimization is tailored for a particular \gls{target
%  machine}, the earliest implementations were (and still often are) done ad hoc
%and by hand. For example, in 2002, \textcite{Krishnaswamy2002} wrote a
%\gls{peephole optimizer} by hand which reduces code size by replacing known
%\glspl{pattern} of \gls{ARM}~code with smaller equivalents. Recognizing the need
%for automation, \textcite{Fraser1979} introduced in 1979 the first technique
%that allowed \glspl{peephole optimizer} to be generated from a formal
%description. The technique is also described in a longer article by
%\textcite{Davidson1980}.
%
%Like \citeauthor{Miller1971}, \citeauthor{Fraser1979} described the semantics of
%the \glspl{instruction} separately in a symbolic \gls{machine description}. The
%\gls{machine description} describes the observable effects that each
%\gls{instruction} has on the \gls{target machine}'s \glspl{register}.
%\citeauthor{Fraser1979} called these effects \glspl{RT}, and each
%\gls{instruction} thus has a corresponding \gls{RTL}. For example, assume that
%we have a three-address \dT{add}~\gls{instruction} which adds an immediate
%value~\dT{imm} to the value in \gls{register}~\dT{r}[s], stores the result in
%\gls{register}~\dT{r}[d], and sets a zero flag~\dT{Z}. For this
%\gls{instruction}, the corresponding \gls{RTL} would be expressed as
%\begin{displaymath}
%  \mFunctionName{\glsentrytext{RTL}}(\mDT{add}) = \left\{
%  \begin{array}{r@{\;\; \leftarrow \;\;}l}
%    \mDT{r}[d] & \mDT{r}[s] + \dT{imm}\\
%        \dT{Z} & (\mDT{r}[s] + \dT{imm}) \Leftrightarrow 0\\
%  \end{array}
%  \right\}.
%\end{displaymath}
%
%The \glspl{RTL} are then fed to a \gls{program} called \gls{PO}, which produces
%a \gls{program optimization} routine that makes two passes over the generated
%\gls{assembly code}. The first pass runs backwards across the \gls{assembly
%  code} to determine the observable effects (that is, the \gls{RTL}) of each
%\gls{instruction} in the \gls{assembly code}. This allows effects that have no
%impact on the \gls{program}'s observable behavior to be removed. For example, if
%the value of a \gls{status flag} is not read by any subsequent
%\gls{instruction}, it is considered to be \gls{unobservable reg-trans} and can
%thus be ignored. The second pass then checks whether the combined \glspl{RTL} of
%two adjacent \glspl{instruction} are equal to that of some other
%\gls{instruction} (in \gls{PO} this check is done via a series of string
%comparisons). If such an instruction is found, the pair is replaced and the
%routine backs up one \gls{instruction} in order to check the combination of the
%new \gls{instruction} with the following \gls{instruction} in the \gls{assembly
%  code}. This way replacements can be cascaded and many \glspl{instruction}
%reduced into a single equivalent, provided there exists an appropriate
%\gls{instruction} for each intermediate step.
%
%Pioneering as it was, \gls{PO} also had several limitations. The main drawbacks
%were that it only supported combinations of two \glspl{instruction} at a time,
%and that these had to be lexicographically adjacent in the \gls{assembly
%  code}. The \glspl{instruction} were also not allowed to cross \gls{block}
%boundaries, meaning that they had to belong to the same
%\gls{block}. \textcite{Davidson1984} later removed the limitation of
%lexicographical adjacency by making use of \glspl{data-flow graph} instead of
%operating directly on the \gls{assembly code}, and they also extended the size
%of the \gls{instruction} window from pairs to triples.
%
%
%\subsubsection{Further Developments}
%
%Much research has been dedicated to improving automated approaches to
%\gls{peephole optimization}. In 1983, \textcite{Giegerich1983} proposed a formal
%design that eliminates the need for a fixed-size \gls{instruction}
%window. Shortly after, \textcite{Kessler1984} introduced a method where
%\gls{RTL} combinations and comparisons can be precomputed as the \gls{compiler}
%is built, thus decreasing compilation time. \textcite{Kessler1986} later
%expanded his work to incorporate an \mbox{$n$-size} \gls{instruction} window,
%similar to that of \citeauthor{Giegerich1983}, although at an exponential cost.
%
%Another scheme was developed by \textcite{Massalin1987} who implemented a system
%called the \gls{Superoptimizer}, and similar systems have subsequently been
%referred to as \glspl{superoptimizer}. The \gls{Superoptimizer} accepts small
%\glspl{program} written in \gls{assembly language}, and then exhaustively
%combines sequences of \glspl{instruction} to find shorter implementations that
%exhibit the same behavior as the original \gls{program}\footnote{The same idea
%  has also been applied by \textcite{El-Khalil2004} and \textcite{Anckaert2005},
%  where the \gls{assembly code} of compiled \glspl{program} is modified in order
%  to support \gls{steganography} (the covert insertion of secret messages). For
%  example, \citeauthor{Anckaert2005} used this technique on nine \glspl{program}
%  from the \gls{SPECint 2000} benchmark suite in order to embed and extract
%  William Shakespeare's play \textit{King Lear}.}. \textcite{Granlund1992} later
%adapted \citeauthor{Massalin1987}'s ideas into a method that minimizes the
%number of branches. Both implementations, however, were implemented by hand and
%customized for a particular \gls{target machine}. Moreover, neither makes any
%guarantees on correctness. A technique for automatically generating
%\gls{peephole optimization}-based \glspl{superoptimizer} was developed by
%\textcite{Bansal2006}, where the \gls{superoptimizer} learns to optimize short
%sequences of \glspl{instruction} from a set of training \glspl{program}. A
%couple of designs that guarantee correctness have been developed by
%\combinedTextcite{Joshi2002, Joshi2006} and \textcite{Crick2009}, who applied
%automatic theorem proving and a method called \gls{answer set programming},
%respectively. Recently, a similar technique based on \gls{quantifier-free
%  bit-vector logic} formulas was introduced by \textcite{Srinivasan2015}.
%
%
%\subsection{Combining \Naive Macro Expansion with Peephole Optimization}
%\labelSection{macro-expansion-with-peephole-optimization}
%
%Up to this point peephole optimizers had mainly been used to improve
%already-generated \gls{assembly code}---in other words, \emph{after}
%instruction selection had been performed. In 1984, however,
%\textcite{Davidson1984} developed an \gls{instruction selection} technique that
%incorporates the power of \gls{peephole optimization} with the simplicity of
%\gls{macro expansion}. Similar yet unsuccessful strategies had already been
%proposed earlier by \textcite{Auslander1982} and \textcite{Harrison1979}, but
%\citeauthor{Davidson1984} struck the right balance between \gls{compiler}
%retargetability and code quality, which made their design a viable option for
%production-quality \glspl{compiler}. This scheme has hence become known as the
%\gls{Davidson-Fraser approach}, and variants of it have been used in several
%\glspl{compiler}, such as the \gls{YC}~\cite{Davidson1982}, the
%\gls{ZephyrVPO}~system~\cite{Appel1998}, the \gls{ACK}~\cite{Tanenbaum1983},
%and, most famously the \gls{GCC}~\cite{Stallman1988, Khedker2012}.
%
%
%\subsubsection{The Davidson-Fraser Approach}
%
%\begin{figure}[t]
%  \centering%
%  \input{\figurePath/macro-expansion/davidson-fraser}
%
%  \figCaption{Overview of the \glsentrytext{Davidson-Fraser approach}}%
%    [Davidson1984]
%  \labelFigure{davidson-fraser}
%\end{figure}
%
%In the \gls{Davidson-Fraser approach} the \gls{instruction selector} consists of
%two parts: an \gls{expander} and a \gls{combiner} (see
%\refFigure{davidson-fraser}). The task of the \gls{expander} is to transform the
%\gls{program} into a series of \glspl{RTL}. The transformation is done by
%executing simple \glspl{macro} that expand every \gls{node} in the \gls{program
%  tree} (assuming the \gls{program} is represented as such) into a corresponding
%\gls{RTL} that describes the effects of that \gls{node}. Unlike the previous
%\glspl{macro expander} we have discussed, these \glspl{macro} do not incorporate
%\gls{register allocation}. Instead the \gls{expander} assigns each result to a
%virtual storage location called a \gls{temporary}, of which it is assumed there
%exists an infinite amount. A subsequent \gls{register allocator} then assigns
%each temporary to a \gls{register}, potentially inserting additional code that
%saves some values to memory for later retrieval when the number of available
%\glspl{register} is not enough (this is called \gls{register spilling}). After
%expansion, but before \gls{register allocation}, the \gls{combiner} is
%run. Using the same technique as that behind \gls{PO}, the \gls{combiner} tries
%to improve code quality by combining several \glspl{RTL} in the \gls{program}
%into a single, larger \gls{RTL} that corresponds to some \gls{instruction} on
%the \gls{target machine}. For this to work, both the \gls{expander} and the
%\gls{combiner} must at every step adhere to a rule, called the \gls{machine
%  invariant}, which dictates that every \gls{RTL} in the \gls{program} must be
%implementable by a single \gls{instruction}.
%
%By using a subsequent \gls{peephole optimizer} to combine the effects of
%multiple \glspl{RTL}, the \gls{instruction selector} can effectively extend over
%multiple \glspl{node} in the \gls{AST} or \gls{program tree}, potentially across
%\gls{block} boundaries. The \gls{instruction} support in
%\citeauthor{Davidson1984}'s design is therefore in theory only restricted by the
%number of \glspl{instruction} that the \gls{peephole optimizer} can compare at a
%time. For example, opportunities to replace three \glspl{instruction} by a
%single \gls{instruction} will be missed if the \gls{peephole optimizer} only
%checks pair combinations. But increasing the window size typically incurs an
%exponential cost in terms of added complexity, thus making it difficult to
%handle complicated \glspl{instruction} that require large \gls{instruction}
%windows.
%
%
%\subsubsection{Further Improvements}
%
%\textcite{Fraser1988} later expanded the work by \citeauthor{Davidson1984}. In a
%paper from~1988, \citeauthor{Fraser1988} describe a method where the
%\gls{expander} and \gls{combiner} are effectively fused together into a single
%component. The idea is to generate the \gls{instruction selector} in two
%steps. The first step produces a \gls{naive macro-exp} \gls{macro expander} that
%is capable of expanding a single \tIRnode at a time. Unlike
%\citeauthor{Davidson1984}, who implemented the \gls{expander} by hand,
%\citeauthor{Fraser1988} applied an elaborate scheme consisting of a series of
%\techTerm{switch} and \techTerm{goto} statements---effectively implementing a
%\gls{state machine}---which allowed their \gls{expander} to be generated
%automatically from a \gls{machine description}. Once produced, the \gls{macro
%  expander} is executed on a carefully designed training set. Using function
%calls embedded into the \gls{instruction selector}, a retargetable \gls{peephole
%  optimizer} is executed in tandem which discovers and gathers statistics on
%target-specific optimizations that can be done on the generated \gls{assembly
%  code}. Based on these results, the beneficial optimization decisions are then
%selected and incorporated directly into the \gls{macro expander}. This
%effectively enables the \gls{macro expander} to expand multiple \tIRnodes at a
%time, thus removing the need for a separate \gls{peephole optimizer} in the
%final \gls{compiler}. \citeauthor{Fraser1988} argued that as the
%\gls{instruction selector} only implements the optimization decisions that are
%deemed to be ``useful,'' the code quality is improved with minimal overhead.
%\textcite{Wendt1990} later improved the technique by providing a more powerful
%\gls{machine description} format, also based on \glspl{RTL}, which subsequently
%evolved into a compact standalone language used for implementing \glspl{code
%  generator} (see \textcite{Fraser1989}).
%
%
%\subsubsection{Enforcing the Machine Invariant with a Recognizer}
%
%The \gls{Davidson-Fraser approach} was also recently extended by
%\textcite{Dias2006}. Instead of requiring each separate
%\mbox{\gls{RTL}-oriented} optimization routine to abide by the \gls{machine
%  invariant}, \citeauthor{Dias2006}'s design employs a \gls{recognizer} to
%determine whether an optimization decision violates the aforementioned
%restriction (see \refFigure{dias-ramsey}). The idea is that, by doing so, the
%optimization routines can be simplified and generated automatically as they no
%longer need to internalize the \gls{machine invariant}.
%
%\begin{figure}[b]
%  \centering%
%  \input{\figurePath/macro-expansion/dias-ramsey}
%
%  \figCaption[An extension of the \glsentrytext{Davidson-Fraser approach}]%
%    {Overview of \citeauthor{Dias2006}'s design}[Dias2006]
%  \labelFigure{dias-ramsey}
%\end{figure}
%
%\begin{figure}[b]%
%  \centering%
%  \begin{minipage}{9cm}%
%    \centering%
%    \lstset{escapechar=|}
%    \begin{plainCode}
%default attribute
%  add(rd, rs1, rs2) is |\$|r[rd] := |\$|rs[rs1] + |\$|r[rs2]
%    \end{plainCode}
%  \end{minipage}
%
%  \figCaption[An \glsentrytext{instruction} expressed in
%      \glsentrytext{lambdaRTL}]
%    {A \glsentrytext{PowerPC} \dataTerm{add} \glsentrytext{instruction}
%      specified using \glsentrytext{lambdaRTL}}[Dias2010]
%  \labelFigure{lambda-rtl-example}
%\end{figure}
%
%In a paper from~2006, \citeauthor{Dias2006} demonstrate how the \gls{recognizer}
%can be produced from a declarative \gls{machine description} written in
%\gls{lambdaRTL}. Originally developed by \textcite{Ramsey1998}, \gls{lambdaRTL}
%is a high-level functional language based on \gls{ML} (which stands for
%\tNewName{Metalanguage}) and raises the level of abstraction for writing
%\glspl{RTL} (see \refFigure{lambda-rtl-example} for an example). In their paper,
%\citeauthor{Dias2006} claim that \gls{lambdaRTL}-based \glspl{machine
%  description} are more concise and simpler to write compared to those of many
%other designs, including \gls{GCC}. In particular, \gls{lambdaRTL} is precise
%and unambiguous, which makes it suitable for automated tool generation and
%verification. The latter has been explored by \textcite{Fernandez1997} and
%\textcite{Bailey2003}.
%
%The \gls{recognizer} checks whether an \gls{RTL} in the \gls{program} fulfills
%the \gls{machine invariant} by performing a syntactic comparison between that
%\gls{RTL} and the \glspl{RTL} of the \glspl{instruction}. However, if a given
%\gls{RTL} in the \gls{program} has $n$~operations, and a given \gls{lambdaRTL}
%description contains $m$~\glspl{instruction} whose \gls{RTL} contains
%$l$~operations, then a \naive implementation would take $\mBigO{nml}$ time to
%check a single \gls{RTL}. Instead, using techniques to be discussed in
%\refChapter{tree-covering}, \citeauthor{Dias2006} automatically generate the
%\gls{recognizer} as a \gls{finite state automaton} that can compare a given
%\gls{RTL} against all \glspl{RTL} in the \gls{lambdaRTL} description with a
%single check.
%
%
%\subsubsection{``One Program to Expand Them All''}
%
%In 2010, \citeauthor{Dias2010} introduced a scheme, described in \cite{Dias2010}
%and \cite{Ramsey2011}, where the \gls{macro expander} only needs to be
%implemented once per every distinct \emph{architecture family} instead of once
%per every distinct \emph{instruction set}. For example, \gls{register}-based and
%stack-based machines are two separate architecture families, whereas \gls{X86},
%\gls{PowerPC}, and \gls{Sparc} are three different \glspl{instruction set}. In
%other words, if two \glspl{target machine} belong to the same architecture
%family, then the same \gls{expander} can be used despite the differing details
%in their \glspl{instruction set}. This is useful because the correctness of the
%\gls{expander} only needs to be proven once, which is a difficult and
%time-consuming process if it is written by hand.
%
%The idea is to have a predefined set of \glspl{tile} that are specific for a
%particular architecture family. A \gls{tile} represents a simple operation which
%is required for any \gls{target machine} belonging to that architecture
%family. For example, stack-based machines require \glspl{tile} for
%\techTerm{push} and \techTerm{pop}~operations, which are not necessary on
%\gls{register}-based machines. Then, instead of expanding each \tIRnode in
%the \gls{program} into a sequence of~\glspl{RTL}, the \gls{expander} expands it
%into a sequence of \glspl{tile}. Since the set of \glspl{tile} is identical for
%all \glspl{target machine} within the same architecture family, the
%\gls{expander} only needs to be implemented once. After \gls{macro expansion}
%the \glspl{tile} are replaced by the \glspl{instruction} used to implement each
%\gls{tile}, and the resulting \gls{assembly code} can then be improved by the
%\gls{combiner}.
%
%A remaining problem is how to find \glspl{instruction} to implement a given
%\gls{tile} for a particular \gls{target machine}. In the same papers,
%\citeauthor{Dias2010} describe a scheme for doing this automatically. By
%expressing both the \glspl{tile} and the \glspl{instruction} as \gls{lambdaRTL},
%\citeauthor{Dias2010} developed a technique where the \glspl{RTL} of the
%\glspl{instruction} are combined such that the effects equal that of a
%\gls{tile}. In broad outline, the algorithm maintains a pool of \glspl{RTL}
%which initially contains those of the \glspl{instruction} found in the
%\gls{machine description}. Using algebraic laws and combining existing
%\glspl{RTL} to produce new \glspl{RTL}, the pool is grown iteratively until
%either all \glspl{tile} have been implemented, or some termination criterion is
%reached. The latter is necessary, as \citeauthor{Dias2010} proved that the
%general problem of finding implementations for arbitrary \glspl{tile} is
%undecidable.
%
%Although the primary aim of \citeauthor{Dias2010}'s design is to facilitate
%\gls{compiler} retargetability, some experiments suggest that it potentially
%also yields better code quality than the original \gls{Davidson-Fraser
%approach}. When a prototype was compared against the default \gls{instruction
%  selector} in \gls{GCC}, the results favored the former. However, this was seen
%only when all target-independent optimizations were disabled; when they were
%reactivated, \gls{GCC} still produced better results.
%
%
%\subsection{Running Peephole Optimization Before Instruction Selection}
%
%In the techniques just discussed, the \gls{peephole optimizer} runs after
%\gls{code generation}. But in a scheme developed in 1989 by
%\textcite{Genin1989}, a similar routine is executed \emph{before} \gls{code
%  generation}. Targeting digital signal processors, their \gls{compiler} first
%transforms the \gls{program} into an \gls{ISFG}, and then executes a
%routine---\citeauthor{Genin1989} called it a \emph{\gls{pattern
%    matcher}}---which attempts to find several low-level operations in the
%\gls{ISFG} that can be merged into single \glspl{node}\footnote{The paper is not
%  clear on how this is done exactly, but presumably \citeauthor{Genin1989}
%  implemented the routine as a handwritten \gls{peephole optimizer} since the
%  intermediate format is fixed and does not change from one \gls{target machine}
%  to another.}. \Gls{code generation} is then done following the conventional
%\gls{macro expansion} approach. For each \gls{node} the \gls{instruction
%  selector} invokes a rule along with the information about the current
%context. The invoked rule produces the \gls{assembly code} appropriate for the
%given context, and can also insert new \glspl{node} to offload decisions that
%are deemed better handled by the rules corresponding to the inserted
%\glspl{node}.
%
%According to \citeauthor{Genin1989}, experiments show that their \gls{compiler}
%generated \gls{assembly code} that was five to 50~times faster than that
%produced by other, contemporary \gls{DSP} \glspl{compiler}, and comparable with
%manually optimized \gls{assembly code}. A disadvantage of this design is that it
%is limited to \glspl{program} where prior knowledge about the application area,
%in this case digital signal processing, can be encoded into specific
%optimization routines, which most likely has to be done manually.
%
%
%
%\subsection{Interactive Code Generation}
%
%The aforementioned techniques yield \glspl{peephole optimizer} which are static
%once they have been generated, meaning they will only recognize and optimize
%\gls{assembly code} for a fixed set of \glspl{pattern}. A method to overcome
%this issue has been designed by \textcite{Kulkarni2006}, which is also the first
%and only one to my knowledge.
%
%In a paper from~2006, \citeauthor{Kulkarni2006} describe a \gls{compiler} system
%called \gls{Vista}, which is an interactive compilation environment where the
%user is given greater control over the \gls{compiler}. Among other things, the
%user can alter \glspl{RTL} derived from the \gls{program}'s \gls{source code}
%and add new customized \gls{peephole optimization} \glspl{pattern}. Hence
%optimization privileges which normally are limited to low-level assembly
%programmers are also granted to higher-level \gls{programming language}
%users. In addition, \citeauthor{Kulkarni2006} employed genetic
%algorithms---these will be explained in \refChapter{tree-covering}\unskip%
%% Removes unwanted space
%---in an attempt to automatically derive a combination of user-provided
%optimization guidelines to improve the code quality of a particular
%\gls{program}. Experiments show that this scheme reduced code size on average by
%4\% and up to 12\% for a selected set of \glspl{program}.
%
%
%\section{Summary}
%
%In this chapter we have discussed techniques and designs based on a
%\gls{principle} known as \gls{macro expansion}, which was the first approach to
%perform \gls{instruction selection}. The idea behind the \gls{principle} is to
%expand the \glspl{node} in the \gls{AST} or \tIRcode into one or more
%target-specific \glspl{instruction}. The expansion is done via \gls{template}
%matching and \gls{macro} invocation, which yields \glspl{instruction selector}
%that are resource-effective and straightforward to implement.
%
%But because \gls{macro}-expanding \glspl{instruction selector} only visit and
%execute \glspl{macro} one \tIRnode at a time, they require a \mbox{1-to-1} or
%\mbox{1-to-$n$} mapping between the \tIRnodes and the \glspl{instruction}
%provided by the \gls{target machine} in order to generate efficient
%\gls{assembly code}. The limitation can be mitigated by incorporating additional
%logic and bookkeeping into the \glspl{macro}, but this quickly becomes an
%unmanageable task for the \gls{macro} writer if done manually. Consequently, the
%code quality yielded by these techniques will typically be low. Moreover, as
%\glspl{instruction} are often emitted one at a time, it also becomes difficult
%to make use of \glspl{instruction} that can have unintended effects on other
%\glspl{instruction}.
%
%A more robust remedy for improving code quality is to append a \gls{peephole
%  optimizer} into the component chain of the \gls{backend}. A \gls{peephole
%  optimizer} combines the effects of multiple \glspl{instruction} in the
%\gls{assembly code} with more efficient alternatives, thereby amending some of
%the poor decisions made by the \gls{instruction selector}. \Gls{peephole
%  optimization} can also be incorporated directly into the \gls{instruction
%  selector}---a scheme which has become known as the \gls{Davidson-Fraser
%  approach}---and thereby extend its \gls{machine instruction} support. Because
%of this versatility, the \gls{Davidson-Fraser approach} remains one of the most
%powerful \gls{instruction selection} techniques to date (a variant is still
%applied in \gls{GCC} as of version 4.8.2).
%
%In \refChapter{tree-covering} we will explore another \gls{principle} of
%\gls{instruction selection}, which solves the problem of implementing several
%\gls{AST} or \tIRnodes using a single \gls{instruction} in a more direct
%fashion.
%
%\glsreset{pattern}
