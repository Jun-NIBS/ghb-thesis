% Copyright (c) 2017, Gabriel Hjort Blindell <ghb@kth.se>
%
% This work is licensed under a Creative Commons 4.0 International License (see
% LICENSE file or visit <http://creativecommons.org/licenses/by/4.0/> for a copy
% of the license).

\chapter{Existing Instruction Selection Techniques and Representations}
\labelChapter{existing-isel-techniques-and-reps}

This chapter introduces terminology and relevant background.
%
\refSection{refining-instruction-selection-into-a-graph-problem} refines
\gls{instruction selection} into a graph problem and provides many definitions
that will be used throughout the dissertation.
%
\refSection{traditional-approaches} discusses existing traditional approaches to
\gls{instruction selection}, and \refSection{combinatorial-approaches} discusses
the combinatorial counterparts.



\section{Refining Instruction Selection into a Graph Problem}
\labelSection{refining-instruction-selection-into-a-graph-problem}

\Gls{instruction selection} can be reduced into two subproblems:%
%
\begin{enumerate}
  \item Finding all instances of instructions that can implement one or more
    \glspl{operation} in the \gls{program}.
  \item Selecting a subset of these instances such that all \glspl{operation}
    are implemented.
\end{enumerate}
%
Without loss of generality, assume that the input to the \gls{instruction
  selector} to consist of a single \gls{function}, which in turn consists of
many \glspl{basic block} (henceforth referred to as simply \glspl!{block}).

Most \glspl{compiler} solve the two problems above using graph-based methods.
%
First, the \gls{IR} code is transformed into a \gls!{data-flow graph}, where
nodes represent \glspl{operation} in the function and edges represent data
dependencies between the \glspl{operation}.
%
\Glspl{data-flow graph} are called \glspl!{expression tree} if they are limited
to single, tree-shaped expressions, \glspl!{block graph} if they capture many
expressions in a \gls{block} as a \gls{DAG}, and \glspl{data-flow graph} if they
capture the data flow of entire functions.
%
Corresponding \glspl{data-flow graph}, called \glspl!{pattern graph} (or simply
\glspl!{pattern}), are also built to represent the \gls{instruction} provided by
the \gls{target machine}.
%
The set of \glspl{pattern graph} for a particular \gls{target machine}
constitute a \gls!{pattern set}.

\begin{filecontents*}{p-match-sel-example.c}
x = A[i + 1];
\end{filecontents*}

\begin{figure}
  \centering%
  \subcaptionbox{C code\labelFigure{p-match-sel-example-c}}%
                {\lstinputlisting[language=c]{p-match-sel-example.c}}%
  \hfill%
  \subcaptionbox{%
                  Instructions. The $*s$ notion means ``get value at address $s$
                  in memory''%
                  \labelFigure{p-match-sel-example-instrs}%
                }%
                [50mm]%
                {%
                  \small
                  \begin{tabular}{%
                                   >{\instrFont}r@{\hspace{4pt}}%
                                   >{$}l<{$}@{ $\leftarrow$ }%
                                   >{$}l<{$}%
                                 }
                    add  & r & s + t\\
                    mul  & r & s \times t\\
                    mad  & r & s \times t + u\\
                    load & r & *s\\
                    load & r & *(s \times t + u)
                  \end{tabular}%
                }%
  \hfill%
  \subcaptionbox{%
                   Corresponding \gls{expression tree} and \glspl{match}%
                   \labelFigure{p-match-sel-example-tree}%
                }%
                [40mm]%
                {%
                  \small%
                  \input{figures/existing-isel-techniques-and-reps/p-match-sel-example-tree}%
                }

  \caption[An example of the pattern matching and selection problem]%
          {%
            An example demonstrating the pattern matching and selection
            problem for a program that loads a value from integer array
            \irVar{A} at offset \irVar{i} $+$ \irVar{1} (it is assumed that
            \irVar{A} is stored in memory and that an integer is 4~bytes).
            Valid covers are \mbox{$\mSet{m_1, m_2, m_3, m_5}$},
            \mbox{$\mSet{m_1, m_4, m_5}$}, and \mbox{$\mSet{m_1, m_6}$}%
          }
  \labelFigure{p-match-sel-example}
\end{figure}

The first subproblem can be reduced to finding all instances where a
\gls{pattern} from the \gls{pattern set} is subgraph isomorphic to~$G$, where
$G$ denotes either a \gls{block graph} or a \gls{function graph}.
%
Each such instance is called a \gls!{match}, and the set of all \glspl{match}
constitute a \gls!{match set}, which is denoted by~$M$.
%
Hence this subproblem is referred to as the \gls!{pattern matching}[ problem].
%
\Gls{pattern matching} can be done in linear time if both $G$ and all
\glspl{pattern} are tree-shaped, otherwise it is an NP-complete
problem~\cite{GareyJohnson:1979,HoffmannODonnell:1982}.
%
Having found $M$, the second subproblem -- which is referred to as the
\gls!{pattern selection}[ problem] -- can be reduced to selecting a set of
\glspl{match} that \gls{cover}[s]~$G$.
%
A subset~\mbox{$C \subseteq M$}, where $M$ is a \gls{match set},
\gls!{cover}[s] $G$ if every \gls{operation} in $G$ appears in exactly one match
from~$C$.
%
Such a subset is called a \gls!{cover}.
%
See \refFigure{p-match-sel-example} for an example.

For a given \gls{program} and \gls{target machine}, there often exists many
valid combinations of \glspl{instruction} -- in terms of $G$ and $M$, this means
there exist many \glspl{cover} of~$G$ -- which may result in code where quality
differs significantly.
%
In certain cases, the performance of two sets of selected instructions may
differ by as much as two orders of magnitude~\cite{ZivojnovicEtAl:1994}.
%
Consequently, the \gls{pattern selection} problem -- originally defined to
accept any valid \gls{cover} -- is augmented into an optimization problem called
\gls!{optimal.ps} \gls{pattern selection}, where only \glspl{cover} with least
cost are accepted.
%
The cost of a \gls{cover}~$C$ is the sum of the costs for the \glspl{match}
appearing in~$C$, where the cost of a \gls{match} has been assigned to reflect a
desired characteristic in the produced code.
%
For example, assume that the \instrCode{add}, \instrCode{mul}, and
\instrCode{mad} \glspl{instruction} in \refFigure{p-match-sel-example} take one
cycle to execute whereas the \instrCode{load} \glspl{instruction} take five
cycles to execute.
%
Assume further that the \gls{compiler} should maximize performance.
%
The corresponding matches \mbox{$m_1, m_2, m_3, m_4, m_5, m_6$} are therefore
assigned costs \mbox{$1, 1, 1, 1, 5, 5$}, respectively.
%
Then, of the valid \glspl{cover} \mbox{$\mSet{m_1, m_2, m_3, m_5}$},
\mbox{$\mSet{m_1, m_4, m_5}$}, and \mbox{$\mSet{m_1, m_6}$}, only the last
\gls{cover} is considered \gls{optimal.ps} as it has a total cost of~6 whereas
the other two \glspl{cover} have costs~8 and~7, respectively.
%
There is typically a strong correlation between the size of a \gls{cover} and
its cost -- smaller \glspl{cover} lead to less cost, and ultimately better code
-- but this depends heavily on the properties of the \gls{target machine}.



\section{Traditional Approaches}
\labelSection{traditional-approaches}

In the interest of producing decent code while maintaining short compilation
times, most modern \glspl{compiler} either limit themselves to \glspl{expression
  tree} or apply greedy, linear-time heuristics.
%
For \glspl{expression tree} one can find \gls{least-cost.c} \glspl{cover} in
linear time, but at the price of precluding optimization decisions that only
become possible when operating on \glsshort{block graph} or \glspl{function
  graph}.



\subsection{Using Expression Trees}

The most common and well known technique that uses \glspl{expression tree} was
introduced by \textcite{AhoEtAl:1989}.
%
The approach assumes the \glspl{instruction} to be expressed in a particular
form, which will therefore be discussed first.



\paragraph{Machine Grammars}

The \gls{instruction set} of a \gls{target machine} is typically described in a
machine-readable \gls{machine description}, which can take many forms.
%
A common form is to describe the \glspl{instruction} as a \gls!{machine grammar}
(or simply called \gls!{grammar}).
%
\Glspl{machine grammar} are based on \glspl{context-free
  grammar}~\cite{AhoEtAl:2006}, which are used for describing language syntax.
%
A \gls{grammar} consists of \glspl!{terminal}, \glspl!{nonterminal}, and
\glspl!{rule}.
%
In this context, a \gls{terminal} is a symbol representing an \gls{operation}
(e.g.\ \opAdd, \opLT, \opLoad), and a \gls{nonterminal} is a symbol representing
an abstract result (e.g.\ $\mNT{Reg}$) produced by the \gls{instruction}.
%
To distinguish between the two, \glspl{terminal} are written entirely in lower
case whereas \glspl{nonterminal} start with a capital letter and are set in
italics.
%
A \gls{rule} describes the behavior of an \gls{instruction} and consists of a
\gls!{production}, a non-negative cost, and an action.
%
\Glspl{production} describe how derive \glspl{nonterminal}, and are written as
%
\begin{displaymath}
  A \rightarrow B C \ldots
\end{displaymath}
%
where the left-hand side is consists a \gls{nonterminal} and the right-hand side
consists of a sequence of \glspl{terminal} and \glspl{nonterminal}.
%
Each \gls{instruction} therefore gives rise to one or more \glspl{production},
where the right-hand side of a \gls{production} captures a \gls{pattern} of the
\gls{instruction} and the left-hand side denotes the result produced by the
\gls{instruction}.
%
The \glspl{production} are typically written in Polish notation to avoid the
need for parentheses (for example, \mbox{$1 + 2$} is written as \mbox{$+ \; 1 \;
  2$}).
%
An example is shown in \refTable{grammar-rules-example}.
%
Similarly, the \gls{expression tree} shown in
\refFigure{p-match-sel-example-tree} can be expressed as \mbox{$\opLoad \;
  \opAdd \; \opMul \; \opAdd \; \opVar{i} \; \opVar{1} \; \opVar{4} \;
  \opVar{A}$}.

\begin{table}[t]
  \centering%
  \small%
  \begin{tabular}{cr@{ $\rightarrow$ }lcl}
    \toprule
    \tabhead \# & \multicolumn{2}{c}{\tabhead Production} & \tabhead Cost
      & \multicolumn{1}{c}{\tabhead Action}\\
    \midrule
    1 & $\mNT{Reg}[1]$ & $\opLoad + \mNT{Reg}[2] \; \irCode{imm}$
      & 1
      & emit: {\instrFont load \$$\mNT{Reg}[1]$, imm(\$$\mNT{Reg}[2]$)}\\
    2 & $\mNT{Reg}[1]$ & $\opLoad + \irCode{imm} \; \mNT{Reg}[2]$
      & 1
      & emit: {\instrFont load \$$\mNT{Reg}[1]$, imm(\$$\mNT{Reg}[2]$)}\\
    3 & $\mNT{Reg}[1]$ & $\opLoad \; \mNT{Reg}[2]$
      & 1
      & emit: {\instrFont load \$$\mNT{Reg}[1]$, 0(\$$\mNT{Reg}[2]$)}\\
    \bottomrule
  \end{tabular}%

  \caption[An example of machine grammar rules]%
          {%
            An example of machine grammar rules corresponding to a
            \mbox{\instrFont load \$t, o(\$s)} \gls{instruction} that loads a
            value from memory at the address given in register~\instrCode{s},
            offset by an immediate value~\instrCode{o}, and stores the loaded
            value in register~\instrCode{t}, in one cycle.
            %
            The subscripts are only needed for reference%
          }
  \labelTable{grammar-rules-example}
\end{table}

Consequently, with a \gls{machine grammar} the \gls{pattern selection} problem
becomes equivalent to finding a sequence of \gls{rule} applications (called
\glspl!{rule reduction}) that reduces the \gls{expression tree} to a given
\gls{nonterminal}.
%
A method for finding the sequence with least cost is described shortly.



\paragraph{Machine Grammars in Linear Form}

To simplify \gls{pattern matching} and \gls{pattern selection}, a \gls{machine
  grammar} can be rewritten into \gls!{linear form.g}. A \gls{grammar} is in
\gls{linear form.g} if every \gls{rule} in the \gls{grammar} has a
\gls{production} in one of the following forms:
%
\begin{enumerate}
  \item \mbox{$\mNT{N} \rightarrow \irCode{op} \; \mNT{A}[1] \; \mNT{A}[2]
    \ldots \mNT{A}[n]$}, where \irCode{op} is a \gls{terminal}, representing an
    \gls{operation} that takes $n$ arguments, and all $\mNT{A}[i]$ are
    \glspl{nonterminal}.
    %
    Such rules are called \gls!{base.r}[ \glspl{rule}].
  \item \mbox{$\mNT{N} \rightarrow \irCode{t}$}, where \irCode{t} is a
    \gls{terminal}.
    %
    Such rules are also called \gls{base.r} \glspl{rule}.
  \item \mbox{$\mNT{N} \rightarrow \mNT{A}$}, where $\mNT{A}$ is a
    \gls{nonterminal}.
    %
    Such rules are called \gls!{chain.r}[ \glspl{rule}].
\end{enumerate}

\begin{table}[t]
  \centering%
  \small%
  \begin{tabular}{cr@{ $\rightarrow$ }lcl}
    \toprule
    \tabhead \# & \multicolumn{2}{c}{\tabhead Production} & \tabhead Cost
      & \multicolumn{1}{c}{\tabhead Action}\\
    \midrule
    1 & $\mNT{Reg}$ & $\opLoad \; \mNT{A}$
      & 1
      & emit: {\instrFont load \$$\mNT{Reg}$, $A.\text{imm}$(\$$A.\mNT{Reg}$)}\\
    4 & $\mNT{A}$ & $+ \; \mNT{Reg} \; \irCode{imm}$
      & 0
      & \\
    2 & $\mNT{Reg}$ & $\opLoad \; \mNT{B}$
      & 1
      & emit: {\instrFont load \$$\mNT{Reg}$, $B.\text{imm}$(\$$B.\mNT{Reg}$)}\\
    5 & $\mNT{B}$ & $+ \; \irCode{imm} \; \mNT{Reg}$
      & 0
      & \\
    3 & $\mNT{Reg}[1]$ & $\opLoad \; \mNT{Reg}[2]$
      & 1
      & emit: {\instrFont load \$$\mNT{Reg}[1]$, 0(\$$\mNT{Reg}[2]$)}\\
    \bottomrule
  \end{tabular}%

  \caption[The grammar from \refTable{grammar-rules-example} in linear form]%
          {%
            The grammar from \refTable{grammar-rules-example} in linear form.
            %
            The nonterminals~$\mNT{A}$ and~$\mNT{B}$ are introduced in order to
            connect rules~1 with~4 and~2 with~5, respectively%
          }
  \labelTable{linear-form-grammar-example}
\end{table}

A \gls{machine grammar} can be mechanically rewritten into \gls{linear form.g}
by introducing new \glspl{nonterminal} and breaking down illegal \glspl{rule}
into multiple, smaller \glspl{rule} until the \gls{grammar} is in \gls{linear
  form.g}.
%
For example, rewriting the \gls{grammar} shown in
\refTable{grammar-rules-example} into \gls{linear form.g} results in the
\gls{grammar} shown in \refTable{linear-form-grammar-example}.
%
Note that the new \glspl{rule} have zero cost and no action as these are only
intermediary steps towards enabling reduction of the original \gls{rule}.

Since all \glspl{production} in a \glsshort{linear form.g} \gls{grammar} have at
most one \gls{terminal}, the \gls{pattern matching} problem becomes trivial
(simply match the node type against the \gls{terminal} in all \gls{base.r}
\glspl{rule}).
%
Otherwise another bottom-up traversal of the \gls{expression tree} would have to
be made in order to find all \glspl{match}, which can be done in linear time for
most reasonable \glspl{grammar}~\cite{HoffmannODonnell:1982}.
%
As we will see, this also simplifies \gls{pattern selection} as the
\glspl{pattern} on the right-hand side in all \glspl{production} have uniform
height.



\paragraph{Optimal Pattern Selection on Expression Trees}

The technique by \citeauthor{AhoEtAl:1989} is centered around the following
assumption: the cost of reducing node~$n$ in an \gls{expression tree} to
\gls{nonterminal}~$s$ using \gls{rule}~$r$ is the cost of $r$ plus the costs of
reducing all children of $n$ to the appropriate \glspl{nonterminal} appearing on
the right-hand side of~$r$.
%
If $r$ is a \gls{chain.r} \gls{rule} then the cost is computed as the cost of
$r$ plus the cost of reducing $n$ to the right-hand-side \gls{nonterminal}
of~$r$.
%
The recursive nature of these costs can be exploited using dynamic programming,
resulting in the algorithm shown in \refAlgorithm{aho-etal-cost-algorithm} which
computes the least cost of reducing a given \gls{expression tree} to a
particular \gls{nonterminal}.

\begin{algorithm}[t]
  \DeclFunction{ComputeCosts}{expression tree $T$, grammar $G$}{
    $S$ \Assign $\mSetBuilder{s}%
                             {\text{$s$ is a nonterminal in $G$}}$\;
    $C$ \Assign matrix of size $|T| \times |S|$, initialized with $\infty$
                cost\;
    ComputeCostsRec (root node of $T$)\;
    \Return{$C$}\;
    \BlankLine
    \DeclFunction{ComputeCostsRec}{node $n$}{
      \ForEach{child $m$ of $n$}{
        \Call{ComputeCostsRec}{$m$}\;
      }
      \ForEach{base rule $r \in G$ that matches at $n$}{%
        $c$ \Assign \Call{ComputeRuleCost}{$n$, $r$}\;
        $l$ \Assign left-hand-side nonterminal of $r$\;
        \If{$c < C[n][l]$.cost}{%
          $C[n][l]$.cost \Assign $c$\;
          $C[n][l]$.rule \Assign $r$\;
        }
      }
      \Repeat{no change to $C$}{%
        \ForEach{chain rule $r \in G$}{%
          $c$ \Assign \Call{ComputeRuleCost}{$n$, $r$}\;
          $l$ \Assign left-hand-side nonterminal of $r$\;
          \If{$c < C[n][l]$.cost}{%
            $C[n][l]$.cost \Assign $c$\;
            $C[n][l]$.rule \Assign $r$\;
          }
        }
      }
    }

    \BlankLine
    \DeclFunction{ComputeRuleCost}{node $n$, rule $r$}{%
      $c$ \Assign cost of $r$\;
      \eIf{$r$ is a chain rule}{%
        $s$ \Assign right-hand-side nonterminal of $r$\;
        $c$ \Assign $c$ $+$ $C[n][s]$.cost
        \cmt*{here cost of node itself is taken instead of its children}
      }{%
        \For{$i \leftarrow 1$ \KwTo $|n|$}{%
          $m$ \Assign $i$th child of $n$\;
          $s$ \Assign $i$th right-hand-side nonterminal of $r$\;
          $c$ \Assign $c$ $+$ $C[m][s]$.cost\;
        }
      }
      \Return{$c$}\;
    }
  }

  \caption[%
            Algorithm for computing the least cost of reducing a given
            expression tree to a particular nonterminal%
          ]{%
            Algorithm for computing the least cost of reducing a given
            expression tree to a particular nonterminal.
            %
            The given grammar is assumed to be in linear form%
          }
  \labelAlgorithm{aho-etal-cost-algorithm}
\end{algorithm}

The algorithm works as follows.
%
It first constructs a cost matrix~$C$, where each row represents a node in the
\gls{expression tree} and each column represents a \gls{nonterminal} in the
\gls{grammar}, which is assumed to be in \gls{linear form.g}.
%
The cost in each element~\mbox{$C[i][j]$} is initialized to infinity,
indicating that there exists no sequence of \glspl{rule reduction} which reduces
node~$i$ to \gls{nonterminal}~$j$.
%
It then computes the costs by traversing the \gls{expression tree} bottom up.
%
At each node~$n$ and for each matching \gls{base.r} \gls{rule}~$r$, with~$s$ as
left-hand-side \gls{nonterminal}, it computes the cost~$c$ of applying $r$
at~$n$ according to the scheme stated above.
%
If $c$ is less than the currently recorded cost for reducing $n$ to $s$, then
the cost and \gls{rule} information for $n$ is updated accordingly.
%
The same is then done for all \gls{chain.r} \glspl{rule} until it reaches a
fixpoint (which must eventually be reached as all \gls{rule} costs are
non-negative and an update only occurs when the cost is strictly less).
%
Since every node is also only processed once, the algorithm runs in linear time
with respect to the size of the \gls{expression tree}.
%
Having computed the costs, the optimal order of \glspl{rule reduction} -- which
is equivalent to the \gls{least-cost.c} \gls{cover} -- can be found using the
algorithm shown in \refAlgorithm{aho-etal-select-algorithm}.

\begin{algorithm}[t]
  \DeclFunction{Select}{expression tree $T$,
                        goal nonterminal $g$,
                        cost matrix $C$}{%
    $n$ \Assign root node of $T$\;
    $r$ \Assign $C[n][g]$.rule\;
    \eIf{$r$ is a chain rule}{%
      $s$ \Assign right-hand-side nonterminal of $r$\;
      \Call{Select}{$T$, $s$, $C$}\;
    }{%
      \For{$i \leftarrow 1$ \KwTo $|n|$}{%
        $m$ \Assign $i$th child of $n$\;
        $s$ \Assign $i$th right-hand-side nonterminal of $r$\;
        \Call{Select}{expression tree rooted at $m$, $s$}\;
      }
    }
    execute actions associated with $r$\;
  }

  \caption{%
            Algorithm for selecting rules based on costs computed by
            \refAlgorithm{aho-etal-cost-algorithm}%
          }
  \labelAlgorithm{aho-etal-select-algorithm}
\end{algorithm}

In many cases, this technique produces code of sufficient quality.
%
In fact, for architectures with simple \glspl{instruction set}, where the
\glspl{pattern} can naturally be modeled as trees, it is often optimal or near
optimal.
%
The MIPS \gls{instruction set}~\cite{Sweetman:2006}, for example, is one such
architecture.



\paragraph{Precomputing Costs and Rule Decisions}

Shortly after its introduction, it was recognized that the costs computed by
\refAlgorithm{aho-etal-cost-algorithm} could be precomputed for any
\glspl{expression tree}, thereby replacing the call to the
\algStyle{ComputeRuleCost} function with a table lookup.
%
While this improvement does not affect the asymptotic complexity, it does
significantly reduce the runtime of the algorithm.
%
Although pioneered by \textcite{HatcherChristopher:1986}, the idea was first
successfully applied by \textcite{Pelegri-LlopartGraham:1988}, and later
improved and simplified by \textcite{BalachandranEtAl:1990} and
\textcite{Proebsting:1992:PLDI}.
%
The \gls{expression tree} is traversed bottom up and labeled with a
\gls!{state}.
%
For a given labeled node~$n$, the \gls{state} essentially holds enough
information needed to optimally reduce the \gls{expression tree} rooted at~$n$
to any \gls{nonterminal}.
%
At first glance, it would seem that this requires an infinite number of
\glspl{state} since \glspl{expression tree} can be of arbitrary size, but this
only applies if the full cost of the entire \gls{expression tree} is taken into
account.
%
Fortunately, it is sufficient to only consider the relative cost differences
between \glspl{rule} for any given node in the \gls{expression tree}, leading to
a finite number of \glspl{state}.

The idea for computing the \glspl{state} -- which will only be described briefly
-- works as follows.
%
First the states for all leaf nodes are built, considering only \gls{base.r}
\glspl{rule} with a single \glspl{terminal} on the right-hand side in the
\gls{production}.
%
The costs and \gls{rule} decisions are computed using the same logic as in
\refAlgorithm{aho-etal-cost-algorithm}, lines~8--21.
%
The leaf \gls{state} are then pushed onto a queue.
%
Each popped \gls{state} is used as the $i$th child to all \gls{base.r}
\glspl{rule} with a non-leaf \glspl{terminal} in combination with all other
existing \glspl{state}.
%
If any combination gives rise to a new set of costs and \gls{rule} decisions,
then a new \gls{state} is created and pushed onto the queue.
%
This process continues until the queue is empty, whereupon all necessary
\glspl{state} have been built.



\paragraph{Limitations}

\begin{filecontents*}{p-sel-example.c}x
t = a + b;
c = c * t;
x = *t;
\end{filecontents*}

\begin{figure}
  \centering%
  \subcaptionbox{C code\labelFigure{p-sel-example}}%
                {\lstinputlisting[language=c]{p-sel-example.c}}%
  \hfill%
  \subcaptionbox{%
                  Instructions. The $*s$ notion means ``get value at address $s$
                  in memory''%
                  \labelFigure{p-sel-example-instrs}%
                }%
                [50mm]%
                {%
                  \small
                  \begin{tabular}{%
                                   >{\instrFont}r@{\hspace{4pt}}%
                                   >{$}l<{$}@{ $\leftarrow$ }%
                                   >{$}l<{$}%
                                   c%
                                 }
                    \toprule
                    \multicolumn{3}{c}{\tabhead Instruction} & \tabhead Cost\\
                    \midrule
                    add  & r & s + t & 2\\
                    mul  & r & s \times t & 3\\
                    adm  & r & (s + t) \times u & 4\\
                    load & r & *s & 5\\
                    \bottomrule
                  \end{tabular}%
                }%
  \hfill%
  \subcaptionbox{%
                   Corresponding \gls{block graph} and \glspl{match}%
                   \labelFigure{p-sel-example-dag}%
                }%
                [35mm]%
                {%
                  \small%
                  \input{figures/existing-isel-techniques-and-reps/p-sel-example-dag}%
                }

  \caption[An example illustrating the limitation of expression trees]%
          {%
            TODO%
          }
  \labelFigure{TODO}
\end{figure}

The main disadvantage of operating on \glspl{expression tree} is that decisions
made for one tree may inhibit \gls{instruction selection} for subsequent
\glspl{expression tree}.
%
For example, ...
\todo{write}



\subsection{Using Block Graphs}

\todo{write}

\subsection{Using Function Graphs}

\todo{write}



\section{Combinatorial Approaches}
\labelSection{combinatorial-approaches}

\todo{write}
