% Copyright (c) 2017, Gabriel Hjort Blindell <ghb@kth.se>
%
% This work is licensed under a Creative Commons 4.0 International License (see
% LICENSE file or visit <http://creativecommons.org/licenses/by/4.0/> for a copy
% of the license).

\chapter{Existing Instruction Selection Techniques and Representations}
\labelChapter{existing-isel-techniques-and-reps}

\todo{write chapter overview}



\section{TODO}

\Gls{instruction selection} can be reduced into two subproblems:%
%
\begin{enumerate}
  \item Finding all instances of instructions that can implement one or more
    \glspl{operation} in the \gls{program}.
  \item Selecting a subset of these instances such that all \glspl{operation}
    are implemented.
\end{enumerate}
%
Without loss of generality, assume that the input to the \gls{instruction
  selector} to consist of a single \gls{function}, which in turn consists of
many \glspl{basic block} (henceforth referred to as simply \glspl!{block}).

Most \glspl{compiler} solve the two problems above using graph-based methods.
%
First, the \gls{IR} code is transformed into a \gls!{data-flow graph}, where
nodes represent \glspl{operation} in the function and edges represent data
dependencies between the \glspl{operation}.
%
\Glspl{data-flow graph} limited to \glspl{block} are called \glspl!{block
  graph}, and \glspl{data-flow graph} capturing data flow of whole functions are
called \glspl!{function graph}.
%
Corresponding \glspl{data-flow graph}, called \glspl!{pattern graph} (or simply
\glspl!{pattern}), are also built to represent the \gls{instruction} provided by
the \gls{target machine}.
%
The set of \glspl{pattern graph} for a particular \gls{target machine}
constitute a \gls!{pattern set}.

\begin{filecontents*}{p-match-sel-example.c}
x = A[i + 1];
\end{filecontents*}

\begin{figure}
  \centering%
  \subcaptionbox{C code\labelFigure{p-match-sel-example-c}}%
                {\lstinputlisting[language=c]{p-match-sel-example.c}}%
  \hspace{5mm}%
  \subcaptionbox{%
                  Instructions. The $*s$ notion means ``get value at address $s$
                  in memory''%
                  \labelFigure{p-match-sel-example-instrs}%
                }%
                [50mm]%
                {%
                  \small
                  \begin{tabular}{%
                                   >{\instrFont}r@{\hspace{4pt}}%
                                   >{$}l<{$}@{ $\leftarrow$ }%
                                   >{$}l<{$}%
                                 }
                    add  & r & s + t\\
                    mul  & r & s \times t\\
                    mad  & r & s \times t + u\\
                    load & r & *s\\
                    load & r & *(s \times t + u)
                  \end{tabular}%
                }%
  \hspace{5mm}%
  \subcaptionbox{%
                   Corresponding \gls{block graph} and \glspl{match}%
                   \labelFigure{p-match-sel-example-graph}%
                }{%
                  \small%
                  \input{figures/introduction/p-match-sel-example-graph}%
                }

  \caption[An example of the pattern matching and selection problem]%
          {%
            An example demonstrating the pattern matching and selection
            problem for a program that loads a value from integer array
            \irVar{A} at offset \irVar{i} $+$ \irVar{1} (it is assumed that
            \irVar{A} is stored in memory and that an integer is 4~bytes).
            Valid covers are \mbox{$\mSet{m_1, m_2, m_3, m_5}$},
            \mbox{$\mSet{m_1, m_4, m_5}$}, and \mbox{$\mSet{m_1, m_6}$}%
          }
  \labelFigure{p-match-sel-example}
\end{figure}

The first subproblem can be reduced to finding all instances where a
\gls{pattern} from the \gls{pattern set} is subgraph isomorphic to~$G$, where
$G$ denotes either a \gls{block graph} or a \gls{function graph}.
%
Each such instance is called a \gls!{match}, and the set of all \glspl{match}
constitute a \gls!{match set}, which is denoted by~$M$.
%
Hence this subproblem is referred to as the \gls!{pattern matching}[ problem].
%
\Gls{pattern matching} can be done in linear time if both $G$ and all
\glspl{pattern} are tree-shaped, otherwise it is an NP-complete
problem~\cite{GareyJohnson:1979,HoffmannODonnell:1982}.
%
Having found $M$, the second subproblem -- which is referred to as the
\gls!{pattern selection}[ problem] -- can be reduced to selecting a set of
\glspl{match} that \gls{cover}[s]~$G$.
%
A subset~\mbox{$C \subseteq M$}, where $M$ is a \gls{match set},
\gls!{cover}[s] $G$ if every \gls{operation} in $G$ appears in exactly one match
from~$C$.
%
Such a subset is called a \gls!{cover}.
%
See \refFigure{p-match-sel-example} for an example.

For a given \gls{program} and \gls{target machine}, there often exists many
valid combinations of \glspl{instruction} -- in terms of $G$ and $M$, this means
there exist many \glspl{cover} of~$G$ -- which may result in code where quality
differs significantly.
%
In certain cases, the performance of two sets of selected instructions may
differ by as much as two orders of magnitude~\cite{ZivojnovicEtAl:1994}.
%
Consequently, the \gls{pattern selection} problem -- originally defined to
accept any valid \gls{cover} -- is augmented into an optimization problem called
\gls!{optimal.ps} \gls{pattern selection}, where only \glspl{cover} with least
cost are accepted.
%
The cost of a \gls{cover}~$C$ is the sum of the costs for the \glspl{match}
appearing in~$C$, where the cost of a \gls{match} has been assigned to reflect a
desired characteristic in the produced code.
%
For example, assume that the \instrCode{add}, \instrCode{mul}, and
\instrCode{mad} \glspl{instruction} in \refFigure{p-match-sel-example} take one
cycle to execute whereas the \instrCode{load} \glspl{instruction} take five
cycles to execute.
%
Assume further that the \gls{compiler} should maximize performance.
%
The corresponding matches \mbox{$m_1, m_2, m_3, m_4, m_5, m_6$} are therefore
assigned costs \mbox{$1, 1, 1, 1, 5, 5$}, respectively.
%
Then, of the valid \glspl{cover} \mbox{$\mSet{m_1, m_2, m_3, m_5}$},
\mbox{$\mSet{m_1, m_4, m_5}$}, and \mbox{$\mSet{m_1, m_6}$}, only the last
\gls{cover} is considered \gls{optimal.ps} as it has a total cost of~6 whereas
the other two \glspl{cover} have costs~8 and~7, respectively.
%
There is typically a strong correlation between the size of a \gls{cover} and
its cost -- smaller \glspl{cover} lead to less cost, and ultimately better code
-- but this depends heavily on the properties of the \gls{target machine}.



\section{Traditional Approaches}

In the interest of producing decent code while maintaining short compilation
times, most modern \glspl{compiler} either limit themselves to \glspl{block
  graph} or apply greedy, linear-time heuristics.
%
For \glspl{block graph} one can find \gls{least-cost.c} \glspl{cover} in linear
time, but at the price of precluding optimization decisions that only become
possible when operating on \glspl{function graph}.



\subsection{Machine Descriptions}

\todo{introduce grammars}
\todo{introduce nonterminals}
\todo{define lhs and rhs nonterminals}
\todo{define chain rules}
\todo{define linear-form grammars}
\todo{relate rule reductions with covers}



\subsection{Operating on Block Graphs}

The most common and well known technique that operates on \glspl{block graph}
was introduced by \textcite{AhoEtAl:1989}.
%
The technique is centered around the following assumption: the cost of reducing
node~$n$ in an \gls{expression tree} to \gls{nonterminal}~$s$ using
\gls{rule}~$r$ is the cost of $r$ plus the costs of reducing all children of $n$
to the appropriate \glspl{nonterminal} appearing on the right-hand side of~$r$.
%
If $r$ is a \gls{chain.r} \gls{rule} then the cost is computed as the cost of
$r$ plus the cost of reducing $n$ to the right-hand-side \gls{nonterminal}
of~$r$.
%
The recursive nature of these costs can be exploited using dynamic programming,
resulting in the algorithm shown in \refAlgorithm{aho-etal-cost-algorithm} which
computes the least cost of reducing a given \gls{expression tree} to a
particular \gls{nonterminal}.

\begin{algorithm}[t]
  \DeclFunction{ComputeCosts (expression tree $T$, grammar $G$)}{
    $S$ \Assign $\mSetBuilder{s}%
                             {\text{$s$ is a nonterminal in $G$}}$\;
    $C$ \Assign matrix of size $|T| \times |S|$, initialized with $\infty$
                cost\;
    ComputeCostsRec (root node of $T$)\;
    \Return{$C$}\;
    \BlankLine
    \DeclFunction{ComputeCostsRec (node $n$)}{
      \ForEach{child $m$ of $n$}{
        ComputeCostsRec ($m$)\;
      }
      $R$ \Assign $\mSetBuilder{r}{\text{%
                                          $r \in G$,
                                          $r$ is a base rule that matches at $n$
                                            \Or $r$ is a chain rule%
                                        }}$\;
      \ForEach{rule $r \in R$, in transitive reduction order}{%
        $c$ \Assign ComputeRuleCost ($n$, $r$)\;
        $l$ \Assign left-hand-side nonterminal of $r$\;
        \If{$c < c_{n,l}$.cost}{%
          $c_{n,l}$.cost \Assign $c$\;
          $c_{n,l}$.rule \Assign $r$\;
        }
      }
    }

    \BlankLine
    \DeclFunction{ComputeRuleCost (node $n$, rule $r$)}{%
      $c$ \Assign cost of $r$\;
      \eIf{$r$ is a chain rule}{%
        $s$ \Assign right-hand-side nonterminal of $r$\;
        $c$ \Assign $c$ $+$ $c_{n,s}$.cost
        \cmt*{here cost of node itself is taken instead of its children}
      }{%
        \For{$i \leftarrow 1$ \KwTo $|n|$}{%
          $m$ \Assign $i$th child of $n$\;
          $s$ \Assign $i$th right-hand-side nonterminal of $r$\;
          $c$ \Assign $c$ $+$ $c_{m,s}$.cost\;
        }
      }
      \Return{$c$}\;
    }
  }

  \caption[%
            Algorithm for computing the least cost of reducing a given
            expression tree to a particular nonterminal%
          ]{%
            Algorithm for computing the least cost of reducing a given
            expression tree to a particular nonterminal. It runs in linear time
            and assumed the given grammar to be in linear form%
          }
  \labelAlgorithm{aho-etal-cost-algorithm}
\end{algorithm}

The algorithm works as follows.
%
It first constructs a cost matrix~$C$, where each row represents a node in the
\gls{expression tree} and each column represents a \gls{nonterminal} in the
\gls{grammar}, which is assumed to be in \gls{linear form.g}.
%
The cost in each element~\mbox{$c_{i, j}$} is initialized to infinity,
indicating that there exists no chain of \glspl{rule reduction} which reduces
node~$i$ to \gls{nonterminal}~$j$.
%
It then computes the costs by traversing the \gls{expression tree} bottom up.
%
At each node~$n$ it constructs a \gls{rule} set~$R$, containing all matching
\gls{base.r} \glspl{rule} -- that is, all \glspl{rule} with a \gls{terminal} on
the right-hand side that matches the type of~$n$ -- as well as all \gls{chain.r}
\glspl{rule}.
%
For each \gls{rule}~$r$ in~$R$, it computes the cost of applying $r$ at~$n$
according to the scheme stated above.
%
Since \gls{chain.r} \glspl{rule} depend on other \glspl{rule} to become
applicable, the ...

Since the application of a \gls{chain.r} \gls{rule} may depend on other
\glspl{rule}, the order in which the \glspl{rule} are processed matters.
%




it selects the \gls{rule} with least cost that reduces the node to a particular
\gls{nonterminal}.
%
Moving one 
Then it traverses one step up in the \gls{expression tree} and


\todo{explain algorithm}
\todo{define transitive reduction order}

\todo{linear time}

\begin{algorithm}[t]
  \DeclFunction{Select (expression tree $T$,
                        goal nonterminal $g$,
                        cost matrix $C$)}{%
    $n$ \Assign root node of $T$\;
    $r$ \Assign $C[n][g]$.rule\;
    \eIf{$r$ is a chain rule}{%
      $s$ \Assign right-hand-side nonterminal of $r$\;
      Select ($T$, $s$)\;
    }{%
      \For{$i \leftarrow 1$ \KwTo $|n|$}{%
        $m$ \Assign $i$th child of $n$\;
        $s$ \Assign $i$th right-hand-side nonterminal of $r$\;
        Select (expression tree rooted at $m$, $s$)\;
      }
    }
    execute actions associated with $r$\;
  }

  \caption{%
            Algorithm for selecting rules based on costs computed by
            \refAlgorithm{aho-etal-cost-algorithm}%
          }
  \labelAlgorithm{aho-etal-select-algorithm}
\end{algorithm}

Having computed the costs, the optimal order of \glspl{rule reduction} -- which
is equivalent to the \gls{least-cost.c} \gls{cover} -- can be found using the
algorithm shown in \refAlgorithm{aho-etal-select-algorithm}.

\todo{explain benefits?}
\todo{explain drawbacks}



\subsection{Operating on Function Graphs}

\todo{write}



\section{Combinatorial Approaches}

\todo{write}
