% Copyright (c) 2017, Gabriel Hjort Blindell <ghb@kth.se>
%
% This work is licensed under a Creative Commons 4.0 International License (see
% LICENSE file or visit <http://creativecommons.org/licenses/by/4.0/> for a copy
% of the license).

\chapter{Existing Instruction Selection Techniques and Representations}
\labelChapter{existing-isel-techniques-and-reps}

This chapter introduces terminology and relevant background.
%
\refSection{refining-instruction-selection-into-a-graph-problem} refines
\gls{instruction selection} into a graph problem and provides many definitions
that will be used throughout the dissertation.
%
\refSection{traditional-approaches} discusses existing \glspl{traditional
  approach} to \gls{instruction selection}, and
\refSection{combinatorial-approaches} discusses the \glsshort{combinatorial
  approach} counterparts.
%
In this dissertation, \glsshort!{traditional approach} approaches include the
techniques discussed in most \gls{compiler} textbooks, and derivations thereof.
%
\Glsshort!{combinatorial approach} approaches include techniques that solve
\gls{instruction selection} using a model that consists of \glspl{variable},
\glspl{constraint}, and an objective function.
%
The model is then solved using either complete methods or greedy heuristics.
%
Lastly, \refSection{limitations-of-existing-approaches} discusses limitations of
these approaches.

A more extensive discussion is also given in
\refAppendixRange{macro-expansion}{graph-covering}.



\section{Refining Instruction Selection into a Graph Problem}
\labelSection{refining-instruction-selection-into-a-graph-problem}

\Gls{instruction selection} can be reduced into two subproblems:%
%
\begin{enumerate}
  \item Finding all instances of instructions that can implement one or more
    \glspl{operation} in the \gls{program}.
  \item Selecting a subset of these instances such that all \glspl{operation}
    are implemented.
\end{enumerate}
%
Without loss of generality, assume that the input to the \gls{instruction
  selector} to consist of a single \gls{function}, which in turn consists of
many \glspl{basic block} (henceforth referred to as simply \glspl!{block}).

Most \glspl{compiler} solve the two problems above using \gls{graph}-based
methods.
%
First, the \gls{IR} code is transformed into a \gls!{data-flow graph}, where
\glspl{node} represent \glspl{operation} in the function and \glspl{edge}
represent data dependencies between the \glspl{operation}.
%
\Glspl{data-flow graph} are called \glspl!{expression tree} if they are limited
to single, tree-shaped expressions, \glspl!{block DAG} if they capture many
expressions in a \gls{block} as a \gls{DAG}, and \glspl{data-flow graph} if they
capture the data flow of entire functions.
%
An \gls{instruction selector} is \gls!{local.is} if it selects
\glspl{instruction} for \glspl{expression tree} or \glspl{block DAG}, and
\gls!{global.is} if it does so for \glspl{function graph}.
%
Corresponding \glspl{data-flow graph}, called \glspl!{pattern graph} (or simply
\glspl!{pattern}), are also built to represent the \gls{instruction} provided by
the \gls{target machine}.
%
The set of \glspl{pattern graph} for a particular \gls{target machine}
constitute a \gls!{pattern set}.

The first subproblem can be reduced to finding all instances where a
\gls{pattern} from the \gls{pattern set} is \gls{subgraph} isomorphic to~$G$,
where $G$ denotes either a \gls{block DAG} or a \gls{function graph}.
%
Each such instance is called a \gls!{match}, and the set of all \glspl{match}
constitute a \gls!{match set}, which is denoted by~$M$.
%
Hence this subproblem is referred to as the \gls!{pattern matching}[ problem].
%
\Gls{pattern matching} can be done in linear time if both $G$ and all
\glspl{pattern} are tree-shaped, otherwise it is an NP-complete
problem~\cite{GareyJohnson:1979,HoffmannODonnell:1982}.
%
Having found $M$, the second subproblem -- which is referred to as the
\gls!{pattern selection}[ problem] -- can be reduced to selecting a set of
\glspl{match} that \gls{cover}[s]~$G$.
%
A subset~\mbox{$C \subseteq M$}, where $M$ is a \gls{match set},
\gls!{cover}[s] $G$ if every \gls{operation} in $G$ appears in exactly one match
from~$C$.
%
Such a subset is called a \gls!{cover}, and an example is shown in
\refFigure{p-match-sel-example}.

\begin{filecontents*}{p-match-sel-example.c}
x = A[i + 1];
\end{filecontents*}

\begin{figure}
  \centering%
  \subcaptionbox{C code\labelFigure{p-match-sel-example-c}}%
                {\lstinputlisting[language=c]{p-match-sel-example.c}}%
  \hfill%
  \subcaptionbox{%
                  Instructions. The $*s$ notion means ``get value at address $s$
                  in memory''%
                  \labelFigure{p-match-sel-example-instrs}%
                }%
                [50mm]%
                {%
                  \figureFontSize%
                  \begin{tabular}{%
                                   >{\instrFont}r@{\hspace{4pt}}%
                                   >{$}l<{$}@{ $\leftarrow$ }%
                                   >{$}l<{$}%
                                 }
                    add    & r & s + t\\
                    mul    & r & s \times t\\
                    muladd & r & s \times t + u\\
                    load   & r & *s\\
                    maload & r & *(s \times t + u)
                  \end{tabular}%
                }%
  \hfill%
  \subcaptionbox{%
                  Expression tree and its matches%
                  \labelFigure{p-match-sel-example-tree}%
                }{%
                  \input{figures/existing-isel-techniques-and-reps/p-match-sel-example-tree}%
                }

  \caption[Example of the pattern matching and selection problem]%
          {%
            Example demonstrating the pattern matching and selection problem
            for a program that loads a value from integer array \irVar{A} at
            offset \mbox{\irVar{i} $+$ \irVar{1}} (it is assumed that \irVar{A}
            is stored in memory and that an integer is 4~bytes).
            %
            Valid covers are \mbox{$\mSet{m_1, m_2, m_3, m_5}$},
            \mbox{$\mSet{m_1, m_4, m_5}$}, and \mbox{$\mSet{m_1, m_6}$}.
            %
            Note that variable assignments are not explicitly represented as
            nodes as this information can be propagated from the root node of
            an expression tree after having found a cover%
          }
  \labelFigure{p-match-sel-example}
\end{figure}

For a given \gls{program} and \gls{target machine}, there often exists many
valid combinations of \glspl{instruction} -- in terms of $G$ and $M$, this means
there exist many \glspl{cover} of~$G$ -- which may result in code where quality
differs significantly.
%
In certain cases, the performance of two sets of selected instructions may
differ by as much as two orders of magnitude~\cite{ZivojnovicEtAl:1994}.
%
Consequently, the \gls{pattern selection} problem -- originally defined to
accept any valid \gls{cover} -- is augmented into an optimization problem called
\gls!{optimal.ps} \gls{pattern selection}, where only \glspl{cover} with least
cost are accepted.
%
The cost of a \gls{cover}~$C$ is the sum of the costs for the \glspl{match}
appearing in~$C$, where the cost of a \gls{match} has been assigned to reflect a
desired characteristic in the produced code.
%
For example, assume that the \instrCode{add}, \instrCode{mul}, and
\instrCode{muladd} \glspl{instruction} in \refFigure{p-match-sel-example} take
one cycle to execute whereas the \instrCode{load} and \instrCode{amload}
\glspl{instruction} take five cycles to execute.
%
Assume further that the \gls{compiler} should maximize performance.
%
The corresponding matches \mbox{$m_1, m_2, m_3, m_4, m_5, m_6$} are therefore
assigned costs \mbox{$1, 1, 1, 1, 5, 5$}, respectively.
%
Then, of the valid \glspl{cover} \mbox{$\mSet{m_1, m_2, m_3, m_5}$},
\mbox{$\mSet{m_1, m_4, m_5}$}, and \mbox{$\mSet{m_1, m_6}$}, only the last
\gls{cover} is considered \gls{optimal.ps} as it has a total cost of~6 whereas
the other two \glspl{cover} have costs~8 and~7, respectively.
%
There is typically a strong correlation between the size of a \gls{cover} and
its cost -- smaller \glspl{cover} lead to less cost, and ultimately better code
-- but this depends heavily on the properties of the \gls{target machine}.



\section{Traditional Approaches}
\labelSection{traditional-approaches}

In the interest of producing decent code while maintaining short compilation
times, most modern \glspl{compiler} either limit themselves to \glspl{expression
  tree} or apply greedy, linear-time heuristics.
%
For \glspl{expression tree} one can find \gls{least-cost.c} \glspl{cover} in
linear time, but at the price of precluding optimization decisions that only
become possible when operating on \glsshort{block DAG} or \glspl{function
  graph}.



\subsection{Approaches Using Expression Trees}

The most common and well known technique that uses \glspl{expression tree} was
introduced by \textcite{AhoEtAl:1989}.
%
The approach assumes the \glspl{instruction} to be expressed in a particular
form, which will therefore be discussed first.



\paragraph{Machine Grammars}
\labelPage{machine-grammars}

The \gls{instruction set} of a \gls{target machine} is typically described in a
machine-readable \gls{machine description}, which can take many forms.
%
A common form is to describe the \glspl{instruction} as a \gls!{machine grammar}
(or simply called \gls!{grammar}).
%
\Glspl{machine grammar} are based on \glspl{context-free
  grammar}~\cite{AhoEtAl:2006}, which are used for describing language syntax.
%
A \gls{grammar} consists of \glspl{terminal}, \glspl{nonterminal}, and
\glspl{rule}.
%
In this context, a \gls!{terminal} is a symbol representing an \gls{operation}
(e.g.\ \opAdd, \opLT, \opLoad), and a \gls!{nonterminal} is a symbol
representing an abstract result (e.g.\ $\mNT{Reg}$) produced by the
\gls{instruction}.
%
To distinguish between the two, \glspl{terminal} are written entirely in lower
case whereas \glspl{nonterminal} start with a capital letter and are set in
italics.
%
A \gls!{rule} describes the behavior of an \gls{instruction} and consists of a
\gls{production}, a non-negative cost, and an action.
%
\Glspl!{production} describe how to derive \glspl{nonterminal}, and are written
as
%
\begin{displaymath}
  a \rightarrow b c \ldots
\end{displaymath}
%
where the left-hand side is consists a \gls{nonterminal} and the right-hand side
consists of a sequence of \glspl{terminal} and \glspl{nonterminal}.
%
Each \gls{instruction} therefore gives rise to one or more \glspl{production},
where the right-hand side of a \gls{production} captures a \gls{pattern} of the
\gls{instruction} and the left-hand side denotes the result produced by the
\gls{instruction}.
%
Hence the left-hand and right-hand sides of a \gls{rule} are referred to as the
\gls{rule}'s \glsshort!{rule result} and \glsshort!{rule pattern}, respectively.
%
The \gls{rule} structure is also illustrated in
\refFigure{machine-grammar-rule-anatomy}, and an example of a few \glspl{rule}
is given in \refTable{grammar-rules-example}.
%
\begin{figure}
  \centering%
  \figureFont\figureFontSize%
  \begin{displaymath}
    \underbrace{
      \overbrace{
        \overbrace{\mNT{Reg}[1]}^{\text{result}}
        \rightarrow \;
        \overbrace{\opAdd{} \; \mNT{Reg}[2] \; \irCode{const}}^{\text{pattern}}%
      }^{\text{production}}
      \qquad
      \overbrace{\text{4}}^{\text{cost}}
      \qquad
      \overbrace{
        \text{emit:}~
        \text{\instrFont add \$$\mNT{Reg}[1]$, \$$\mNT{Reg}[2]$, \#const}%
      }^{\text{action}}
    }_{\text{rule}}
  \end{displaymath}

  \vspace*{-\baselineskip}

  \caption{Anatomy of a rule in a machine grammar}
  \labelFigure{machine-grammar-rule-anatomy}
\end{figure}
%
The \glspl{production} are typically written in Polish notation to avoid the
need for parentheses (for example, \mbox{$1 + (2 + 3)$} is written as \mbox{$+
  \; 1 + 2 \; 3$}).
%
Similarly, the \gls{expression tree} shown in
\refFigure{p-match-sel-example-tree} can be expressed as \mbox{$\opLoad \;
  \opAdd \; \opMul \; \opAdd \; \opVar{i} \; \opVar{1} \; \opVar{4} \;
  \opVar{A}$}.

\begin{table}[t]
  \centering%
  \figureFontSize%
  \begin{tabular}{cr@{ $\rightarrow$ }lcl}
    \toprule
    \tabhead \# & \multicolumn{2}{c}{\tabhead Production} & \tabhead Cost
      & \multicolumn{1}{c}{\tabhead Action}\\
    \midrule
    1 & $\mNT{Reg}[1]$ & $\opLoad \; \opAdd \; \mNT{Reg}[2] \; \irCode{const}$
      & 1
      & emit: {\instrFont load \$$\mNT{Reg}[1]$, const(\$$\mNT{Reg}[2]$)}\\
    2 & $\mNT{Reg}[1]$ & $\opLoad \; \opAdd \; \irCode{const} \; \mNT{Reg}[2]$
      & 1
      & emit: {\instrFont load \$$\mNT{Reg}[1]$, const(\$$\mNT{Reg}[2]$)}\\
    3 & $\mNT{Reg}[1]$ & $\opLoad \; \mNT{Reg}[2]$
      & 1
      & emit: {\instrFont load \$$\mNT{Reg}[1]$, 0(\$$\mNT{Reg}[2]$)}\\
    \bottomrule
  \end{tabular}%

  \caption[Example of rules]%
          {%
            Example of rules corresponding to a
            \mbox{\instrFont load \$t, o(\$s)} \gls{instruction} that loads a
            value from memory at the address given in register~\instrCode{s},
            offset by an immediate value~\instrCode{o}, and stores the loaded
            value in register~\instrCode{t}, in one cycle.
            %
            The subscripts are only needed for reference%
          }
  \labelTable{grammar-rules-example}
\end{table}

Consequently, with a \gls{grammar} the \gls{pattern selection} problem
becomes equivalent to finding a sequence of \gls{rule} applications (called
\glspl!{rule reduction}) that reduces the \gls{expression tree} to a given
\gls{nonterminal}.
%
A method for finding the sequence with least cost is described shortly.



\paragraph{Machine Grammars in Normal Form}

To simplify \gls{pattern matching} and \gls{pattern selection}, a \gls{grammar}
can be rewritten into \gls!{normal form.g}. A \gls{grammar} is in \gls{normal
  form.g} if every \gls{rule} in the \gls{grammar} has a \gls{production} in one
of the following forms:
%
\begin{enumerate}
  \item \mbox{$\mNT{N} \rightarrow \irCode{op} \; \mNT{A}[1] \; \mNT{A}[2]
    \ldots \mNT{A}[n]$}, where \irCode{op} is a \gls{terminal}, representing an
    \gls{operation} that takes $n$ arguments, and all $\mNT{A}[i]$ are
    \glspl{nonterminal}.
    %
    Such rules are called \gls!{base.r}[ \glspl{rule}].
  \item \mbox{$\mNT{N} \rightarrow \irCode{t}$}, where \irCode{t} is a
    \gls{terminal}.
    %
    Such rules are also called \gls{base.r} \glspl{rule}.
  \item \mbox{$\mNT{N} \rightarrow \mNT{A}$}, where $\mNT{A}$ is a
    \gls{nonterminal}.
    %
    Such rules are called \gls!{chain.r}[ \glspl{rule}].
\end{enumerate}

\begin{table}[t]
  \centering%
  \figureFontSize%
  \begin{tabular}{cr@{ $\rightarrow$ }lcl}
    \toprule
    \tabhead \# & \multicolumn{2}{c}{\tabhead Production} & \tabhead Cost
      & \multicolumn{1}{c}{\tabhead Action}\\
    \midrule
    1 & $\mNT{Reg}$ & $\opLoad \; \mNT{A}$
      & 1
      & emit: {\instrFont load \$$\mNT{Reg}$,
                               $A.C.\text{const}$(\$$A.\mNT{Reg}$)}\\
    4 & $\mNT{A}$ & $\opAdd \; \mNT{Reg} \; \mNT{C}$
      & 0
      & \\
    2 & $\mNT{Reg}$ & $\opLoad \; \mNT{B}$
      & 1
      & emit: {\instrFont load \$$\mNT{Reg}$,
                               $B.C.\text{const}$(\$$B.\mNT{Reg}$)}\\
    5 & $\mNT{B}$ & $\opAdd \; \mNT{C} \; \mNT{Reg}$
      & 0
      & \\
    6 & $\mNT{C}$ & $\irCode{const}$
      & 0
      & \\
    3 & $\mNT{Reg}[1]$ & $\opLoad \; \mNT{Reg}[2]$
      & 1
      & emit: {\instrFont load \$$\mNT{Reg}[1]$, 0(\$$\mNT{Reg}[2]$)}\\
    \bottomrule
  \end{tabular}%

  \caption[The grammar from \refTable{grammar-rules-example} in normal form]%
          {%
            The grammar from \refTable{grammar-rules-example} in normal form.
            %
            Nonterminals~$\mNT{A}$,~$\mNT{B}$ and~$\mNT{C}$ and
            rules~\mbox{4--6} are introduced in order to transform rules~1 and~2
            into base rules%
          }
  \labelTable{normal-form-grammar-example}
\end{table}

A \gls{grammar} can be mechanically rewritten into \gls{normal form.g} by
introducing new \glspl{nonterminal} and breaking down illegal \glspl{rule} into
multiple, smaller \glspl{rule} until the \gls{grammar} is in \gls{normal
  form.g}.
%
For example, rewriting the \gls{grammar} shown in
\refTable{grammar-rules-example} into \gls{normal form.g} results in the
\gls{grammar} shown in \refTable{normal-form-grammar-example}.
%
Note that the new \glspl{rule} have zero cost and no action as these are only
intermediary steps towards enabling reduction of the original \gls{rule}.

Since all \glspl{production} in a \glsshort{normal form.g} \gls{grammar} have at
most one \gls{terminal}, the \gls{pattern matching} problem becomes trivial
(simply match the \gls{node} type against the \gls{terminal} in all \gls{base.r}
\glspl{rule}).
%
Otherwise another bottom-up traversal of the \gls{expression tree} would have to
be made in order to find all \glspl{match}, which can be done in linear time for
most reasonable \glspl{grammar}~\cite{HoffmannODonnell:1982}.
%
As we will see, this also simplifies \gls{pattern selection} as the
\glspl{pattern} on the right-hand side in all \glspl{production} have uniform
height.



\paragraph{Optimal Pattern Selection on Expression Trees}

The technique by \citeauthor{AhoEtAl:1989} is centered around the following
assumption: the cost of reducing \gls{node}~$n$ in an \gls{expression tree} to
\gls{nonterminal}~$s$ using \gls{rule}~$r$ is the cost of $r$ plus the costs of
reducing all children of $n$ to the appropriate \glspl{nonterminal} appearing on
the right-hand side of~$r$.
%
If $r$ is a \gls{chain.r} \gls{rule} then the cost is computed as the cost of
$r$ plus the cost of reducing $n$ to the \glsshort{rule result} of~$r$.
%
The recursive nature of these costs can be exploited using dynamic programming,
resulting in the algorithm shown in \refAlgorithm{aho-etal-cost-algorithm} which
computes the least cost of reducing a given \gls{expression tree} to a
particular \gls{nonterminal}.

\begin{algorithm}[p]
  \DeclFunction{ComputeCosts}%
               {expression tree $T$, normal-form grammar $G$}%
  {%
    $S$ \Assign $\mSetBuilder{s}%
                             {\text{$s$ is a nonterminal in $G$}}$\;
    $C$ \Assign matrix of size $|T| \times |S|$, costs initialized to $\infty$\;
    \Call{ComputeCostsRec}{root node of $T$}\;
    \Return{$C$}\;
    \BlankLine
    \DeclFunction{ComputeCostsRec}{node $n$}{%
      \ForEach{child $m$ of $n$}{%
        \Call{ComputeCostsRec}{$m$}\;
      }
      \ForEach{base rule $r \in \text{\Call{FindMatchingRules}{$n$}}$}{%
        $c$ \Assign \Call{ComputeReductionCost}{$n$, $r$}\;
        $l$ \Assign result of $r$\;
        \If{$c < C[n][l]$.cost}{%
          $C[n][l]$.cost \Assign $c$\;
          $C[n][l]$.rule \Assign $r$\;
        }
      }
      \Repeat{no change to $C$}{%
        \ForEach{chain rule $r \in G$}{%
          $c$ \Assign \Call{ComputeReductionCost}{$n$, $r$}\;
          $l$ \Assign result of $r$\;
          \If{$c < C[n][l]$.cost}{%
            $C[n][l]$.cost \Assign $c$\;
            $C[n][l]$.rule \Assign $r$\;
          }
        }
      }
    }

    \BlankLine
    \DeclFunction{FindMatchingRules}{node $n$}{%
      $M$ \Assign $\emptyset$\;
      \ForEach{base rule $r \in G$}{%
        \If{$\text{terminal in pattern of $r$} = \text{node type of $n$}$}{%
          $M$ \Assign $M \cup \mSet{r}$\;
        }
      }
      \Return{$M$}\;
    }

    \BlankLine
    \DeclFunction{ComputeReductionCost}{node $n$, rule $r$}{%
      $c$ \Assign cost of $r$\;
      \eIf{$r$ is a chain rule}{%
        $s$ \Assign nonterminal in pattern of $r$\;
        $c$ \Assign $c$ $+$ $C[n][s]$.cost
        \cmt*{here cost of node itself is taken instead of its children}
      }{%
        \For{$i \leftarrow 1$ \KwTo number of children for $n$}{%
          $m$ \Assign $i$th child of $n$\;
          $s$ \Assign $i$th nonterminal in pattern of $r$\;
          $c$ \Assign $c$ $+$ $C[m][s]$.cost\;
        }
      }
      \Return{$c$}\;
    }
  }

  \caption[%
            Algorithm for computing the optimal sequence of rules that reduces
            the given expression tree to a particular nonterminal%
          ]{%
            Computes the optimal sequence of rules that reduces the given
            expression tree to a particular nonterminal%
          }
  \labelAlgorithm{aho-etal-cost-algorithm}
\end{algorithm}

The algorithm works as follows.
%
It first constructs a cost matrix~$C$, where rows represent \glspl{node} in the
\gls{expression tree} and columns represent \glspl{nonterminal} in the
\gls{grammar}, which is assumed to be in \gls{normal form.g}.%
%
\!\footnote{%
  The algorithm can be adapted to accept any \gls{grammar} by expanding the
  \algStyle{FindMatchingRules} and \algStyle{ComputeReductionCost} functions to
  handle \glspl{rule} of arbitrary form.
}
%
The cost in each element~\mbox{$C[i][j]$} is initialized to infinity, indicating
that there exists no sequence of \glspl{rule reduction} which reduces
\gls{node}~$i$ to \gls{nonterminal}~$j$.
%
It then computes the costs by traversing the \gls{expression tree} bottom up.
%
At each \gls{node}~$n$ and for each matching \gls{base.r} \gls{rule}~$r$, with
\gls{nonterminal}~$s$ as \glsshort{rule result}, it computes the cost~$c$ of
applying $r$ at~$n$ to produce~$s$ according to the scheme stated above.
%
If $c$ is less than the currently recorded cost for reducing $n$ to $s$, then
the cost and \gls{rule} information for $n$ is updated accordingly.
%
The same is then done for all \gls{chain.r} \glspl{rule} until it reaches a
fixpoint (which must eventually be reached as all \gls{rule} costs are
non-negative and an update only occurs when the cost is strictly less).
%
Since every \gls{node} is also only processed once, the algorithm runs in linear
time with respect to the size of the \gls{expression tree}.
%
Having computed the costs, the optimal order of \glspl{rule reduction} -- which
is equivalent to the \gls{least-cost.c} \gls{cover} -- can be found using the
algorithm shown in \refAlgorithm{aho-etal-select-algorithm}.

\begin{algorithm}[t]
  \DeclFunction{Select}{expression tree $T$,
                        goal nonterminal $g$,
                        cost matrix $C$}%
  {%
    $n$ \Assign root node of $T$\;
    $r$ \Assign $C[n][g]$.rule\;
    \eIf{$r$ is a chain rule}{%
      $s$ \Assign result of $r$\;
      \Call{Select}{$T$, $s$, $C$}\;
    }{%
      \For{$i \leftarrow 1$ \KwTo number of children for $n$}{%
        $m$ \Assign $i$th child of $n$\;
        $s$ \Assign $i$th nonterminal in pattern of $r$\;
        \Call{Select}{expression tree rooted at $m$, $s$}\;
      }
    }
    execute actions associated with $r$\;
  }

  \caption[%
            Algorithm for selecting the rules chosen by
            \refAlgorithm{aho-etal-cost-algorithm}%
          ]%
          {%
            Selects optimal sequence of rules that reduces a given expression
            tree to a given nonterminal, based on costs computed
            by \refAlgorithm{aho-etal-cost-algorithm}%
          }
  \labelAlgorithm{aho-etal-select-algorithm}
\end{algorithm}

In many cases, this technique produces code of sufficient quality.
%
In fact, for architectures with simple \glspl{instruction set}, where the
\glspl{rule pattern} can naturally be modeled as trees, it is often optimal or
near optimal.
%
The MIPS \gls{instruction set}~\cite{Sweetman:2006}, for example, is one such
architecture.



\paragraph{Precomputing Costs and Rule Decisions}

Shortly after its introduction, it was recognized that the costs computed by
\refAlgorithm{aho-etal-cost-algorithm} could be precomputed for any
\glspl{expression tree}, thereby replacing the call to the
\algStyle{ComputeReductionCost} function with a table lookup.
%
While this improvement does not affect the asymptotic complexity, it does
significantly reduce the runtime of the algorithm.
%
Although pioneered by \textcite{HatcherChristopher:1986}, the idea was first
successfully applied by \textcite{Pelegri-LlopartGraham:1988}, and later
improved and simplified by \textcite{BalachandranEtAl:1990} and
\textcite{Proebsting:1992:PLDI}.
%
The \gls{expression tree} is traversed bottom up and labeled with a
\gls!{state}.
%
For a given labeled \gls{node}~$n$, the \gls{state} essentially holds enough
information needed to optimally reduce the \gls{expression tree} rooted at~$n$
to any \gls{nonterminal}.
%
At first glance, it would seem that this requires an infinite number of
\glspl{state} since \glspl{expression tree} can be of arbitrary size, but this
only applies if the full cost of the entire \gls{expression tree} is taken into
account.
%
Fortunately, it is sufficient to only consider the relative cost differences
between \glspl{rule} for any given \gls{node} in the \gls{expression tree},
leading to a finite number of \glspl{state}.

The idea for computing the \glspl{state} -- which will only be described briefly
-- works as follows.
%
For each \gls{terminal} representing a $k$-argument \gls{operation}, an
\mbox{$k$-dimensional} matrix is maintained.
%
This is called the \gls{terminal}'s \gls!{state table}, which indicates the
state to assign such \glspl{node} given the labels of its children.
%
First the states for all leaf \glspl{node} are built, considering only
\gls{base.r} \glspl{rule} with a single \glspl{terminal} on the right-hand side
in the \gls{production}.
%
The costs and \gls{rule} decisions are computed using the same logic as in
\refAlgorithm{aho-etal-cost-algorithm}, lines~8--21.
%
The leaf \gls{state} are then pushed onto a queue.
%
Each popped \gls{state} is used as the $i$th child to all \gls{base.r}
\glspl{rule} with a non-leaf \glspl{terminal} in combination with all other
existing \glspl{state}.
%
If any combination gives rise to a new set of costs and \gls{rule} decisions,
then a new \gls{state} is created and pushed onto the queue after having updated
the \glspl{state table}.
%
This process continues until the queue is empty, whereupon all necessary
\glspl{state} have been built.

\begin{algorithm}[t]
  \DeclFunction{Label}%
               {expression tree $T$, list $L$ of state tables}%
  {%
    $n$ \Assign root node of $T$\;
    $k$ \Assign number of children for $n$\;
    \For{$i \leftarrow 1$ \KwTo $k$}{%
      $m_i$ \Assign $i$th child of $n$\;
      \Call{Label}{expression tree rooted at $m_i$, $L$}\;
    }
    $S$ \Assign $L[\text{terminal corresponding to $n$}]$\;
    $n$.label \Assign $S[m_1.\text{label}, \ldots, m_k.\text{label}]$\;
  }

  \caption[%
            Algorithm for labeling an expression tree for optimal pattern
            selection%
          ]{%
            Labels an expression tree for optimal pattern selection%
          }
  \labelAlgorithm{opt-pat-sel-labeling-algorithm}
\end{algorithm}

The algorithm for labeling an \gls{expression tree} is shown in
\refAlgorithm{opt-pat-sel-labeling-algorithm}.
%
To derive the \gls{rule} selection algorithm, one only needs to adapt line~2 in
\refAlgorithm{aho-etal-select-algorithm}.



\paragraph{Limitations}

\begin{filecontents*}{exp-trees-limit-example.c}
t = a + b;
x = c * t;
y = *((int*) t);
\end{filecontents*}

\begin{figure}
  \centering%
  \mbox{}%
  \hfill%
  \subcaptionbox{C code\labelFigure{exp-trees-limit-example-c}}%
                {\lstinputlisting[language=c]{exp-trees-limit-example.c}}%
  \hfill%
  \subcaptionbox{%
                  Instructions. The $*s$ notion means ``get value at address $s$
                  in memory''%
                  \labelFigure{exp-trees-limit-example-instrs}%
                }%
                [50mm]%
                {%
                  \figureFontSize
                  \begin{tabular}{%
                                   >{\instrFont}r@{\hspace{4pt}}%
                                   >{$}l<{$}@{ $\leftarrow$ }%
                                   >{$}l<{$}%
                                   c%
                                 }
                    \toprule
                    \multicolumn{3}{c}{\tabhead Instruction} & \tabhead Cost\\
                    \midrule
                    add     & r & s + t & 2\\
                    mul     & r & s \times t & 3\\
                    addmul  & r & (s + t) \times u & 4\\
                    load    & r & *s & 5\\
                    addload & r & *(s + t) & 5\\
                    \bottomrule
                  \end{tabular}%
                }%
  \hfill%
  \mbox{}%

  \vspace{\betweensubfigures}

  \mbox{}%
  \hfill%
  \subcaptionbox{%
                  Expression trees after edge splitting%
                  \labelFigure{exp-trees-limit-example-trees}%
                }{%
                  \input{figures/existing-isel-techniques-and-reps/exp-trees-limit-example-trees}%
                }%
  \hfill\hfill%
  \subcaptionbox{%
                  Block DAG%
                  \labelFigure{exp-trees-limit-example-dag}%
                }{%
                  \input{figures/existing-isel-techniques-and-reps/exp-trees-limit-example-dag}%
                }%
  \hfill%
  \mbox{}%

  \caption[Example illustrating the limitation of expression trees]%
          {%
            Example illustrating that using block DAGs results in
            better code compared to using expression trees%
          }
  \labelFigure{exp-trees-limit-example}
\end{figure}

The main disadvantage of operating on \glspl{expression tree} is that common
subexpressions have to be either split along the \glspl{edge} or duplicated when
building the \gls{IR}.
%
These transformations are referred to as \gls!{edge splitting} and \gls!{node
  duplication}, respectively, and depending on the \gls{instruction set}, these
decisions can prevent selection of \glspl{instruction} that would lead to higher
code quality.
%
An example illustrating this effect is shown in
\refFigure{exp-trees-limit-example}.
%
If the \gls{IR} is represented as trees, where the common subexpression for
computing \irVar{t} has its own \gls{expression tree} (see
\refFigure{exp-trees-limit-example-trees}), then matches~$m_1$,~$m_2$, and~$m_3$
must be selected, which results in a total cost of \mbox{$2 + 3 + 5 = 10$}.
%
If represented as a \gls{block DAG} (see
\refFigure{exp-trees-limit-example-dag}), it becomes possible of selecting
matches matches~$m_4$ and~$m_5$, resulting in a total cost is \mbox{$4 + 5 =
  9$}.
%
\Glsshort{node duplication}[ing] the \glspl{node} of the common subexpression
would, in this case, yield the same \glspl{cover}, but would have resulted in
suboptimal code in cases where the \instrCode{addmul} and \instrCode{addload}
\glspl{instruction} are not available.




\subsection{Approaches Using Block DAGs}

The cost of improving code quality by moving from \glspl{expression tree} to
\glspl{block DAG} is that \gls{optimal.ps} \gls{pattern selection} becomes an
NP-complete problem~\cite{KoesGoldstein:2008, Proebsting:1995:Proof} (the proof
is also available in \refAppendix{dag-covering}).
%
Consequently, traditional approaches sacrifice optimality in favor of retaining
linear-time complexity.



\paragraph{Maximum Munch}

The most common approach for greedy \gls{pattern selection} on \glspl{block DAG}
is called \gls!{maximum munch} (coined by \textcite{Cattell:1978}).
%
The idea is to traverse the \gls{block DAG} top down, select the largest
\gls{pattern} that matches the current \gls{node}, and repeat the process for
remaining, uncovered parts of the \gls{block DAG}.
%
The approach -- which is used in, for example, LLVM~\cite{LattnerAdve:2004} --
works well for architectures with a regular \gls{instruction set} and where
there is a strong correlation between the effectiveness of the \gls{instruction}
and the size of its \gls{pattern}.
%
In addition to being non-optimal, however, it also suffers from the same
drawback as \glspl{expression tree} regarding whether to select \glspl{cover}
that effectively \glsshort{edge splitting} or \glsshort{node duplication}[e] the
common subexpressions.



\paragraph{Balancing Splitting and Duplication}

Several approaches have been made in attempting to balance \gls{edge splitting}
and \gls{node duplication}.
%
\textcite{FauthEtAl:1994} designed an heuristic algorithm that rewrites the
\gls{block DAG} into \glspl{expression tree} before \gls{instruction
  selection}.
%
Using a rough estimate of cost, the algorithm favors \gls{node duplication} and
resorts to \gls{edge splitting} when the former becomes too costly.
%
Once rewritten, the \glspl{expression tree} are covered using an improved
variant of \refAlgorithm{aho-etal-cost-algorithm}.

\textcite{Ertl:1999} showed that, for certain \glspl{grammar},
\refAlgorithm{aho-etal-cost-algorithm} can be adapted to produce optimal
\glspl{cover} for \glspl{block DAG}.
%
The idea is to first compute the costs for each \gls{node} as if the \gls{block
  DAG} had been rewritten into a \gls{expression tree} using \gls{node
  duplication}.
%
Then, if several \glspl{rule} reduce the same \gls{node} to the same
\gls{nonterminal}~$n$, then $n$ can be shared between the \glspl{rule} whose
\glsplshort{rule pattern} contain $n$.
%
\citeauthor{Ertl:1999} also introduced an algorithm for checking whether
\gls{optimal.ps} \gls{pattern selection} is guaranteed for a given
\gls{grammar}.

\textcite{KoesGoldstein:2008} combined the ideas by \citeauthor{FauthEtAl:1994}
and \citeauthor{Ertl:1999} by introducing a design that first uses
\refAlgorithm{aho-etal-cost-algorithm} to compute the costs for a \glsshort{node
  duplication}[ed] \gls{expression tree}.
%
Then, at each \glspl{node}~$n$ where several \glspl{pattern} in the optimal
\gls{cover} overlap in the \gls{block DAG}, two costs are estimated: the cost
incurred by allowing overlap, and the cost incurred by \glsshort{edge
  splitting}[ting] the \glspl{edge}.
%
If the latter cost is less then $n$ is marked as \gls!{fixed.n}, meaning it can
only be covered by \glspl{pattern} where $n$ is the root \gls{node}.
%
Once all such \glspl{node} have been processed, another pass is performed to
recompute the costs, this time forbidding overlap at \gls{fixed.n} \glspl{node}.



\paragraph{Limitations}

Although using \glspl{block DAG} addresses the issue of whether to
\glsshort{edge splitting} or \glsshort{node duplication}[e] common
subexpressions within a \gls{block}, the problem still remains for expressions
that are spread across multiple \glspl{block}.
%
To fully address this problem, one must resort to \glspl{function graph}.

\begin{filecontents*}{block-dags-limit-example.c}
int f(int* A, int* B, int N) {
  int s = 0;
  for (int i = 0; i < N; i++) {
    s = s + A[i] * B[i];
  }
  return s;
}
\end{filecontents*}

\begin{figure}
  \centering%
  \subcaptionbox{C code\labelFigure{block-dags-limit-example-c}}%
                {\lstinputlisting[language=c]{block-dags-limit-example.c}}%
  \hfill%
  \subcaptionbox{%
                  Block graphs involving variable~\opVar{s}.
                  %
                  For brevity, the subtrees concerning \irCode{A[i]}
                  and \irCode{B[i]} are not included%
                  \labelFigure{block-dags-limit-dags}%
                }%
                [72mm]%
                {%
                  \input{figures/existing-isel-techniques-and-reps/block-dags-limit-example-dags}%
                }

  \vspace{\betweensubfigures}

  \subcaptionbox%
    {%
      Rules.
      %
      For brevity, the actions are not included.
      %
      $\mNT{Null}$ is a dummy nonterminal since \opRet*{} does not return
      anything, yet all productions must have a result.
      %
      All rules are assumed to have equal cost%
    }%
    [\textwidth]%
    {%
      \figureFontSize%
      \newcolumntype{L}{@{}l@{}}%
      \begin{tabular}{r@{ $\rightarrow$ }l@{\hspace{3em}}r@{ $\rightarrow$ }lc}
        \toprule
        \multicolumn{5}{c}{\tabhead Rules}\\
        \midrule
        $\mNT{Reg}$ & \irCode{const}
          & $\mNT{SReg}$
          & \multicolumn{2}{L}{$\opMul \; \mNT{Reg} \; \mNT{Reg}$}\\
        $\mNT{SReg}$ & \irCode{const}
          & $\mNT{Null}$ & \multicolumn{2}{L}{$\opRet* \; \mNT{Reg}$}\\
        $\mNT{Reg}$ & $\opAdd \; \mNT{Reg} \; \mNT{Reg}$
          & $\mNT{Reg}$  & $\mNT{SReg}$ & $(r \ll 1)$\\
        $\mNT{SReg}$ & $\opAdd \; \mNT{SReg} \; \mNT{SReg}$
          & $\mNT{SReg}$ & $\mNT{Reg}$  & $(r \gg 1)$\\
        \bottomrule
      \end{tabular}%
    }

  \caption[Example illustrating the need for global instruction selection]%
          {%
            Example illustrating the need for global instruction selection%
          }
  \labelFigure{block-dags-limit-example}
\end{figure}

This also applies to other situations where decisions made for one \gls{block}
can inhibit subsequent decisions for other \glspl{block}, such as enforcing
specific storage locations or value modes.
%
For example, \refFigure{block-dags-limit-example} shows a \gls{function} that
multiplies the elements of two arrays and sums the results.
%
Assume that the arrays consist of fixed-point values.
%
For efficiency, a common idiosyncracy in many \glspl{DSP} is that multiplication
of two fixed-point values return a value that is shifted one bit to the left.
%
For such \glspl{target machine}, both the value~\irCode{0} and the accumulator
\gls{variable}~\irVar{s} should be in shifted mode throughout the entire
\gls{function}, and only restored into normal mode upon return.
%
Otherwise the accumulated value would be needlessly shifted back and forth
within the loop.
%
Achieving this, however, is difficult when limited to covering only a single
\gls{block DAG} at a time.
%
Assume for example that the function had no multiplication.
%
In that case, deciding to load value~\irCode{0} in shifted mode would instead
lower code quality as the value would needlessly have to be shifted back before
returning, which takes an extra \gls{instruction}.

Lastly, most of these approaches are restricted to tree-shaped \glspl{pattern},
meaning they only support \glspl{instruction} that produce a single value.
%
Many \glspl{instruction set}, however, contain \glspl{instruction} that produce
multiple values, such as memory \glspl{instruction} that load a value from
memory while simultaneously incrementing the address.
%
Modeling such \glspl{instruction} require \gls{DAG}-shaped \glspl{pattern},
which violate underlying assumptions made by many of the aforementioned
approaches.



\subsection{Approaches Using Function Graphs}

To the best of my knowledge, only \textcite{PalecznyEtAl:2001} have attempted to
apply \glsshort{traditional approach} methods on \glspl{function graph}.
%
The approach represents \glspl{function} using a graph-based \gls{IR}, which
will therefore be discussed first.



\paragraph{Sea-of-nodes IRs}

\begin{filecontents*}{fact.c}
int fact(int n) {
  entry:
    int f = 1;
  head:
    if (n <= 1) goto end;
  body:
    f = f * n;
    n = n - 1;
    goto head;
  end:
    return f;
}
\end{filecontents*}

\begin{filecontents*}{fact-ssa.c}
int fact(int $\irVar{n}[1]$) {
  entry:
    int $\irVar{f}[1]$ = 1;
  head:
    int $\irVar{f}[2]$ = $\mPhi$($\irVar{f}[1]$:entry, $\irVar{f}[3]$:body);
    int $\irVar{n}[2]$ = $\mPhi$($\irVar{n}[1]$:entry, $\irVar{n}[3]$:body);
    if ($\irVar{n}[2]$ <= 1) goto end;
  body:
    int $\irVar{f}[3]$ = $\irVar{f}[2]$ * $\irVar{n}[2]$;
    int $\irVar{n}[3]$ = $\irVar{n}[2]$ - 1;
    goto head;
  end:
    return $\irVar{f}[2]$;
}
\end{filecontents*}

\begin{figure}
  \centering%
  \mbox{}%
  \hfill%
  \subcaptionbox{%
                  C implementation of factorial%
                  \labelFigure{sea-of-nodes-example-c}%
                }%
                [50mm]%
                {\lstinputlisting[language=c]{fact.c}}%
  \hfill%
  \subcaptionbox{Code in SSA form\labelFigure{sea-of-nodes-example-ssa-c}}%
                {\lstinputlisting[language=c,mathescape]{fact-ssa.c}}%
  \hfill%
  \mbox{}

  \vspace*{\betweensubfigures}

  \subcaptionbox{SSA graph\labelFigure{sea-of-nodes-example-ssa-graph}}%
                {%
                  \input{figures/existing-isel-techniques-and-reps/sea-of-nodes-example-ssa-graph}%
                }

  \vspace*{\betweensubfigures}

  \subcaptionbox{%
                  Click-Paleczny graph%
                  \labelFigure{sea-of-nodes-example-click-paleczny-graph}%
                }{%
                  \input{figures/existing-isel-techniques-and-reps/sea-of-nodes-example-click-paleczny-graph}%
                }

  \caption[%
            Example of an SSA graph and a Click-Paleczny graph%
          ]%
          {%
            Example of an SSA graph and a Click-Paleczny graph.
            %
            Thin-lined nodes and edges denote data operations and data flow.
            %
            Thick-lined nodes and edges denote control operations and control
            flow.
            %
            Dashed edges indicate to which block an operation belongs%
          }
  \labelFigure{sea-of-nodes-example}
\end{figure}

Graph-based \glspl{IR} that represent entire \glspl{function} are colloquially
referred to as \glspl!{sea-of-nodes IR}.
%
One such \gls{IR} was introduced by \textcite{ClickPaleczny:1995} which, for
lack of better name, will be referred to as the \gls!{Click-Paleczny graph}.

The \gls{Click-Paleczny graph} assumes the \gls{function} to be in \gls!{SSA}
\glsEmph{form}~\cite{CytronEtAl:1991}, in which a \gls{variable} is defined
exactly once.
%
For example, the \gls{function} shown in \refFigure{sea-of-nodes-example-c}
is not in \gls{SSA}~form as \glspl{variable}~\irCode{f} and~\irCode{n} are
redefined within the loop.
%
By introducing new \glspl{variable} and connecting these using
\glspl!{phi-function} where the value depends on control flow, the
\gls{function} can be rewritten into \gls{SSA}~form, as shown in
\refFigure{sea-of-nodes-example-ssa-c}.

Like in \glspl{data-flow graph}, each \gls{operation} in the SSA-based
\gls{function} is represented as a \gls{node}.
%
These nodes are connected using data-flow \glspl{edge}, ignoring the fact that
the \glspl{operation} may belong to different \glspl{block}.
%
This forms a \gls{data-flow graph} called the \gls!{SSA
  graph}~\cite{GerlekEtAl:1995} (see
\refFigure{sea-of-nodes-example-ssa-graph}).
%
The control flow is captured using \glspl{node} to represent the \glspl{block}
in the \gls{function} and \glspl{edge} to represent jumps between \glspl{block}.
%
Conditional jumps flow through special \emph{if}~\glspl{node}, thereby allowing
dependencies between data and control flow to be captured.
%
An example is shown in \refFigure{sea-of-nodes-example-click-paleczny-graph}.

Since \glspl{operation} are not pre-assigned to specific \glspl{block}, the same
\gls{IR} can be used for performing \gls{global code motion}.
%
If an \gls{operation} should not be allowed to be moved to another \gls{block}
-- like with \glspl{phi-function} and returns -- then an \gls{edge} can be used
to connect the \gls{operation} with its original \gls{block} placement.



\paragraph{Pattern Selection on a Sea of Nodes}

\citeauthor{PalecznyEtAl:2001} first divides the \gls{function graph} into a set
of possibly overlapping \glspl{expression tree}.
%
This is done by labeling certain \glspl{node} in the \gls{function graph} as
tree roots.
%
Root candidates are \glspl{node} representing \glspl{operation} whose result are
shared or \glspl{operation} with side effects and may therefore not be
\glsshort{node duplication}[ed].
%
The selection of roots is geared towards duplicating address computations and
other expressions that can be subsumed into a single \gls{instruction}.

Once labeled, each \gls{expression tree} is covered using a variant of
\refAlgorithm{opt-pat-sel-labeling-algorithm}.
%
The \glspl{instruction} are then emitted and placed in \glspl{block} using a
method described in~\cite{Click:1995}.



\section{Combinatorial Approaches}
\labelSection{combinatorial-approaches}

A trait exhibited by \glspl{combinatorial approach} is that, unlike
\glspl{traditional approach}, they first create a model that describes the
\gls{instruction selection} problem and then solve the model.
%
In doing so, they may choose between applying either complete solving methods --
for optimality -- or greedy heuristics -- for tractability.

Since there already exist methods for optimally covering \glspl{expression tree}
in linear time, nearly all \glspl{combinatorial approach} operate on
\glspl{block DAG}, with one exception operating on \glspl{function graph}.
%
The approaches in this section are therefore grouped according to the tools they
apply for modeling \gls{instruction selection}.

Most approaches also natively support \gls{DAG}-shaped \glspl{pattern}, but
finding \glspl{match} for such \glspl{pattern} can no longer be done in linear
time.
%
Consequently, they typically apply generic \glshyphened{subgraph isomorphism}
algorithms when \gls{pattern matching}.



\paragraph{Pattern Matching as a Subgraph Isomorphism Problem}

\def\mGP{G_{\textsc{p}}}
\def\mGF{G_{\textsc{f}}}
\def\mNP{N_{\textsc{p}}}
\def\mNF{N_{\textsc{f}}}
\def\mEP{E_{\textsc{p}}}
\def\mEF{E_{\textsc{f}}}

The \gls!{subgraph isomorphism} problem, which is known to be
NP-complete~{\cite{Cook:1971}}, is to find instances where a
\gls{graph}~\mbox{$\mGP = \mPair*{\mNP}{\mEP}$} is \glsshort{isomorphism} to a
\gls{subgraph} in another \gls{graph}~\mbox{$\mGF = \mPair*{\mNF}{\mEF}$}.
%
$\mGP$ is \glsshort!{isomorphism} to $\mGF$ if and only if there exists a
mapping \mbox{$\mFunDecl{m}{\mNP}{\mNF}$} such that \mbox{$\mPair*{n}{o} \in
  \mEP$} implies \mbox{$\mPair*{f(n)}{f(o)} \in \mEF$}.
%
In this context, $\mGP$ denotes the \gls{pattern}, $\mGF$ denotes the \gls{block
  DAG} or \gls{function graph}, and $m$ denotes a \gls{match}.

As \gls{subgraph isomorphism} appears in many other fields, much research has
been devoted to this problem (see for example \cite{Ullmann:1976,
  CordellaEtAl:2001, GuoEtAl:2003, KrissinelHenrick:2004, SorlinSolnon:2004,
  Gallagher:2006, FanEtAl:2010, FanEtAl:2011, HinoEtAl:2012, McCreesh:2017}).
%
Due to its simplicity, most approaches apply the algorithm given in
\cite{CordellaEtAl:2001}, of which a detailed description is given in
\refChapter{pattern-matching}.



\subsection{MIS- and MWIS-based Approaches}

One approach to modeling \gls{instruction selection} is to model it as a problem
of finding \glspl{independent set}.
%
Given a \gls{graph}~\mbox{$G = \mPair{N}{E}$}, a set \mbox{$S \subseteq N$} is
an \gls!{independent set} if no pairs of nodes~\mbox{$m, n \in S$} are adjacent
in~$G$.
%
An \gls{independent set} is called a \gls!{MIS} if no more \glspl{node} can be
added and still be an \gls{independent set}.
%
If each \gls{node} in the graph has a weight, then a \gls!{MWIS} is a \gls{MIS}
that maximizes/minimizes \mbox{$\sum_n \mWeight(n)$}.
%
In general, finding a \gls{MIS} or \gls{MWIS} is
NP-complete~\cite{GareyJohnson:1979}.

Modeling \gls{instruction selection} as either a \gls{MIS} or \gls{MWIS} problem
is done as follows.
%
After \gls{pattern matching}, an \gls!{interference graph} is constructed where
a \gls{node} represents a \gls{match} and an \gls{edge} represents overlapping
between two \glspl{match}.
%
If all \glspl{operation} in the \gls{block DAG} or \gls{function graph} can be
covered by at least one \gls{match}, then a \gls{MIS} of the \gls{interference
  graph} corresponds to a \gls{cover}.
%
Likewise, a \gls{MWIS} corresponds to a \gls{least-cost.c} \gls{cover}.
%
An example is shown in \refFigure{mis-example}.

\begin{figure}
  \centering%
  \mbox{}%
  \hfill%
  \subcaptionbox{%
                  Block DAG%
                  \labelFigure{mis-example-dag}%
                }{%
                  \input{figures/existing-isel-techniques-and-reps/mis-example-dag}%
                }%
  \hfill%
  \subcaptionbox{%
                  Interference graph%
                  \labelFigure{mis-example-int-graph}%
                }%
                [32mm]%
                {%
                  \input{figures/existing-isel-techniques-and-reps/mis-example-int-graph}%
                }%
  \hfill%
  \mbox{}

  \caption[Example of modeling instruction selection as a MIS problem]%
          {%
            Example of modeling instruction selection as a MIS problem.
            %
            Valid maximal independent sets of the interference graph are
            \mbox{$\mSet{m_1, m_2, m_4}$}, \mbox{$\mSet{m_2, m_5}$}, and
            \mbox{$\mSet{m_3, m_4}$}, which correspond to valid covers of the
            block DAG%
          }
  \labelFigure{mis-example}
\end{figure}



\paragraph{Applications}

\textcite{ScharwaechterEtAl:2007} appears to have pioneered the modeling of
\gls{instruction selection} as a \gls{MWIS} problem, although the main
contribution of their paper is the extension of \glspl{machine grammar} to
handle \glspl{instruction} that produce multiple results.
%
The idea is to model such \glspl{instruction} using \gls!{complex.r}[
  \glspl{rule}], which each consists of multiple \glspl{production} -- one for
every result.
%
\labelPage{extended-machine-grammars}
%
In this dissertation, such \glspl{production} and their \glspl{pattern} are
called \gls!{proxy.r}[ \glspl{rule}]%
%
\footnote{%
  In the original paper, they are called \gls!{split.r}[ \glspl{rule}].
  %
  This is also is inconsistent with the terminology defined
  on \refPage{machine-grammars}, but we forsake consistency here in favor of
  simplicity.
}
%
and \gls!{proxy.p}[ \glspl{pattern}], respectively, whereas \glspl{rule} with a
single \gls{production} and their \glspl{pattern} are called \gls!{simple.r}[
  \glspl{rule}] and \gls!{simple.p}[ \glspl{pattern}], respectively.
%
The \gls{rule} structure is also illustrated in
\refFigure{extended-machine-grammar-rule-anatomy}.

\begin{figure}
  \centering%
  \figureFont\figureFontSize%
  \newcommand{\simplePatternText}{%
    \trimbox{0 4pt 0 4pt}{%
      \begin{tabular}{@{}c@{}}
        simple\\[-1ex]
        pattern
      \end{tabular}%
    }%
  }
  \newcommand{\proxyPatternText}{%
    \trimbox{0 4pt 0 4pt}{%
      \begin{tabular}{@{}c@{}}
        proxy\\[-1ex]
        pattern
      \end{tabular}%
    }%
  }%
  \begin{displaymath}
    \underbrace{
      \mNT{A}
      \rightarrow
      \overbrace{
        \irCode{op} \ldots
      }^{\text{\simplePatternText}}
      \quad
      \text{cost}
      \quad
      \text{action}
    }_{\text{simple rule}}
    \qquad
    \underbrace{
      \langle \mNT{A}, \mNT{B}, \ldots \rangle
      \rightarrow
      \overbrace{
        \langle
          \:
          \overbrace{\irCode{op} \ldots}^{\text{\proxyPatternText}},
          \:
          \overbrace{\irCode{op} \ldots}^{\text{\proxyPatternText}},
          \:
          \ldots
          \:
        \rangle
      }^{\text{complex pattern}}
      \quad
      \text{cost}
      \quad
      \text{action}
    }_{\text{complex rule}}
  \end{displaymath}

  \vspace*{-\baselineskip}

  \caption{Anatomy of simple and complex rules in an extended machine grammar}
  \labelFigure{extended-machine-grammar-rule-anatomy}
\end{figure}

The \gls{proxy.p} \glspl{pattern} are matched individually together with the
\gls{simple.p} \glspl{pattern}.
%
After \gls{pattern matching}, \glspl{match} derived from \gls{proxy.p}
\glspl{pattern} are then either combined -- indicating use of the
\gls{complex.r} \gls{rule} -- or kept -- indicating use of \glspl{rule} that
only produce a single result.
%
This is done according to a heuristic that estimates the cost saved by using the
\gls{complex.r} \gls{rule} versus the cost incurred by having to \glsshort{node
  duplication}[e] \glspl{node} in common subexpressions.
%
Once these decisions have been taken, the \gls{interference graph} is built and
the \gls{MWIS} found using a greedy heuristic~\cite{SakaiEtAl:2003}.

The approach was later extended by \textcite{AhnEtAl:2009} to include scheduling
dependency conflicts between \gls{complex.p} \glspl{pattern} in order to
facilitate \gls{register allocation}.
%
Also, a shortcoming of both designs -- that \gls{complex.r} \glspl{rule} can
only consist of disconnected \glspl{pattern}, hence forbidding sharing of values
across the \gls{proxy.p} \glspl{pattern} -- was addressed by
\textcite{YounEtAl:2011} by introducing the use of index subscripts for the
\glspl{node} representing the input arguments.



\subsection{PBQP-based Approaches}

Another method of modeling \gls{instruction selection} is to model it as a
\gls!{PBQP}.
%
First introduced by \textcite{ScholzEckstein:2002} to model and solve
\gls{register allocation}, \gls{PBQP} is a variant of the \gls!{QAP}, which is a
fundamental combinatorial optimization problem in the operations research field
(see \cite{LoiolaEtAl:2007} for a survey).
%
Although both problems are NP-complete in general, a subclass of \gls{PBQP} can
be solved in linear time which inspired \citeauthor{ScholzEckstein:2002} in
developing a greedy, linear-time solver.

A \gls{PBQP} is defined as follows:
%
\begin{equation}
  \begin{array}{rll}
    \text{minimize}
      & \multicolumn{2}{c}{
          \displaystyle
          \sum_{1 \leq i < j \leq n} \hspace*{-.5em}
          \mVar{x}_i^T C_{ij} \, \mVar{x}_j +
          \sum_{1 \leq i \leq n} \mVector{c}_i^T \mVar{x}_i,
        } \\
    \text{subject to}
      & \mVar{x}_i \in \mSet{0, 1}^{|c_i|}\!,
      & \forall 1 \leq i \leq n, \\
    \text{and}
      & \mVector{1}^T \mVar{x}_i = 1,
      & \forall 1 \leq i \leq n. \\
  \end{array}
\end{equation}
%
Intuitively, one can interpret this definition as follows.
%
Assume a problem consists of $n$~decisions, each with $k$~choices.
%
Then $\mVar{x}_i$ is a \gls!{decision variable} with $k$~elements, where
\mbox{$\mVar{x}_i[j] = 1$} means that choice~$j$ has been selected for
decision~$i$.
%
For each \gls{decision variable}, the condition
\raisebox{0pt}[\height-2pt]{$\mVector{1}^T \mVar{x}_i = 1$} ensures that exactly
one choice is selected.
%
The cost of selecting a particular choice for decision~$i$ is represented
through a cost vector~$\mVector{c}_i$, and the cost of combining two
decisions~$i$ and~$j$ are represented through a cost matrix~$C_{ij}$.

In this context, $\mVar{x}_i$ decides whether to select a particular \gls{match}
to cover \gls{node}~$i$, $\mVector{c}_i$ contains the cost for each such
\gls{match}, and $C_{ij}$ contains the cost of additional \glspl{instruction}
that may need to be selected due to certain combinations of \glspl{match}.
%
For example, assume two nodes~$i$ and~$j$ where $j$ depends on $i$.
%
Assume further that the \glspl{instruction} are represented as a \gls{machine
  grammar}, and that~$i$ and~$j$ can be covered using two \glspl{rule}, $r_i$
and $r_j$, with \glspl{production} \mbox{$\mNT{A} \rightarrow
  \irCode{op}_{\mathit{i}} \; \mNT{A} \; \mNT{A}$} and \mbox{$\mNT{B}
  \rightarrow \irCode{op}_{\mathit{j}} \; \mNT{B} \; \mNT{B}$}, respectively.
%
Since the result of $r_i$ does not match the operands of $r_j$, this \gls{rule}
combination requires a \gls{chain.r} \gls{rule} -- or a chain of these, if
necessary -- that derives $\mNT{B}$ from $\mNT{A}$.
%
Illegal combinations are prevented by assigning infinite cost.
%
An example of a \gls{PBQP} instance is shown in \refFigure{pbqp-example}.

\begin{figure}
  \centering%
  \mbox{}%
  \hfill%
  \subcaptionbox%
    {%
      Rules.
      %
      For brevity, the actions are not included%
    }%
    [68mm]%
    {%
      \figureFontSize%
      \begin{tabular}{r@{ $\rightarrow$ }lc}
        \toprule
        \multicolumn{2}{c}{\tabhead Rules} & \tabhead Cost\\
        \midrule
        $\mNT{Reg}$ & \irCode{var} & 0\\
        $\mNT{Reg}$ & $\opAdd \; \mNT{Reg} \; \mNT{Reg}$ & 1\\
        $\mNT{Reg}$ & $\opLoad \; \mNT{Addr}$ & 3\\
        $\mNT{Reg}$ & $\opLoad \; \opAdd \; \mNT{Reg} \; \mNT{Reg}$ & 5\\
        $\mNT{Addr}$ & $\mNT{Reg}$ & 2\\
        \bottomrule
      \end{tabular}%
    }%
  \hfill%
  \subcaptionbox{%
                  SSA graph%
                  \labelFigure{pbqp-example-ssa-graph}%
                }{%
                  \input{figures/existing-isel-techniques-and-reps/pbqp-example-ssa-graph}%
                }%
  \hfill%
  \mbox{}

  \vspace{\betweensubfigures}

  \subcaptionbox{%
                  PBQP instance.
                  %
                  The rows and columns in the cost vectors and matrices are
                  labeled with the matches they represent.
                  %
                  Cost matrices for uninteresting combinations are assumed to
                  consist of~0s%
                  \labelFigure{pbqp-example-instance}%
                }{%
                  \figureFontSize%
                  \apptocmd{\irFont}{\scriptsize}{}{}%
                  \newcommand{\lskip}{0.1ex}%
                  \newcommand{\newlineskip}{-1em}%
                  \newcolumntype{R}{r@{\;=\;}}%
                  \BAnewcolumntype{C}{>{\;}c<{\:}}%
                  \BAnewcolumntype{X}{@{\;\;\,}c}%
                  \newcommand{\mlabel}[1]{\raisebox{.2ex}{$\scriptstyle #1$}}%
                  \begin{minipage}{22mm}%
                    \begin{displaymath}
                      \begin{array}{r@{\;\in\;}l}
                        \mVar{x}_{\opVar{a}} & \mSet{0, 1}    \\[.7ex]
                        \mVar{x}_{\opVar{b}} & \mSet{0, 1}    \\[.7ex]
                        \mVar{x}_{\opAdd}   & \mSet{0, 1}^{2} \\[.7ex]
                        \mVar{x}_{\opLoad}  & \mSet{0, 1}^{2}
                      \end{array}
                    \end{displaymath}%
                  \end{minipage}%
                  \hspace{5mm}%
                  \trimbox{0 8pt 0 0}{%
                    \begin{minipage}{27mm}%
                      \begin{displaymath}
                        \begin{array}{Rl}
                          \mVector{c}_{\opVar{a}}
                            & \begin{adjblockarray}{cc}{1.3ex}
                                \begin{block}{[C]X}
                                  0 & \mlabel{m_1} \\
                                \end{block}
                              \end{adjblockarray} \\[1.5ex]
                          \mVector{c}_{\opVar{b}}
                            & \begin{adjblockarray}{cc}{1.3ex}
                                \begin{block}{[C]X}
                                  0 & \mlabel{m_2} \\
                                \end{block}
                              \end{adjblockarray} \\[1.5ex]
                          \mVector{c}_{\opAdd}
                            & \begin{adjblockarray}{cc}{1.3ex}
                                \begin{block}{[C]X}
                                  1 & \mlabel{m_3} \\
                                  5 & \mlabel{m_5} \\
                                \end{block}
                              \end{adjblockarray} \\[3ex]
                          \mVector{c}_{\opLoad}
                            & \begin{adjblockarray}{cc}{1.3ex}
                                \begin{block}{[C]X}
                                  3 & \mlabel{m_4} \\
                                  5 & \mlabel{m_5} \\
                                \end{block}
                              \end{adjblockarray}
                        \end{array}
                      \end{displaymath}%
                    \end{minipage}%
                  }%
                  \hspace{5mm}%
                  \begin{minipage}{33mm}%
                    \begin{displaymath}
                      \begin{array}{Rc}
                        C_{\opVar{a} \opAdd}
                          & \begin{adjblockarray}{ccc}{-0.1ex}
                              \scriptstyle m_3 & \scriptstyle m_5 & \\[\lskip]
                              \begin{block}{[cc]X}
                                0 & 0 & \mlabel{m_1} \\
                              \end{block}
                            \end{adjblockarray} \\[-2.2ex]
                        C_{\opVar{b} \opAdd}
                          & \begin{adjblockarray}{ccc}{-0.1ex}
                              \scriptstyle m_3 & \scriptstyle m_5 & \\[\lskip]
                              \begin{block}{[cc]X}
                                0 & 0 & \mlabel{m_1} \\
                              \end{block}
                            \end{adjblockarray} \\[-2.2ex]
                        C_{\opAdd\opLoad}
                          & \begin{adjblockarray}{ccc}{0ex}
                              \scriptstyle m_4 & \scriptstyle m_5 & \\[\lskip]
                              \begin{block}{[cc]X}
                                2      & \infty & \mlabel{m_3} \\
                                \infty & 0      & \mlabel{m_5} \\
                              \end{block}
                            \end{adjblockarray}
                      \end{array}
                    \end{displaymath}%
                  \end{minipage}%
                }

  \caption{Example of modeling instruction selection as a PBQP}
  \labelFigure{pbqp-example}
\end{figure}



\paragraph{Handling DAG-shaped Patterns}

The \gls{PBQP} model above assumes that all \glspl{pattern} are shaped as trees.
%
To handle \gls{DAG}-shaped \glspl{pattern}, the model must be extended.
%
First assume an extended \gls{grammar} where multi-output \glspl{instruction}
are described using \gls{complex.r} \glspl{rule} (described
on \refPage{extended-machine-grammars}, see also
\refFigure{extended-machine-grammar-rule-anatomy}).
%
For each combination of \glspl{match} derived from \gls{proxy.r} \glspl{rule}
that can be combined into an instance of a \gls{complex.r} \gls{rule}, a
\gls!{complex.m}[ \gls{match}] is created.
%
Each \gls{complex.m} \gls{match}~$i$ in turn introduces a \gls{decision
  variable}~\raisebox{0pt}[\height-2pt]{$\mVar{x}_i \in \mSet{0, 1}^2$} to
decide whether $i$ is selected.
%
Because of the \raisebox{0pt}[\height-2pt]{$\mVector{1}^T \mVar{x}_i = 1$}
condition, every such \gls{variable} has exactly two elements (one representing
\emph{on} and the other \emph{off}).
%
Like with the \gls{simple.r} \glspl{rule}, the costs of selecting a
\gls{complex.m} \gls{rule} and interactions between these -- for example, two
\gls{complex.m} \glspl{match} are not allowed to overlap or cause cyclic data
dependencies -- are represented through the cost vectors and matrices.

In order to select a \gls{complex.r} \gls{rule}, all of its \gls{proxy.r}
\glspl{rule} must also be selected.
%
This is achieved by first extending, for each node~$i$, the domain of its
\gls{decision variable}~$\mVar{x}_i$ with \glspl{match} derived from
\gls{proxy.r} \glspl{rule}.
%
Then a new set of cost matrices~$D_{ij}$ is created such that, for a node~$i$
and \gls{complex.m} \gls{match}~$j$, the costs are 0 if \mbox{$\mVar{x}_j =
  \text{\emph{off}}$} or $\mVar{x}_i$ is set to a \gls{proxy.r} \gls{rule}
associated with $j$.
%
Otherwise the costs are $\infty$.
%
Consequently, if a \gls{complex.m} \gls{match} covering some node~$n$ is
selected, then the only choice for $\mVar{x}_n$ with non-infinite cost is an
associated \gls{proxy.r} \gls{rule}.
%
The \gls{PBQP} model is thus augmented with another sum
%
\begin{equation}
  \sum_{i \in N, j \in M}
  \hspace*{-.5em}
  \mVar{x}_i^T D_{ij} \, \mVar{x}_j
\end{equation}
%
where $N$ denotes the set of \glspl{node} in the \gls{SSA graph} and $M$ denotes
the set of \gls{complex.m} \glspl{match}.

This alone, however, allows \glspl{solution} where all \gls{proxy.r}
\glspl{rule} but none of the \gls{complex.r} \glspl{rule} are selected.
%
This is resolved by assigning an artificially large cost~$K$ to the selection of
\gls{proxy.r} \glspl{rule}, which is offset when selecting the corresponding
\gls{complex.r} \gls{rule}.
%
For example, if a \gls{complex.r} \gls{rule}~$r$ with cost~2 consists of three
\gls{proxy.r} \glspl{rule}, then the new cost of selecting $r$ is \mbox{$2 -
  3K$}.



\paragraph{Applications}

\textcite{EcksteinEtAl:2003} were first with modeling \gls{instruction
  selection} as a \gls{PBQP}, and \textcite{EbnerEtAl:2008} extended their
approach to support \gls{DAG}-shaped \glspl{pattern}.
%
\textcite{BuchwaldZwinkau:2010} reused the \gls{PBQP} model but replaced the use
of \glspl{machine grammar} with rewrite rules based on algebraic graph
transformations~\cite{LoweEhrig:1991}.



\subsection{IP-based Approaches}

Several approaches model \gls{instruction selection} using \gls!{IP} -- often
also referred to as \gls!{integer linear programming} -- which is a method for
solving combinatorial optimization problems (see \cite{Wolsey:1998} for an
overview).
%
An \gls{IP} problem is defined as follows:
%
\begin{equation}
  \begin{array}{rl}
    \text{maximize} &
      \multirow{2}{*}{$\mVector{c}^T \mVar{x}$} \\
    \text{or minimize} & \\
    \text{subject to} & A \mVar{x} \leq \mVector{b}, \\
      & A \mVar{x} \in \mathbb{Z}^{n \times n}\!, \\
    \text{and} & \mVar{x} \in \mathbb{N}^n\!, \\
  \end{array}
\end{equation}
%
where $\mVector{c}$ and $\mVector{b}$ are integer vectors, $A$ is an integer
matrix, and $\mVar{x}$ is a set of integer \glspl{decision variable}.
%
Such problems are NP-complete in general, but extensive research in the field
has made \gls{IP} a practical tool for solving problems containing tens of
thousands of \glspl{variable}.

In most \gls{IP}-based approaches, \gls{pattern selection} is modeled as
%
\begin{equation}
  \sum_{\substack{m \in M, \\ n \in \mCovers(m)}}
  \hspace*{-1em}
  \mVar{x}_m = 1, \forall n \in N,
  \labelEquation{pattern-selection-in-ip}
\end{equation}
%
where $N$ denotes the set of nodes in a \gls{block DAG}, $M$ denotes the
\gls{match set}, $\mCovers(m)$ denotes the set of \glspl{node} covered by
match~$m$, and $\mVar{x}_m$ is a Boolean \gls{decision variable} indicating
whether match~$m$ is selected.



\paragraph{Applications}

Although mostly known for their work in \gls!{integrated.cg}[ \gls{code
    generation}] -- meaning \gls{instruction selection}, \gls{instruction
  scheduling} and \gls{register allocation} is solved in unison --
\textcite{WilsonEtAl:1994} also pioneered the use of \gls{IP} for modeling
\gls{instruction selection}.
%
Their design performs \gls{global.is} \gls{instruction selection} on a
\gls{function graph} that has been augmented with additional copy
\glspl{operation} to represent potential \gls!{register spilling}.%
%
\footnote{If there are not enough \glspl{register} to allocate for all
  \gls{program} \glspl{variable}, then one or more \glspl{register} must be
  freed.
  %
  This is done by spilling their values to memory, which must then be copied
  back before use.%
}
%
In most cases, not all copies are needed and therefore not all \glspl{operation}
must be covered.
%
Consequently, \citeauthor{WilsonEtAl:1994} model \gls{pattern selection} as
\mbox{$\sum \mVar{x}_m \leq 1$}.

Another approach for \gls{integrated.cg} \gls{code generation} was made by
\textcite{BednarskiKessler:2006}, whose design was later reused by
\citeauthor{ErikssonEtAl:2008}~\cite{ErikssonEtAl:2008, ErikssonKessler:2012}.
%
Unlike all other \gls{instruction selection} approaches,
\citeauthor{BednarskiKessler:2006} combine \gls{pattern matching} and
\gls{pattern selection}.
%
Consequently, in addition to the \glspl{variable} that decide which
\glspl{match} are selected, their \gls{IP}~model also has \glspl{decision
  variable} that, for each \gls{match}, maps \glspl{node} and \glspl{edge} in
the \gls{block DAG} to \glspl{node} and \glspl{edge} in a \gls{pattern}.
%
An upper bound on the number of \glspl{match} needed is computed using a
heuristic.

An \gls{IP}-based approach for selecting \gls{instruction} with multiple output
was introduced by \textcite{LeupersMarwedel:1995, LeupersMarwedel:1996}.
%
Each \gls{DAG}-shaped \gls{pattern} of a multi-output \gls{instruction} is first
decomposed into trees, which are used for covering an \gls{expression tree}.
%
After having found a \gls{least-cost.c} \gls{cover} (using
\refAlgorithm{opt-pat-sel-labeling-algorithm}), the \gls{expression tree} is
collapsed into a tree of \glspl!{super node}, where each \gls{super node}
represents a set of \glspl{node} in the \gls{expression tree} covered by the
same \gls{pattern}.
%
The problem is then to try to merge \glspl{super node} such that the combination
can be implemented using a multi-output \gls{instruction}.
%
This is often called \gls!{instruction compaction}, and as there is an abundance
of overlap between such combinations, \citeauthor{LeupersMarwedel:1995} solved
this problem using \gls{IP}.

\textcite{Leupers:2000} later introduced another \gls{IP}-based approach for
selecting \gls{SIMD.instr} \glspl{instruction}.
%
Again, each \gls{DAG}-shaped \gls{pattern} of a \gls{SIMD.instr}
\gls{instruction} is decomposed into trees, but in this approach they are used
for covering a \gls{block DAG} that has been transformed into \glspl{expression
  tree} through \gls{edge splitting}.
%
Since the potential of using \gls{SIMD.instr} \glspl{instruction} can be
increased by allowing them to cover \glspl{node} from multiple \glspl{expression
  tree}, covering each tree individually often leads to suboptimal code.
%
To address this problem, \citeauthor{Leupers:2000} first extended a \gls{machine
  grammar} with additional \glspl{nonterminal} to indicate whether a
\gls{SIMD.instr} \gls{instruction} is used in covering a particular \gls{node}
and then modified \refAlgorithm{opt-pat-sel-labeling-algorithm} to compute the
all optimal \glspl{cover} instead of a single solution.
%
Once all \gls{least-cost.c} \glspl{cover} have been found for all
\glspl{expression tree} in a \gls{block}, an \gls{IP}~model is built to decide
how to make the best use of \gls{SIMD.instr} \glspl{instruction} for this
\gls{block}.
%
\textcite{Leupers:2000}'s model was later extended by \textcite{TanakaEtAl:2003}
to take the cost of data transfers into account by extending the \gls{block DAG}
with additional copy \glspl{node}, which is needed for architectures with
irregular \glspl{instruction set}.

The last \gls{IP}-based approach to be discussed is that of
\textcite{Gebotys:1997}, who applied to the theory of \glspl{Horn clause} to
\gls{code generation}.
%
A \gls!{Horn clause} is a disjunctive Boolean formula that contains at most one
positive (unnegated) literal, and \gls{IP}~models built using \glspl{Horn
  clause} can be solved in linear time~\cite{Hooker:1988}.
%
\citeauthor{Gebotys:1997} exploited this fact in developing an \gls{IP}~model
where \glspl{Horn clause} are applied to model \gls{register allocation}
\gls{instruction compaction}.
%
\Gls{pattern selection}, however, is modeled as in
\refEquation{pattern-selection-in-ip}.



\subsection{CP-based Approaches}

\Glsdesc{CP} is another method for solving combinatorial optimization problems,
which is discussed in detail in \refChapter{constraint-programming} (and
therefore only a brief introduction will be given here).
%
Like \gls{IP}, a \gls{constraint model} consists of a set of \glspl{decision
  variable}, a set of \glspl{constraint} over the \glspl{variable}, and
typically also an objective function to either minimize or maximize.
%
A crucial difference, however, is that \glspl{constraint} are not limited to
linear equations.
%
Instead, relations among multiple \glspl{variable} are modeled using
\gls{global.c} \glspl{constraint}, which simplifies modeling and improves
solving.



\paragraph{Modeling Pattern Selection Using Global Constraints}

\def\mBinaryRel{\Join}

\Gls{pattern selection} can be modeled using a \gls{global.c} \gls{constraint}
called the \gls!{global cardinality constraint}.
%
The \gls{constraint}, which is typically referred to as $\mCount$ or $\mGCC$, is
defined as follows:
%
\begin{equation}
  \mCount(v, \mVar{x}, \mBinaryRel, l)
  \equiv
  |\mSetBuilder*{\mVar{x}_i}{\mVar{x}_i = v}| \mBinaryRel l.
  \labelEquation{pattern-selection-using-count}
\end{equation}
%
In other words, if $N$ is the number of \glspl{decision variable} in the
set~$\mVar{x}$, then $\mCount$ enforces that \mbox{$N \mBinaryRel l$} holds,
where $\mBinaryRel$ is a binary relation.
%
For example, the constraint \mbox{$\mCount(4, \langle \mVar{x}_1 = 4, \mVar{x}_2
  = 5, \mVar{x}_3 = 6 \rangle, \leq, 1)$} holds since at most one
\glsshort{decision variable} is assigned the value~4.

To model \gls{pattern selection} using $\mCount$, two new sets of
\glspl{decision variable} are needed.
%
Assume that $M$ denotes the \gls{match set} and $\mCovers(m)$ denotes the set of
\glspl{node} covered by \gls{match}~$m$.
%
Then, for each node~$n$ to be covered, \glsshort{decision variable}
\mbox{$\mVar{match}_n \in \mSetBuilder{m}{m \in M, i \in \mCovers(m)}$} decides
which \gls{match} covers~$n$.
%
Also, for each match~$m$, \glsshort{decision variable} \mbox{$\mVar{count}_m \in
  \mSet{0, |\mCovers(m)|}$} decides how many \glspl{node} are covered by $m$.
%
Hence each match covers either no \glspl{node} or all \glspl{node} in its
\gls{pattern}.
%
With these \glsplshort{decision variable}, \gls{pattern selection} can be
modeled as
%
\begin{equation}
  \mCount(
    m,
    \cup_{n \in \mCovers(m)} \, \mVar{match}_n,
    =,
    \mVar{count}_m
  ),
  \forall m \in M,
\end{equation}
%
which is more restrictive than \refEquation{pattern-selection-in-ip} and thus
reduces solving time~\cite{FlochEtAl:2010}.



\paragraph{Applications}

The use of \gls{CP}-based \gls{instruction selection} appears to have been
pioneered by \textcite{BashfordLeupers:1999}.
%
To generate code for highly irregular \glspl{DSP},
\citeauthor{BashfordLeupers:1999} used \gls{CP} to model the interactions
between \gls{instruction selection} and the use of processor resources, such as
functional units and \glspl{register}.
%
Consequently, the approach essentially integrates \gls{instruction selection}
with a form of \gls{register allocation}.
%
Glossing over the details, the approach works as follows.
%
For each \gls{operation}, a so-called \gls!{FRT} is built which encodes the
resource requirements for the operands and result and the cost of every
\gls{instruction} that may be used to implement such \glspl{operation}.
%
Taking a \gls{block DAG} as input, the problem is to cover all \glspl{node}
using \glspl{FRT} such that all resource requirements are fulfilled.
%
Special resources are available for \glspl{instruction} that cover multiple
\glspl{node}, allowing adjacent \glspl{node} to be covered by the same
\gls{instruction}.
%
To solve this problem, \citeauthor{BashfordLeupers:1999} modeled it as a
\gls{constraint model}.

\textcite{MartinEtAl:2009, MartinEtAl:2012} developed a \gls{constraint model}
that integrates \gls{instruction selection} and \gls{instruction scheduling}.
%
Because they target \glspl{ASIP}, the \glspl{pattern} are not predefined but
must be found prior to \gls{instruction selection}.
%
For this task \citeauthor{MartinEtAl:2009} applied a \gls{CP}-based
\glshyphened{pattern matching} algorithm, described
in~\cite{WolinskiKuchcinski:2007}, and then modeled \gls{pattern selection} as
in \refEquation{pattern-selection-in-ip} in another \gls{constraint model}.
%
The rest of the \glsshort{constraint model}, however, is devoted to
\gls{instruction scheduling}.
%
\textcite{FlochEtAl:2010} later reused the \gls{constraint model} but replaced
the modeling for \gls{pattern selection} with
\refEquation{pattern-selection-using-count}.


\todo{discuss \cite{Beg:2013}}

\todo{discuss \cite{ArslanKuchcinski:2014}}



\section{Limitations of Existing Approaches}
\labelSection{limitations-of-existing-approaches}

\todo{write}
