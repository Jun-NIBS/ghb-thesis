% Copyright (c) 2017, Gabriel Hjort Blindell <ghb@kth.se>
%
% This work is licensed under a Creative Commons 4.0 International License (see
% LICENSE file or visit <http://creativecommons.org/licenses/by/4.0/> for a copy
% of the license).

\chapter{Existing Instruction Selection Techniques and Representations}
\labelChapter{existing-isel-techniques-and-reps}

With the first publications beginning to appear at the end of the 1960s,
\gls{instruction selection} has been actively researched for over four decades.
%
When surveying these techniques, it was discovered that essentially all apply
one of four fundamental \glspl!{principle} of \gls{instruction selection}:
\gls{macro expansion}, \gls{tree covering}, \gls{DAG covering}, and \gls{graph
  covering}.
%
\begin{figure}
  \centering%
  \def\numTotalPublications{?}% Will be redefined in the file included below
  \input{figures/existing-isel-techniques-and-reps/principles-timeline-styles}%
  {%
    \figureFont\figureFontSize%
    \input{figures/existing-isel-techniques-and-reps/principles-timeline}%
  }

  \caption[Principle timeline diagram]%
          {%
            Diagram illustrating how research on instruction selection with
            respect to the fundamental principles, has progressed over time.
            %
            With \numTotalPublications~publications in total, the width of each
            bar indicates the number of relative publications for a given year
            (\tikz{\node [nothing, fill=black, minimum height=1.25ex,
                          minimum width=\pgfkeysvalueof{/tikz/bar unit width}]
                         {}} represents one publication)%
          }
  \labelFigure{principles-timeline}
\end{figure}
%
The trend of applying these \glspl{principle} over time is shown in
\refFigure{principles-timeline}.
%
It was also discovered that the capabilities of these techniques can be compared
in terms of handling \glspl{instruction} with five \glsplshort!{instruction
  characteristic}.
%
The approaches can thus be systematically classified according to their
\gls{principle} and supported \glspl{instruction characteristic}.

Since a full survey is not needed in order to understand \gls{universal
  instruction selection} -- which apply \gls{graph covering} -- we examine in
this chapter only the techniques most relevant to our approach.
%
\RefSection{ex-isel-rep-instruction-characteristics} introduces the
\glspl{instruction characteristic}, and
\refSectionList{ex-isel-rep-macro-expansion, ex-isel-rep-tree-covering,
  ex-isel-rep-dag-covering, ex-isel-rep-graph-covering} discusses \gls{macro
  expansion}, \gls{tree covering}, \gls{DAG covering}, and \gls{graph covering},
respectively.
%
The remaining techniques are discussed in
\refAppendixRange{macro-expansion}{graph-covering}, and the full survey is also
available in~\cite{HjortBlindell:2016:Survey}.
%
Lastly, \refSection{ex-isel-rep-limitations-of-existing-approaches} discusses
the limitations of these techniques.

Without loss of generality, we henceforth assume that the input to the
\gls{instruction selector} consists of a single \gls{function}, which in turn
consists of many \glspl{basic block} (henceforth referred to as simply
\glspl!{block}).
%
Of these \glspl{block} exactly one represents the \gls{function}'s point of
entry, called the \gls!{entry block}.
%
\Gls{instruction selection} can then be reduced into two subproblems:%
%
\begin{enumerate}
  \item Finding all instances of instructions that can implement one or more
    \glspl{operation} in the \gls{function}.
    %
    This problem is called \gls!{matching}.
  \item Selecting a subset of these instances such that all \glspl{operation}
    are implemented.
    %
    This problem is called \gls!{selection}.
\end{enumerate}
%
Unless the second subproblem is constructed such that it impacts the first --
and I have yet to come across a situation where this is required -- both
subproblems can be solved in isolation without compromising code quality.


\section{Instruction Characteristics}
\labelSection{ex-isel-rep-instruction-characteristics}

An \gls{instruction} can be said to exhibit five \glsplshort{instruction
  characteristic}: \gls{single-output.ic}, \gls{multi-output.ic},
\gls{disjoint-output.ic}, \gls{inter-block.ic}, and \gls{interdependent.ic}.
%
The first three \glsplshort{instruction characteristic} form sets of
\glspl{instruction} that are disjoint from one another, whereas the last two
\glsplshort{instruction characteristic} can be combined as appropriate with any
of the other \glsplshort{instruction characteristic}.


\paragraph{Single-Output Instructions}

The simplest kind of \gls{instruction} forms the set of
\gls!{single-output.ic}[ \glspl{instruction}].
%
These produce only a single observable output, in the sense that ``observable''
means a value that can be accessed through the \gls{assembly code}.
%
This includes all \glspl{instruction} that implement a single operation (such as
addition, multiplication, and bit operations), but it also includes more
complicated \glspl{instruction} that implement several operations (such as
memory operations with complicated \glspl{addressing mode}).
%
As long as the observable output constitutes a single value, a
\gls{single-output.ic} \gls{instruction} can be arbitrarily complex.

This class comprises the majority of \glspl{instruction} in most
\glspl{instruction set}, and in simple \glspl{RISC}, such as \gls{MIPS}
architectures, nearly all \glspl{instruction} are \gls{single-output.ic}
\glspl{instruction}. Naturally, all \glspl{instruction selector} are expected to
support this kind of \gls{instruction}.


\paragraph{Multi-Output Instructions}

As expected from their name, \gls!{multi-output.ic}[ \glspl{instruction}]
produce more than one observable output from the same input.
%
Examples include \instrCode*{divmod}~\glspl{instruction}, which compute both the
quotient and the remainder of two input values, as well as arithmetic
\glspl{instruction} that, in addition to computing the result, also set one or
more \glspl{status flag}.\!%
%
\footnote{%
  A \gls!{status flag} (sometimes also known as a \gls!{condition flag} or a
  \gls!{condition code}) is a single bit that signifies additional information
  about the result of a computation, for example if there was a carry overflow
  or the result was equal to~0.%
}
%
For this reason such \glspl{instruction} are often said to have side effects,
but in reality these bits are nothing else but additional output values produced
by the \gls{instruction}, and will thus be referred to as \gls{multi-output.ic}
\glspl{instruction}.
%
Memory load and store \glspl{instruction}, which access a memory value and then
increment the address pointer, are also considered \gls{multi-output.ic}
\glspl{instruction}.

Many architectures such as \gls{X86}, \gls{ARM}, and \gls{Hexagon} provide
\glspl{instruction} of this class, although they are typically not as common as
\gls{single-output.ic} \glspl{instruction}.


\paragraph{Disjoint-Output Instructions}

\Glspl{instruction} which produce many observable output values from many
different input values are called \gls!{disjoint-output.ic}[
  \glspl{instruction}].
%
These are similar to \glspl{multi-output.ic} \glspl{instruction} with the
exception that all output values in the latter originate from the same input
values.
%
Another way to put it is that if one formed the \glspl{pattern} that correspond
to each output -- this will be explained in
\refSection{ex-isel-rep-tree-covering} -- all these \glspl{pattern} would be
disjoint from one another.
%
This typically includes \gls{SIMD.i} \glspl{instruction}, which execute the same
operations simultaneously on many distinct input values.

\Glspl{disjoint-output.ic} \glspl{instruction} are common in high-throughput
graphics architectures and \glspl{DSP}, but also appear in \gls{X86} as
extensions under names like \gls!{SSE} and \gls!{AVX}~\cite{Intel64:2015}.
%
Recently, certain \gls{ARM} processors are also equipped with such
extensions~\cite{ARM11:2008}.


\paragraph{Inter-Block Instructions}

\Glspl{instruction} whose behavior essentially spreads across multiple
\glspl{block} are called \gls!{inter-block.ic}[ \glspl{instruction}].
%
Examples of such \glspl{instruction} are those implementing \gls{saturation
  arithmetic}%
%
\footnote{%
  Recently, a request was made to extend the \gls{LLVM} \gls{compiler} with
  \glspl!{compiler intrinsic} -- a kind of special \gls{IR} operations -- to
  facilitate selection of such \glspl{instruction}~\cite{LLVM:2015:Intrinsics}.%
}
%
and hardware loop \glspl{instruction}, which repeat a fixed sequence of
\glspl{operation} a certain number of times.

\Glspl{instruction} with this \glsshort{instruction characteristic} typically
appear in customized architectures and \glspl{DSP} such as \gls{ARM}'s
\gls{Cortex-M7} processor~\cite{ARM:Cortex-M7:2015} and \gls{TI}'s
\gls{TMS320C55x} processor~\cite{TI:TMS320C55x:2002}.
%
But because of their complexity, these \glspl{instruction} require sophisticated
techniques for capturing their behavior which are not provided by most
\glspl{compiler}.
%
Instead, individual \glspl{instruction} are supported either via customized
\gls{program} optimization routines or through \glspl{compiler intrinsic}.
%
If no such routine or \gls{compiler intrinsic} is available, making use of these
\glspl{instruction} requires the \gls{program} to be written directly in
\gls{assembly code}.


\paragraph{Interdependent Instructions}

The last class is the set of \gls!{interdependent.ic}[ \glspl{instruction}].
%
This includes \glspl{instruction} which exhibit additional constraints that
appear when they are combined with other \glspl{instruction} in certain ways.
%
An example includes an \instrCode*{add}~\gls{instruction}, again from the
\gls{TMS320C55x} \gls{instruction set}, which cannot be combined with an
\instrCode*{rpt}~instruction if a particular \gls{addressing mode} is used for
the \instrCode*{add}~instruction.

\Gls{interdependent.ic} \glspl{instruction} are rare and can typically be found
in complex, heterogeneous architectures such as \glspl{DSP}.
%
This is another class of \glspl{instruction} that most \glspl{instruction
  selector} struggle with, mainly because these \glspl{instruction} typically
violate the set of assumptions made by the underlying techniques.


\section{Macro Expansion}
\labelSection{ex-isel-rep-macro-expansion}

The first \gls{principle} to emerge was \gls!{macro expansion}, with
applications starting to appear in the 1960s.
%
In \gls{macro expansion}, the \glspl{instruction} are expressed as
\glspl!{macro} which consist of two parts: a \gls!{template} to be matched over
the \gls{function}, and an \gls!{expand procedure} to be executed upon the part
of the \gls{function} that was matched (see \refFigure{macro-example} for an
example).
%
\begin{filecontents*}{macro-example.c}
expand($\irAssign{\text{\$3}}{\irAdd{\text{\$1}}{\text{\$2}}}$) {
  r1 = getRegOf($\$$1);
  r2 = getRegOf($\$$2);
  r3 = mkNewReg($\$$3);
  print "add " + r3 + ", " + r1 + ", " + r2;
}
\end{filecontents*}%
%
\begin{figure}
  \centering%
  \begin{minipage}{.6\textwidth}
    \lstinputlisting[mathescape]{macro-example.c}
  \end{minipage}

  \caption[Example of a macro]%
          {%
            Example of a macro expanding an IR addition into assembly code.
            %
            The template to match is given as argument to \cCode*{expand}, and
            the procedure to run upon expansion is given as \cCode*{expand}'s
            body%
          }
  \labelFigure{macro-example}
\end{figure}%
%
A \gls!{macro expander} traverses the \gls{function} under compilation and
attempts one \gls{macro} after another, typically in the order they are declared
in the \gls{machine description}.
%
Upon a \gls{match} it executes the corresponding \gls{expand procedure} and then
resumes the traversal with the next, unmatched part until the entire
\gls{function} has been expanded.
%
Consequently, \gls{matching} and \gls{selection} is combined into a single task
as the first \gls{macro} matched is the \gls{macro} to be selected.

The main benefit of \gls{macro expansion} is that it is intuitive and
straightforward to apply.
%
Because the \gls{macro expander} is implemented separately from the
\glspl{macro}, the former can be kept generic and simple while the latter can be
made as customized as needed for the \gls{target machine}.
%
This also allows the \gls{macro expander} to be void of any \glsshort{target
  machine}-specific details, thus necessiting only the \glspl{macro} to be
rewritten when retargeting the \gls{compiler} to another machine.
%
To this end, the \glspl{macro} are typically written in some dedicated language
in order to simplify this task by raising the level of abstraction.

Due to its simplicity, the principle also suffers from two shortcomings.
%
Depending on the complexity of the \glspl{macro}, \gls{macro expansion} could
in principle support all kinds of \gls{instruction}.
%
In practice, however, \glshyphened{macro expansion}[ding] \glspl{instruction
  selector} are typically limited to \gls{single-output.ic} \glspl{instruction}
and often only expand one \gls{IR}~operation at at time, which results in poor
code quality.
%
Another disadvantage is that because of idiosyncrasies of the dedicated
language, the \glspl{macro} are often hard to read and understand, making them
difficult to extend and maintain.
%
We call this variant of the principle \gls!{naive.me}[ \gls{macro expansion}].

To mitigate these problems, \gls{macro expansion} can be combined with
\gls{peephole optimization}.\!%
%
\footnote{%
  A \gls!{peephole optimizer} is a \gls{program} that combines two or more
  adjacent \glspl{instruction} into a single \gls{instruction}.
  %
  The term \emph{peephole} come from the narrow window of operation, as a
  \gls{peephole optimizer} only considers a small number of \glspl{instruction}
  at a time.
}
%
First, a \gls{naive.me} \gls{macro expander} expands the \gls{function} under
compilation one \gls{IR}~operation at at time.
%
Once fully expanded, a \gls{peephole optimizer} runs over the result and
replaces inefficient sequences of \glspl{instruction} with more compentent
equivalences.
%
Consequently, the \glspl{macro} can be divided up into those required for
correctness -- that is, the simple, single-operation \gls{macro} which ensure
that code can always be produced -- and those used for efficiency, which makes
for a simpler and more incremental retargeting effort.
%
Due to the people who pioneered the idea, this scheme is known as the
\gls!{Davidson-Fraser approach}~\cite{DavidsonFraser:1984}.

Because this \gls{principle} has little relevence for our approach, we will not
examine applications of \gls{naive.me} \gls{macro expansion} and the
\gls{Davidson-Fraser approach} in this chapter.


\section{Tree Covering}
\labelSection{ex-isel-rep-tree-covering}

Beginning of 1970s, a \gls{principle} called \gls!{tree covering} began to
emerge.
%
Unlike \gls{macro expansion}, \gls{tree covering} approaches \gls{instruction
  selection} as a \gls{graph} problem and, in doing so, separates
\gls{selection} from \gls{matching}.
%
This gives several advantages over \gls{macro expansion}.
%
First, capturing the behavior of \glspl{instruction} becomes simpler.
%
Second, tradeoffs in selecting certain combinations of \glspl{match} can be
considered, which improves code quality.
%
Third, due to \glspl{machine grammar} (to be described shortly), \gls{tree
  covering} can be based on a formal foundation which enables proof of
completeness.


\subsection{Instruction Selection as a Graph Problem}

First, the \gls{IR} code is transformed into a \gls!{data-flow graph}, where
\glspl{node} represent \glspl{operation} in the function and \glspl{edge}
represent data dependencies between the \glspl{operation}.
%
\Glspl{data-flow graph} are called \glspl!{expression tree} if they are limited
to single, tree-shaped expressions, \glspl!{block DAG} if they capture many
expressions in a \gls{block} as a \gls{DAG}, and \glspl!{function graph} if they
capture the data flow of entire functions.
%
An \gls{instruction selector} is \gls!{local.is} if it selects
\glspl{instruction} for \glspl{expression tree} or \glspl{block DAG}, and
\gls!{global.is} if it does so for \glspl{function graph}.

Corresponding \glspl{data-flow graph} are also built to represent the
\gls{instruction} provided by the \gls{target machine}.
%
Such \glspl{data-flow graph} are called either \glspl!{pattern tree},
\glspl!{pattern DAG}, or \glspl!{pattern graph}, depending on whether they are
shaped as \glspl{tree}, \glspl{DAG}, or \glspl{graph}, respectively.
%
When the shape is clear from the context, they are simply called
\glspl!{pattern}.
%
The set of \glspl{pattern} for a particular \gls{target machine} constitute a
\gls!{pattern set}.

The \gls{matching} problem can be reduced to finding all instances where a
\gls{pattern} from the \gls{pattern set} is \gls{subgraph} isomorphic to~$G$,
where $G$ denotes either a \gls{block DAG} or a \gls{function graph}.
%
Each such instance is called a \gls!{match}, and the set of all \glspl{match}
constitute a \gls!{match set}, which is denoted by~$M$.
%
Hence, in this context the subproblem is referred to as the \gls!{pattern
  matching}[ problem].
%
\Gls{pattern matching} can be done in linear time if both $G$ and all
\glspl{pattern} are tree-shaped, otherwise it is an NP-complete
problem~\cite{GareyJohnson:1979,HoffmannODonnell:1982}.

Having found $M$, the \gls{selection} problem -- which in this context is
referred to as the \gls!{pattern selection}[ problem] -- can be reduced to
selecting a set of \glspl{match} that \gls{cover}[s]~$G$.
%
A subset~\mbox{$C \subseteq M$}, where $M$ is a \gls{match set}, \gls!{cover}[s]
$G$ if every \gls{node} in $G$ appears in exactly one match from~$C$.
%
Such a subset is called a \gls!{cover}, and examples are shown in
\refFigure{p-match-sel-example}.

\begin{filecontents*}{p-match-sel-example.c}
x = A[i + 1];
\end{filecontents*}

\begin{figure}
  \centering%
  \subcaptionbox{C code\labelFigure{p-match-sel-example-c}}%
                {%
                  \begin{lstpage}{2.6cm}%
                    \lstinputlisting[language=c]{p-match-sel-example.c}%
                  \end{lstpage}%
                }%
  \hfill%
  \subcaptionbox{%
                  Instructions. The $*s$ \mbox{notation} means ``get value at
                  address $s$ in memory''%
                  \labelFigure{p-match-sel-example-instrs}%
                }{%
                  \figureFontSize%
                  \begin{tabular}{%
                                   >{\instrFont{}}r@{\hspace{4pt}}%
                                   >{$}l<{$}@{ $\leftarrow$ }%
                                   >{$}l<{$}%
                                 }
                    \toprule
                    mv     & r & \mathit{int}\\
                    add    & r & s + t\\
                    mul    & r & s \times t\\
                    muladd & r & s \times t + u\\
                    load   & r & *s\\
                    maload & r & *(s \times t + u)\\
                    \bottomrule
                  \end{tabular}%
                }%
  \hfill%
  \subcaptionbox{%
                  Expression tree and its matches%
                  \labelFigure{p-match-sel-example-tree}%
                }{%
                  \input{figures/existing-isel-techniques-and-reps/%
                    p-match-sel-example-tree%
                  }%
                }

  \caption[Example of the pattern matching and selection problem]%
          {%
            Example demonstrating the pattern matching and selection problem for
            a program that loads a value from integer array \irVar*{A} at offset
            \mbox{\irVar*{i} $+$ \irVar*{1}}.
            %
            It is assumed that \irVar*{i} is
            stored in register and that \irVar*{A} is stored in memory and that
            an integer is 4~bytes.
            %
            Valid covers are \mbox{$\mSet{m_1, \ldots, m_7, m_9}$},
            \mbox{$\mSet{m_1, \ldots, m_5, m_8, m_9}$}, and
            \mbox{$\mSet{m_1, \ldots, m_5, m_{10}}$}, where $m_1$ corresponds to
            a dummy pattern for covering nodes representing variables.
            %
            Variable assignments need not explicitly represented as nodes as
            this information can be propagated from the root node after having
            found a cover%
          }
  \labelFigure{p-match-sel-example}
\end{figure}

For a given \gls{program} and \gls{target machine}, there often exists many
valid combinations of \glspl{instruction} -- in terms of $G$ and $M$, this means
there exist many \glspl{cover} of~$G$ -- which may result in code where quality
differs significantly.
%
In certain cases, the performance of two sets of selected instructions may
differ by as much as two orders of magnitude~\cite{ZivojnovicEtAl:1994}.
%
Consequently, the \gls{pattern selection} problem -- originally defined to
accept any valid \gls{cover} -- is augmented into an optimization problem called
\gls!{optimal.ps} \gls{pattern selection}, where only \glspl{cover} with least
cost are accepted.
%
The cost of a \gls{cover}~$C$ is the sum of the costs for the \glspl{match}
appearing in~$C$, where the cost of a \gls{match} has been assigned to reflect a
desired characteristic in the produced code.

For example, assume that the \instrCode*{mv}, \instrCode*{add},
\instrCode*{mul}, and \instrCode*{muladd} \glspl{instruction} in
\refFigure{p-match-sel-example} all take one cycle to execute whereas the
\instrCode*{load} and \instrCode*{amload} \glspl{instruction} take five cycles
to execute.
%
Assume further that the \gls{compiler} should maximize performance.
%
The corresponding matches \mbox{$m_1, m_2, \ldots, m_8, m_9, m_{10}$} are
therefore assigned costs \mbox{$0, 1, \ldots, 1, 5, 5$}, respectively ($m_1$ has
zero cost since \gls{variable}~\irVar*{i} is already in a \gls{register}).
%
Then, of the valid \glspl{cover} \mbox{$\mSet{m_1, \ldots, m_7, m_9}$},
\mbox{$\mSet{m_1, \ldots, m_5, m_8, m_9}$}, and \mbox{$\mSet{m_1, \ldots, m_5,
    m_{10}}$}, only the last \gls{cover} is considered \gls{optimal.ps} as it
has a total cost of~9 whereas the other two \glspl{cover} have costs~11 and~10,
respectively.
%
There is typically a strong correlation between the size of a \gls{cover} and
its cost -- smaller \glspl{cover} lead to less cost, and ultimately better code
-- but this depends heavily on the properties of the \gls{target machine}.

For practical reasons, the \gls{instruction set} of a \gls{target machine} must
be described in a machine-readable format.
%
A common method is to model the \glspl{instruction} as a \gls{machine grammar},
which we will now describe.


\subsection{Machine Grammars}
\labelSection{machine-grammars}

\Glspl!{machine grammar} (or simply called \gls!{grammar}) are based on
\glspl{context-free grammar}~\cite{AhoEtAl:2006}, which are used for describing
language syntax.
%
A \gls{grammar} consists of \glspl{terminal}, \glspl{nonterminal}, and
\glspl{rule}.
%
In this context, a \gls!{terminal} is a \gls{symbol} representing an
\gls{operation} (e.g.\ \opAdd, \opLT, \opLoad), and a \gls!{nonterminal} is a
\gls{symbol} representing an abstract result (e.g.\ $\mNT{Reg}$) produced by the
\gls{instruction}.
%
To distinguish between the two, \glspl{terminal} are written entirely in lower
case whereas \glspl{nonterminal} start with a capital letter and are set in
italics.
%
A \gls!{rule} describes the behavior of an \gls{instruction} and consists of a
\gls{production}, a non-negative cost, and an \gls{action}.
%
\Glspl!{production} describe how to derive \glspl{nonterminal}, and are written
as
%
\begin{displaymath}
  a \rightarrow b c \ldots
\end{displaymath}
%
where the left-hand side is consists a \gls{nonterminal} and the right-hand side
consists of a sequence of \glspl{terminal} and \glspl{nonterminal}.
%
Each \gls{instruction} therefore gives rise to one or more \glspl{production},
where the right-hand side of a \gls{production} captures a \gls{pattern} of the
\gls{instruction} and the left-hand side denotes the result produced by the
\gls{instruction}.
%
Hence the left-hand and right-hand sides of a \gls{rule} are referred to as the
\gls{rule}'s \glsshort!{rule result} and \glsshort!{rule pattern}, respectively.
%
The \gls{rule} structure is also illustrated in
\refFigure{machine-grammar-rule-anatomy}, and an example of a few \glspl{rule}
is given in \refTable{grammar-rules-example}.
%
\begin{figure}
  \centering%
  \figureFont\figureFontSize%
  \begin{displaymath}
    \underbrace{
      \overbrace{
        \overbrace{\mNT{Reg}[1]}^{\text{result}}
        \rightarrow \;
        \overbrace{\opAdd{} \; \mNT{Reg}[2] \; \irCode{const}}^{\text{pattern}}%
      }^{\text{production}}
      \qquad
      \overbrace{\text{4}}^{\text{cost}}
      \qquad
      \overbrace{
        \text{emit:}~
        \text{\instrFont add \$$\mNT{Reg}[1]$, \$$\mNT{Reg}[2]$, \#const}%
      }^{\text{action}}
    }_{\text{rule}}
  \end{displaymath}

  \vspace*{-\baselineskip}

  \caption{Anatomy of a rule in a machine grammar}
  \labelFigure{machine-grammar-rule-anatomy}
\end{figure}
%
The \glspl{production} are typically written in \gls{Polish notation} to avoid
the need for parentheses (for example, \mbox{$1 + (2 + 3)$} is written as
\mbox{$+ \; 1 + 2 \; 3$}).
%
Similarly, the \gls{expression tree} shown in
\refFigure{p-match-sel-example-tree} can be expressed as \mbox{$\opLoad \;
  \opAdd \; \opMul \; \opAdd \; \opVar{i} \; \opVar{1} \; \opVar{4} \;
  \opVar{A}$}.

\begin{table}[t]
  \centering%
  \figureFontSize%
  \begin{tabular}{cr@{ $\rightarrow$ }lcl}
    \toprule
    \tabhead \# & \multicolumn{2}{c}{\tabhead production} & \tabhead cost
      & \multicolumn{1}{c}{\tabhead action}\\
    \midrule
    1 & $\mNT{Reg}[1]$ & $\opLoad \; \opAdd \; \mNT{Reg}[2] \; \irCode{const}$
      & 1
      & emit: {\instrFont load \$$\mNT{Reg}[1]$, const(\$$\mNT{Reg}[2]$)}\\
    2 & $\mNT{Reg}[1]$ & $\opLoad \; \opAdd \; \irCode{const} \; \mNT{Reg}[2]$
      & 1
      & emit: {\instrFont load \$$\mNT{Reg}[1]$, const(\$$\mNT{Reg}[2]$)}\\
    3 & $\mNT{Reg}[1]$ & $\opLoad \; \mNT{Reg}[2]$
      & 1
      & emit: {\instrFont load \$$\mNT{Reg}[1]$, 0(\$$\mNT{Reg}[2]$)}\\
    \bottomrule
  \end{tabular}

  \caption[Example of rules]%
          {%
            Example of rules corresponding to a
            \mbox{\instrFont* load \$t, o(\$s)} \gls{instruction} that loads a
            value from memory at the address given in register~\instrCode*{s},
            offset by an immediate value~\instrCode*{o}, and stores the loaded
            value in register~\instrCode*{t}, in one cycle.
            %
            The subscripts are only needed for referencing the right
            nonterminal in the action%
          }
  \labelTable{grammar-rules-example}
\end{table}

Consequently, with a \gls{grammar} the \gls{pattern selection} problem
becomes equivalent to finding a sequence of \gls{rule} applications (called
\glspl!{rule reduction}) that reduces the \gls{expression tree} to a given
\gls{nonterminal}.
%
A method for finding the sequence with least cost is described shortly.


\paragraph{Normal Form}

To simplify \gls{pattern matching} and \gls{pattern selection}, a \gls{grammar}
can be rewritten into \gls!{normal form.g}. A \gls{grammar} is in \gls{normal
  form.g} if every \gls{rule} in the \gls{grammar} has a \gls{production} in one
of the following forms:
%
\begin{enumerate}
  \item \mbox{$\mNT{N} \rightarrow \irCode*{op} \; \mNT{A}[1] \; \mNT{A}[2]
    \ldots \mNT{A}[n]$}, where \irCode*{op} is a \gls{terminal}, representing an
    \gls{operation} that takes $n$ arguments, and all $\mNT{A}[i]$ are
    \glspl{nonterminal}.
    %
    Such rules are called \gls!{base.r}[ \glspl{rule}].
  \item \mbox{$\mNT{N} \rightarrow \irCode*{t}$}, where \irCode*{t} is a
    \gls{terminal}.
    %
    Such rules are also called \gls{base.r} \glspl{rule}.
  \item \mbox{$\mNT{N} \rightarrow \mNT{A}$}, where $\mNT{A}$ is a
    \gls{nonterminal}.
    %
    Such rules are called \gls!{chain.r}[ \glspl{rule}].
\end{enumerate}

\begin{table}[t]
  \centering%
  \figureFontSize%
  \begin{tabular}{cr@{ $\rightarrow$ }lcl}
    \toprule
    \tabhead \# & \multicolumn{2}{c}{\tabhead production} & \tabhead cost
      & \multicolumn{1}{c}{\tabhead action}\\
    \midrule
    1 & $\mNT{Reg}$ & $\opLoad \; \mNT{A}$
      & 1
      & emit: {\instrFont load \$$\mNT{Reg}$,
                               $A.C.\text{const}$(\$$A.\mNT{Reg}$)}\\
    4 & $\mNT{A}$ & $\opAdd \; \mNT{Reg} \; \mNT{C}$
      & 0
      & \\
    2 & $\mNT{Reg}$ & $\opLoad \; \mNT{B}$
      & 1
      & emit: {\instrFont load \$$\mNT{Reg}$,
                               $B.C.\text{const}$(\$$B.\mNT{Reg}$)}\\
    5 & $\mNT{B}$ & $\opAdd \; \mNT{C} \; \mNT{Reg}$
      & 0
      & \\
    6 & $\mNT{C}$ & $\irCode{const}$
      & 0
      & \\
    3 & $\mNT{Reg}[1]$ & $\opLoad \; \mNT{Reg}[2]$
      & 1
      & emit: {\instrFont load \$$\mNT{Reg}[1]$, 0(\$$\mNT{Reg}[2]$)}\\
    \bottomrule
  \end{tabular}%

  \caption[Grammar from \refTable{grammar-rules-example} in normal form]%
          {%
            The grammar from \refTable{grammar-rules-example} in normal form.
            %
            Nonterminals~$\mNT{A}$,~$\mNT{B}$ and~$\mNT{C}$ and
            rules~\mbox{4--6} are introduced in order to transform rules~1 and~2
            into base rules%
          }
  \labelTable{normal-form-grammar-example}
\end{table}

A \gls{grammar} can be mechanically rewritten into \gls{normal form.g} by
introducing new \glspl{nonterminal} and breaking down illegal \glspl{rule} into
multiple, smaller \glspl{rule} until the \gls{grammar} is in \gls{normal
  form.g}.
%
For example, rewriting the \gls{grammar} shown in
\refTable{grammar-rules-example} into \gls{normal form.g} results in the
\gls{grammar} shown in \refTable{normal-form-grammar-example}.
%
Note that the new \glspl{rule} have zero cost and no \gls{action} as these are
only intermediary steps towards enabling reduction of the original \gls{rule}.

Since all \glspl{production} in a \glsshort{normal form.g} \gls{grammar} have at
most one \gls{terminal}, the \gls{pattern matching} problem becomes trivial
(simply match the \gls{node} type against the \gls{terminal} in all \gls{base.r}
\glspl{rule}).
%
Otherwise another bottom-up traversal of the \gls{expression tree} would have to
be made in order to find all \glspl{match}, which can be done in linear time for
most reasonable \glspl{grammar}~\cite{HoffmannODonnell:1982}.
%
As we will see, this also simplifies \gls{pattern selection} as the
\glspl{pattern} on the right-hand side in all \glspl{production} have uniform
height.


\subsection{Optimal Pattern Selection on Expression Trees}

\textcite{AhoEtAl:1989} introduced a method for finding the \gls{optimal.ps}
\gls{cover} for any given \gls{expression tree} in linear time, which is also
the most common and well known technique based on \gls{tree covering}.

The technique is centered around the following assumption: the cost of reducing
\gls{node}~$n$ in an \gls{expression tree} to \gls{nonterminal}~$s$ using
\gls{rule}~$r$ is the cost of $r$ plus the costs of reducing all children of $n$
to the appropriate \glspl{nonterminal} appearing on the right-hand side of~$r$.
%
If $r$ is a \gls{chain.r} \gls{rule} then the cost is computed as the cost of
$r$ plus the cost of reducing $n$ to the \glsshort{rule result} of~$r$.
%
The recursive nature of these costs can be exploited using \glsdesc{DP},
resulting in the algorithm shown in \refAlgorithm{aho-etal-cost-algorithm} which
computes the least cost of reducing a given \gls{expression tree} to a
particular \gls{nonterminal}.

\begin{algorithm}[p]
  \DeclFunction{ComputeCosts}%
               {expression tree $T$, normal-form grammar $G$}%
  {%
    $S$ \Assign $\mSetBuilder{s}%
                             {\text{$s$ is a nonterminal in $G$}}$\;
    $\mMatrix{C}$ \Assign
      matrix of size $|T| \times |S|$, costs initialized to $\infty$\;
    \Call{ComputeCostsRec}{root node of $T$}\;
    \Return{$\mMatrix{C}$}\;
    \BlankLine
    \DeclFunction{ComputeCostsRec}{node $n$}{%
      \ForEach{child $m$ of $n$}{%
        \Call{ComputeCostsRec}{$m$}\;
      }
      \ForEach{base rule $r \in \text{\Call{FindMatchingRules}{$n$}}$}{%
        $c$ \Assign \Call{ComputeReductionCost}{$n$, $r$}\;
        $l$ \Assign result of $r$\;
        \If{$c < \mMatrix{C}[n][l]$.cost}{%
          $\mMatrix{C}[n][l]$.cost \Assign $c$\;
          $\mMatrix{C}[n][l]$.rule \Assign $r$\;
        }
      }
      \Repeat{no change to $\mMatrix{C}$}{%
        \ForEach{chain rule $r \in G$}{%
          $c$ \Assign \Call{ComputeReductionCost}{$n$, $r$}\;
          $l$ \Assign result of $r$\;
          \If{$c < \mMatrix{C}[n][l]$.cost}{%
            $\mMatrix{C}[n][l]$.cost \Assign $c$\;
            $\mMatrix{C}[n][l]$.rule \Assign $r$\;
          }
        }
      }
    }

    \BlankLine
    \DeclFunction{FindMatchingRules}{node $n$}{%
      $M$ \Assign $\emptyset$\;
      \ForEach{base rule $r \in G$}{%
        \If{$\text{terminal in pattern of $r$} = \text{node type of $n$}$}{%
          $M$ \Assign $M \cup \mSet{r}$\;
        }
      }
      \Return{$M$}\;
    }

    \BlankLine
    \DeclFunction{ComputeReductionCost}{node $n$, rule $r$}{%
      $c$ \Assign cost of $r$\;
      \eIf{$r$ is a chain rule}{%
        $s$ \Assign nonterminal in pattern of $r$\;
        $c$ \Assign $c$ $+$ $\mMatrix{C}[n][s]$.cost
        \cmt*{here cost of node itself is taken instead of its children}
      }{%
        \For{$i \leftarrow 1$ \KwTo number of children for $n$}{%
          $m$ \Assign $i$th child of $n$\;
          $s$ \Assign $i$th nonterminal in pattern of $r$\;
          $c$ \Assign $c$ $+$ $\mMatrix{C}[m][s]$.cost\;
        }
      }
      \Return{$c$}\;
    }
  }

  \caption[%
            Algorithm for computing the optimal sequence of rules that reduces
            the given expression tree to a particular nonterminal%
          ]{%
            Computes the optimal sequence of rules that reduces the given
            expression tree to a particular nonterminal%
          }
  \labelAlgorithm{aho-etal-cost-algorithm}
\end{algorithm}

The algorithm works as follows.
%
It first constructs a cost matrix~$\mMatrix{C}$, where rows represent
\glspl{node} in the \gls{expression tree} and columns represent
\glspl{nonterminal} in the \gls{grammar}, which is assumed to be in \gls{normal
  form.g}.\!%
%
\footnote{%
  The algorithm can be adapted to accept any \gls{grammar} by expanding the
  \algStyle{FindMatchingRules} and \algStyle{ComputeReductionCost} functions to
  handle \glspl{rule} of arbitrary form.%
}
%
The cost in each element~\mbox{$C[i][j]$} is initialized to infinity, indicating
that there exists no sequence of \glspl{rule reduction} which reduces
\gls{node}~$i$ to \gls{nonterminal}~$j$.
%
It then computes the costs by traversing the \gls{expression tree} bottom up.
%
At each \gls{node}~$n$ and for each matching \gls{base.r} \gls{rule}~$r$, with
\gls{nonterminal}~$s$ as \glsshort{rule result}, it computes the cost~$c$ of
applying $r$ at~$n$ to produce~$s$ according to the scheme stated above.
%
If $c$ is less than the currently recorded cost for reducing $n$ to $s$, then
the cost and \gls{rule} information for $n$ is updated accordingly.
%
The same is then done for all \gls{chain.r} \glspl{rule} until it reaches a
fixpoint (which must eventually be reached as all \gls{rule} costs are
non-negative and an update only occurs when the cost is strictly less).
%
Since every \gls{node} is also only processed once, the algorithm runs in linear
time with respect to the size of the \gls{expression tree}.
%
Having computed the costs, the optimal order of \glspl{rule reduction} -- which
is equivalent to the \gls{least-cost.c} \gls{cover} -- can be found using the
algorithm shown in \refAlgorithm{aho-etal-select-algorithm}.

\begin{algorithm}[t]
  \DeclFunction{Select}{expression tree $T$,
                        goal nonterminal $g$,
                        cost matrix $\mMatrix{C}$}%
  {%
    $n$ \Assign root node of $T$\;
    $r$ \Assign $C[n][g]$.rule\;
    \eIf{$r$ is a chain rule}{%
      $s$ \Assign result of $r$\;
      \Call{Select}{$T$, $s$, $C$}\;
    }{%
      \For{$i \leftarrow 1$ \KwTo number of children for $n$}{%
        $m$ \Assign $i$th child of $n$\;
        $s$ \Assign $i$th nonterminal in pattern of $r$\;
        \Call{Select}{expression tree rooted at $m$, $s$}\;
      }
    }
    execute actions associated with $r$\;
  }

  \caption[%
            Algorithm for selecting the rules chosen by
            \refAlgorithm{aho-etal-cost-algorithm}%
          ]%
          {%
            Selects optimal sequence of rules that reduces a given expression
            tree to a given nonterminal, based on costs computed
            by \refAlgorithm{aho-etal-cost-algorithm}%
          }
  \labelAlgorithm{aho-etal-select-algorithm}
\end{algorithm}

In many cases, this technique produces code of sufficient quality.
%
In fact, for architectures with simple \glspl{instruction set}, where the
\glspl{rule pattern} can naturally be modeled as trees (such as
\gls{single-output.ic} \glspl{instruction}), it is often optimal or near
optimal.
%
The \gls{MIPS} \gls{instruction set}~\cite{Sweetman:2006}, for example, is one
such architecture.


\subsection{Precomputing Costs and Rule Decisions}

Shortly after \citeauthor{AhoEtAl:1989} published their approach, it was
recognized that the costs computed by \refAlgorithm{aho-etal-cost-algorithm}
could be precomputed for any \glspl{expression tree}, thereby replacing the call
to the \algStyle{ComputeReductionCost} function with a table lookup.
%
While this improvement does not affect the asymptotic complexity, it does
significantly reduce the runtime of the algorithm.
%
Although pioneered by \textcite{HatcherChristopher:1986}, the idea was first
successfully applied by \textcite{Pelegri-LlopartGraham:1988}, and later
improved and simplified by \textcite{BalachandranEtAl:1990} and
\textcite{Proebsting:1992:BURS}.

The \gls{expression tree} is traversed bottom up and labeled with a
\gls!{state}.
%
For a given labeled \gls{node}~$n$, the \gls{state} essentially holds enough
information needed to optimally reduce the \gls{expression tree} rooted at~$n$
to any \gls{nonterminal}.
%
At first glance, it would seem that this requires an infinite number of
\glspl{state} since \glspl{expression tree} can be of arbitrary size, but this
only applies if the full cost of the entire \gls{expression tree} is taken into
account.
%
Fortunately, it is sufficient to only consider the relative cost differences
between \glspl{rule} for any given \gls{node} in the \gls{expression tree},
leading to a finite number of \glspl{state}.
%
\begin{algorithm}[t]
  \DeclFunction{Label}%
               {expression tree $T$, list $L$ of state tables}%
  {%
    $n$ \Assign root node of $T$\;
    $k$ \Assign number of children for $n$\;
    \For{$i \leftarrow 1$ \KwTo $k$}{%
      $m_i$ \Assign $i$th child of $n$\;
      \Call{Label}{expression tree rooted at $m_i$, $L$}\;
    }
    $S$ \Assign $L[\text{terminal corresponding to $n$}]$\;
    $n$.label \Assign $S[m_1.\text{label}, \ldots, m_k.\text{label}]$\;
  }

  \caption[%
            Algorithm for labeling an expression tree for optimal pattern
            selection%
          ]{%
            Labels an expression tree for optimal pattern selection%
          }
  \labelAlgorithm{opt-pat-sel-labeling-algorithm}
\end{algorithm}
%
The algorithm for labeling an \gls{expression tree} is shown in
\refAlgorithm{opt-pat-sel-labeling-algorithm}.
%
To derive the \gls{rule} selection algorithm, one only needs to adapt line~2 in
\refAlgorithm{aho-etal-select-algorithm} to perform the appropriate table
lookups.

The idea for computing the \glspl{state} -- which will only be described briefly
-- works as follows.
%
For each \gls{terminal} representing a $k$-argument \gls{operation}, an
\mbox{$k$-dimensional} matrix is maintained.
%
This is called the \gls{terminal}'s \gls!{state table}, which indicates the
state to assign such \glspl{node} given the labels of its children.
%
First the states for all leaf \glspl{node} are built, considering only
\gls{base.r} \glspl{rule} with a single \glspl{terminal} on the right-hand side
in the \gls{production}.
%
The costs and \gls{rule} decisions are computed using the same logic as in
\refAlgorithm{aho-etal-cost-algorithm}, lines~8--21.
%
The leaf \gls{state} are then pushed onto a queue.
%
Each popped \gls{state} is used as the $i$th child to all \gls{base.r}
\glspl{rule} with a non-leaf \glspl{terminal} in combination with all other
existing \glspl{state}.
%
If any combination gives rise to a new set of costs and \gls{rule} decisions,
then a new \gls{state} is created and pushed onto the queue after having updated
the \glspl{state table}.
%
This process continues until the queue is empty, whereupon all necessary
\glspl{state} have been built.


\subsection{Limitations of Tree Covering}

The main disadvantage of operating on \glspl{expression tree} is that common
subexpressions have to be either split along the \glspl{edge} or duplicated when
building the \gls{IR}.
%
These transformations are referred to as \gls!{edge splitting} and \gls!{node
  duplication}, respectively, and depending on the \gls{instruction set}, these
decisions can prevent selection of \glspl{instruction} that would lead to higher
code quality.

\begin{filecontents*}{exp-trees-limit-example.c}
t = a + b;
x = c * t;
y = *((int*) t);
\end{filecontents*}

\begin{figure}
  \centering%
  \mbox{}%
  \hfill%
  \subcaptionbox{C code\labelFigure{exp-trees-limit-example-c}}%
                {%
                  \begin{lstpage}{3cm}
                    \lstinputlisting[language=c]{exp-trees-limit-example.c}%
                  \end{lstpage}%
                }%
  \hfill%
  \subcaptionbox{%
                  Instructions. The $*s$ notation means ``get value at address
                  $s$ in memory''%
                  \labelFigure{exp-trees-limit-example-instrs}%
                }%
                [50mm]%
                {%
                  \figureFontSize
                  \begin{tabular}{%
                                   >{\instrFont{}}r@{\hspace{4pt}}%
                                   >{$}l<{$}@{ $\leftarrow$ }%
                                   >{$}l<{$}%
                                   c%
                                 }
                    \toprule
                    \multicolumn{3}{c}{\tabhead instruction} & \tabhead cost\\
                    \midrule
                    add     & r & s + t & 2\\
                    mul     & r & s \times t & 3\\
                    addmul  & r & (s + t) \times u & 4\\
                    load    & r & *s & 5\\
                    addload & r & *(s + t) & 5\\
                    \bottomrule
                  \end{tabular}%
                }%
  \hfill%
  \mbox{}%

  \vspace{\betweensubfigures}

  \mbox{}%
  \hfill%
  \subcaptionbox{%
                  Expression trees after edge splitting%
                  \labelFigure{exp-trees-limit-example-trees}%
                }{%
                  \input{figures/existing-isel-techniques-and-reps/%
                    exp-trees-limit-example-trees%
                  }%
                }%
  \hfill\hfill%
  \subcaptionbox{%
                  Block DAG%
                  \labelFigure{exp-trees-limit-example-dag}%
                }{%
                  \input{figures/existing-isel-techniques-and-reps/%
                    exp-trees-limit-example-dag%
                  }%
                }%
  \hfill%
  \mbox{}%

  \caption[Example illustrating the limitation of expression trees]%
          {%
            Example illustrating that using block DAGs results in
            better code compared to using expression trees.
            %
            It is assumed variables \irVar*{a}, \irVar*{b}, \irVar*{c},
            \irVar*{t} are stored in registers%
          }
  \labelFigure{exp-trees-limit-example}
\end{figure}

An example illustrating this effect is shown in
\refFigure{exp-trees-limit-example}.
%
If the \gls{IR} is represented as trees, where the common subexpression for
computing \irVar*{t} has its own \gls{expression tree} (see
\refFigure{exp-trees-limit-example-trees}), then matches~\mbox{$m_1, \ldots,
  m_7$}, and~$m_9$ must be selected, which results in a total cost of \mbox{$0 +
  \cdots + 0 + 2 + 3 + 5 = 10$}.
%
If represented as a \gls{block DAG} (see
\refFigure{exp-trees-limit-example-dag}), it becomes possible of selecting
matches~$m_8$ and~$m_{10}$, resulting in a total cost of \mbox{$0 + \cdots + 0 +
  4 + 5 = 9$}.
%
\Glsshort{node duplication}[ing] the \glspl{node} of the common subexpression
would, in this case, yield the same \glspl{cover}, but would have resulted in
suboptimal code in cases where the \instrCode*{addmul} and \instrCode* {addload}
\glspl{instruction} are not available.


\section{DAG Covering}
\labelSection{ex-isel-rep-dag-covering}

By replacing the \glspl{expression tree} used in \gls{tree covering} with
\glspl{block DAG}, and allowing \glspl{instruction} to be modeled either as
\glspl{pattern tree} or \glspl{pattern DAG}, we attain a more general
\gls{principle} called \gls!{DAG covering}.

\Gls{DAG covering} has several advantages of \gls{tree covering}.
%
First, the \gls{block DAG} does not need to be decomposed into \glspl{expression
  tree}, which compromises code quality.
%
Second, it enables selection of \gls{multi-output.ic} \glspl{instruction}, which
must be modeled as \glspl{pattern DAG}.

But unlike \gls{tree covering}, which can be solved optimally in linear time,
finding the \gls{least-cost.c} \gls{cover} of a \gls{block DAG} is
NP-complete~\cite{KoesGoldstein:2008, Proebsting:1995:Proof}.
%
The proof is also available in \refAppendix{dag-covering}.
%
Consequently, \gls{DAG covering} started to gain traction in the beginning of
the 1990s when exponential increase in compute power and significant progress
made in the field of combinatorial optimization enabled such methods to be
applied to \gls{instruction selection}.
%
Most combinatorial approaches natively support \gls{DAG}-shaped \glspl{pattern},
but finding \glspl{match} for such \glspl{pattern} can no longer be done in
linear time.
%
Such approaches therefore typically apply generic \glshyphened{subgraph
  isomorphism} algorithms when \gls{pattern matching}.


\subsection{Pattern Matching as a Subgraph Isomorphism Problem}

\def\mGP{G_{\textsc{p}}}
\def\mGF{G_{\textsc{f}}}
\def\mNP{N_{\textsc{p}}}
\def\mNF{N_{\textsc{f}}}
\def\mEP{E_{\textsc{p}}}
\def\mEF{E_{\textsc{f}}}

The \gls!{subgraph isomorphism} problem is to find instances where a
\gls{graph}~\mbox{$\mGP = \mPair{\mNP}{\mEP}$} is \glsshort{isomorphism} to a
\gls{subgraph} in another \gls{graph}~\mbox{$\mGF = \mPair{\mNF}{\mEF}$}.
%
$\mGP$ is \glsshort!{isomorphism} to $\mGF$ if and only if there exists a
mapping \mbox{$\mFunDecl{m}{\mNP}{\mNF}$} such that \mbox{$\mPair{n}{o} \in
  \mEP$} implies \mbox{$\mPair{f(n)}{f(o)} \in \mEF$}.
%
In the context of \gls{instruction selection}, $\mGP$ denotes the \gls{pattern},
$\mGF$ denotes the \gls{block DAG} or \gls{function graph}, and $m$ denotes a
\gls{match}.

As \gls{subgraph isomorphism}, which is known to be
NP-complete~{\cite{Cook:1971}}, appears in many other fields, much research has
been devoted to this problem (see for example \cite{Ullmann:1976,
  CordellaEtAl:2001, GuoEtAl:2003, KrissinelHenrick:2004, SorlinSolnon:2004,
  Gallagher:2006, FanEtAl:2010, FanEtAl:2011, HinoEtAl:2012, McCreesh:2017}).
%
Due to its simplicity, however, most combinatorial approaches apply the
\gls{VF2}~algorithm, which is described in
\refSection{ex-isel-rep-vf2-algorithm}.


\subsection{Maximum Munch}

The most common approach for greedy \gls{pattern selection} on \glspl{block DAG}
is called \gls!{maximum munch} (coined by \textcite{Cattell:1978}).
%
The idea is to traverse the \gls{block DAG} top down, select the largest
\gls{pattern} that matches the current \gls{node}, and repeat the process for
remaining, uncovered parts of the \gls{block DAG}.
%
The approach -- which is used in, for example, LLVM~\cite{LattnerAdve:2004} --
works well for architectures with a regular \gls{instruction set} and where
there is a strong correlation between the effectiveness of the \gls{instruction}
and the size of its \gls{pattern}.
%
In addition to being non-optimal, however, it also suffers from the same
drawback as \glspl{expression tree} regarding whether to select \glspl{cover}
that effectively \glsshort{edge splitting} or \glsshort{node duplication}[e] the
common subexpressions.


\subsection{Approaches for Balancing Splitting and Duplication}
\labelSection{ex-isel-rep-balancing-splitting-and-duplication}

Several approaches have been made in attempting to balance \gls{edge splitting}
and \gls{node duplication}.
%
\textcite{FauthEtAl:1994} designed an heuristic algorithm that rewrites the
\gls{block DAG} into \glspl{expression tree} before \gls{instruction
  selection}.
%
Using a rough estimate of cost, the algorithm favors \gls{node duplication} and
resorts to \gls{edge splitting} when the former becomes too costly.
%
Once rewritten, the \glspl{expression tree} are covered using an improved
variant of \refAlgorithm{aho-etal-cost-algorithm}.

\textcite{Ertl:1999} showed that, for certain \glspl{grammar},
\refAlgorithm{aho-etal-cost-algorithm} can be adapted to produce optimal
\glspl{cover} for \glspl{block DAG}.
%
The idea is to first compute the costs for each \gls{node} as if the \gls{block
  DAG} had been rewritten into a \gls{expression tree} using \gls{node
  duplication}.
%
Then, if several \glspl{rule} reduce the same \gls{node} to the same
\gls{nonterminal}~$n$, then $n$ can be shared between the \glspl{rule} whose
\glsplshort{rule pattern} contain $n$.
%
\citeauthor{Ertl:1999} also introduced an algorithm for checking whether
\gls{optimal.ps} \gls{pattern selection} is guaranteed for a given
\gls{grammar}.

\textcite{KoesGoldstein:2008} combined the ideas by \citeauthor{FauthEtAl:1994}
and \citeauthor{Ertl:1999} by introducing a design that first uses
\refAlgorithm{aho-etal-cost-algorithm} to compute the costs for a \glsshort{node
  duplication}[ed] \gls{expression tree}.
%
Then, at each \glspl{node}~$n$ where several \glspl{pattern} in the optimal
\gls{cover} overlap in the \gls{block DAG}, two costs are estimated: the cost
incurred by allowing overlap, and the cost incurred by \glsshort{edge
  splitting}[ting] the \glspl{edge}.
%
If the latter cost is less then $n$ is marked as \gls!{fixed.n}, meaning it can
only be covered by \glspl{pattern} where $n$ is the root \gls{node}.
%
Once all such \glspl{node} have been processed, another pass is performed to
recompute the costs, this time forbidding overlap at \gls{fixed.n} \glspl{node}.


\subsection{MIS- and MWIS-based Approaches}

Another approach to modeling \gls{instruction selection} is to model it as a
problem of finding \glspl{independent set}.
%
Given a \gls{graph}~\mbox{$G = \mPair{N}{E}$}, a set \mbox{$S \subseteq N$} is
an \gls!{independent set} if no pairs of nodes~\mbox{$m, n \in S$} are adjacent
in~$G$.
%
An \gls{independent set} is called a \gls!{MIS} if no more \glspl{node} can be
added and still be an \gls{independent set}.
%
If each \gls{node} in the graph has a weight, then a \gls!{MWIS} is a \gls{MIS}
that maximizes/minimizes \mbox{$\sum_n \mWeight(n)$}.
%
In general, finding a \gls{MIS} or \gls{MWIS} is
NP-complete~\cite{GareyJohnson:1979}.

Modeling \gls{instruction selection} as either a \gls{MIS} or \gls{MWIS} problem
is done as follows.
%
After \gls{pattern matching}, an \gls!{interference graph} is constructed where
a \gls{node} represents a \gls{match} and an \gls{edge} represents overlapping
between two \glspl{match}.
%
If all \glspl{operation} in the \gls{block DAG} or \gls{function graph} can be
covered by at least one \gls{match}, then a \gls{MIS} of the \gls{interference
  graph} corresponds to a \gls{cover}.
%
Likewise, a \gls{MWIS} corresponds to a \gls{least-cost.c} \gls{cover}.
%
An example is shown in \refFigure{mis-example}.

\begin{figure}
  \centering%
  \mbox{}%
  \hfill%
  \subcaptionbox{%
                  Block DAG%
                  \labelFigure{mis-example-dag}%
                }{%
                  \input{figures/existing-isel-techniques-and-reps/%
                    mis-example-dag%
                  }%
                }%
  \hfill%
  \subcaptionbox{%
                  Interference graph%
                  \labelFigure{mis-example-int-graph}%
                }{%
                  \input{figures/existing-isel-techniques-and-reps/%
                    mis-example-int-graph%
                  }%
                }%
  \hfill%
  \mbox{}

  \caption[Example of modeling instruction selection as a MIS problem]%
          {%
            Example of modeling instruction selection as a MIS problem.
            %
            Valid maximal independent sets of the interference graph are
            \mbox{$\mSet{m_1, \ldots, m_5, m_7}$}, \mbox{$\mSet{m_1, m_2, m_3,
                m_5, m_8}$}, and \mbox{$\mSet{m_1, m_2, m_3, m_6, m_7}$}, which
            correspond to valid covers of the block DAG%
          }
  \labelFigure{mis-example}
\end{figure}


\paragraph{Applications}

\textcite{ScharwaechterEtAl:2007} appears to have pioneered the modeling of
\gls{instruction selection} as a \gls{MWIS} problem, although the main
contribution of their paper is the extension of \glspl{machine grammar} to
handle \gls{multi-output.ic} \glspl{instruction}.
%
The idea is to model such \glspl{instruction} using \gls!{complex.r}[
  \glspl{rule}], which each consists of multiple \glspl{production} -- one for
every result.
%
\labelPage{extended-machine-grammars}
%
In this dissertation, such \glspl{production} and their \glspl{pattern} are
called \gls!{proxy.r}[ \glspl{rule}]%
%
\footnote{%
  In the original paper, they are called \gls!{split.r}[ \glspl{rule}].
  %
  This is also is inconsistent with the terminology defined
  in \refSection{machine-grammars}, but we forsake consistency here in favor of
  simplicity.%
}
%
and \gls!{proxy.p}[ \glspl{pattern}], respectively, whereas \glspl{rule} with a
single \gls{production} and their \glspl{pattern} are called \gls!{simple.r}[
  \glspl{rule}] and \gls!{simple.p}[ \glspl{pattern}], respectively.
%
The \gls{rule} structure is also illustrated in
\refFigure{extended-machine-grammar-rule-anatomy}.

\begin{figure}
  \centering%
  \figureFont\figureFontSize%
  \newcommand{\simplePatternText}{%
    \trimbox{0 4pt 0 4pt}{%
      \begin{tabular}{@{}c@{}}
        simple\\[-1ex]
        pattern
      \end{tabular}%
    }%
  }
  \newcommand{\proxyPatternText}{%
    \trimbox{0 4pt 0 4pt}{%
      \begin{tabular}{@{}c@{}}
        proxy\\[-1ex]
        pattern
      \end{tabular}%
    }%
  }%
  \begin{displaymath}
    \underbrace{
      \mNT{A}
      \rightarrow
      \overbrace{
        \irCode{op} \ldots
      }^{\text{\simplePatternText}}
      \quad
      \text{cost}
      \quad
      \text{action}
    }_{\text{simple rule}}
    \qquad
    \underbrace{
      \langle \mNT{A}, \mNT{B}, \ldots \rangle
      \rightarrow
      \overbrace{
        \langle
          \:
          \overbrace{\irCode{op} \ldots}^{\text{\proxyPatternText}},
          \:
          \overbrace{\irCode{op} \ldots}^{\text{\proxyPatternText}},
          \:
          \ldots
          \:
        \rangle
      }^{\text{complex pattern}}
      \quad
      \text{cost}
      \quad
      \text{action}
    }_{\text{complex rule}}
  \end{displaymath}

  \vspace*{-\baselineskip}

  \caption{Anatomy of simple and complex rules in an extended machine grammar}
  \labelFigure{extended-machine-grammar-rule-anatomy}
\end{figure}

The \gls{proxy.p} \glspl{pattern} are matched individually together with the
\gls{simple.p} \glspl{pattern}.
%
After \gls{pattern matching}, \glspl{match} derived from \gls{proxy.p}
\glspl{pattern} are then either combined -- indicating use of the
\gls{complex.r} \gls{rule} -- or kept -- indicating use of \glspl{rule} that
only produce a single result.
%
This is done according to a heuristic that estimates the cost saved by using the
\gls{complex.r} \gls{rule} versus the cost incurred by having to \glsshort{node
  duplication}[e] \glspl{node} in common subexpressions.
%
Once these decisions have been taken, the \gls{interference graph} is built and
the \gls{MWIS} found using a greedy heuristic~\cite{SakaiEtAl:2003}.

The approach was later extended by \textcite{AhnEtAl:2009} to include scheduling
dependency conflicts between \gls{complex.p} \glspl{pattern} in order to
facilitate \gls{register allocation}.
%
Also, a shortcoming of both designs -- that \gls{complex.r} \glspl{rule} can
only consist of disconnected \glspl{pattern}, hence forbidding sharing of values
across the \gls{proxy.p} \glspl{pattern} -- was addressed by
\textcite{YounEtAl:2011} by introducing the use of index subscripts for the
\glspl{node} representing the input arguments.


\subsection{IP-based Approaches}

Several approaches model \gls{instruction selection} using \gls!{IP} -- often
also referred to as \gls!{ILP} -- which is a method for solving combinatorial
optimization problems (see \cite{Wolsey:1998} for an overview).
%
Formally, an \gls{IP} problem is defined as follows.
%
\begin{definition}[IP]
  Let $\mVector{c}$ and $\mVector{b}$ be integer vectors, $\mMatrix{A}$ be an
  integer matrix, and $\mVector{\mVar{x}}$ be a vector of integer
  \glspl{decision variable}.
  %
  Then
  %
  \begin{displaymath}
    \begin{array}{rl}
      \text{maximize} &
        \multirow{2}{*}{$\mVector{c}^{\transp} \mVector{\mVar{x}}$} \\
      \text{or minimize} & \\
      \text{subject to} & \mMatrix{A} \mVector{\mVar{x}} \leq \mVector{b}, \\
        & \mMatrix{A} \mVector{\mVar{x}} \in \mathbb{Z}^{n \times n}\!, \\
      \text{and} & \mVector{\mVar{x}} \in \mathbb{N}^n\!. \\
    \end{array}
  \end{displaymath}
\end{definition}

Such problems are NP-complete in general, but extensive research in the field
has made \gls{IP} a practical tool for solving problems containing tens of
thousands of \glspl{variable}.

In most \gls{IP}-based approaches, \gls{pattern selection} is modeled as
%
\begin{equation}
  \forall n \in N :
  \sum_{\mathclap{\substack{m \,\in\, M \text{ \st} \\ n \,\in\, \mCovers(m)}}}
  \mVector{\mVar{x}}[m] = 1,
  \labelEquation{pattern-selection-in-ip}
\end{equation}
%
where $N$ denotes the set of nodes in a \gls{block DAG}, $M$ denotes the
\gls{match set}, $\mCovers(m)$ denotes the set of \glspl{node} covered by
match~$m$, and $\mVector{\mVar{x}}[m]$ is a Boolean (\mbox{$\mSet{0, 1}$})
\gls{decision variable} indicating whether match~$m$ is selected.


\paragraph{Applications}

Although mostly known for their work in \gls!{integrated.cg}[ \gls{code
    generation}] -- meaning \gls{instruction selection}, \gls{instruction
  scheduling} and \gls{register allocation} is solved in unison --
\textcite{WilsonEtAl:1994} also pioneered the use of \gls{IP} for modeling
\gls{instruction selection}.
%
Their design performs \gls{global.is} \gls{instruction selection} on a
\gls{function graph} that has been augmented with additional copy
\glspl{operation} to represent potential \gls{register}
\glsplshort{spilling.r}.\!%
%
\footnote{%
  \Gls!{spilling.r} is the act of temporarily storing a \gls{register} value to
  memory in order to free up the \gls{register}.%
}
%
In most cases, not all copies are needed and therefore not all \glspl{operation}
must be covered.
%
Consequently, \citeauthor{WilsonEtAl:1994} model \gls{pattern selection} as
\mbox{$\sum_m \mVector{\mVar{x}}[m] \leq 1$}.

Another approach for \gls{integrated.cg} \gls{code generation} was made by
\textcite{BednarskiKessler:2006}, whose design was later reused by
\citeauthor{ErikssonEtAl:2008}~\cite{ErikssonEtAl:2008, ErikssonKessler:2012}.
%
Unlike all other \gls{instruction selection} approaches,
\citeauthor{BednarskiKessler:2006} combine \gls{pattern matching} and
\gls{pattern selection}.
%
Consequently, in addition to the \glspl{variable} that decide which
\glspl{match} are selected, their \gls{IP}~model also has \glspl{decision
  variable} that, for each \gls{match}, maps \glspl{node} and \glspl{edge} in
the \gls{block DAG} to \glspl{node} and \glspl{edge} in a \gls{pattern}.
%
An upper bound on the number of \glspl{match} needed is computed using a
heuristic.

An \gls{IP}-based approach for selecting \gls{multi-output.ic} \gls{instruction}
was introduced by \textcite{LeupersMarwedel:1995, LeupersMarwedel:1996}.
%
Each \gls{DAG}-shaped \gls{pattern} of a \gls{multi-output.ic} \gls{instruction}
is first decomposed into trees, which are used for covering an \gls{expression
  tree}.
%
After having found a \gls{least-cost.c} \gls{cover} (using
\refAlgorithm{opt-pat-sel-labeling-algorithm}), the \gls{expression tree} is
collapsed into a tree of \glspl!{super node}, where each \gls{super node}
represents a set of \glspl{node} in the \gls{expression tree} covered by the
same \gls{pattern}.
%
The problem is then to try to merge \glspl{super node} such that the combination
can be implemented using a \gls{multi-output.ic} \gls{instruction}.
%
This is often called \gls!{instruction compaction}, and as there is an abundance
of overlap between such combinations, \citeauthor{LeupersMarwedel:1995} solved
this problem using \gls{IP}.

\textcite{Leupers:2000:SIMD} later introduced another \gls{IP}-based approach
for selecting \gls{SIMD.i} \glspl{instruction}.
%
Again, each \gls{DAG}-shaped \gls{pattern} of a \gls{SIMD.i} \gls{instruction}
is decomposed into trees -- making it a \gls{disjoint-output.ic}
\gls{instruction} -- but in this approach they are used for covering a
\gls{block DAG} that has been transformed into \glspl{expression tree} through
\gls{edge splitting}.
%
Since the potential of using \gls{SIMD.i} \glspl{instruction} can be increased
by allowing them to cover \glspl{node} from multiple \glspl{expression tree},
covering each tree individually often leads to suboptimal code.
%
To address this problem, \citeauthor{Leupers:2000:SIMD} first extended a
\gls{machine grammar} with additional \glspl{nonterminal} to indicate whether a
\gls{SIMD.i} \gls{instruction} is used in covering a particular \gls{node} and
then modified \refAlgorithm{opt-pat-sel-labeling-algorithm} to compute the all
optimal \glspl{cover} instead of a single solution.
%
Once all \gls{least-cost.c} \glspl{cover} have been found for all
\glspl{expression tree} in a \gls{block}, an \gls{IP}~model is built to decide
how to make the best use of \gls{SIMD.i} \glspl{instruction} for this
\gls{block}.
%
\textcite{Leupers:2000:SIMD}'s model was later extended by
\textcite{TanakaEtAl:2003} to take the cost of data transfers into account by
extending the \gls{block DAG} with additional copy \glspl{node}, which is needed
for architectures with irregular \glspl{instruction set}.

The last \gls{IP}-based approach to be discussed is that of
\textcite{Gebotys:1997}, who applied to the theory of \glspl{Horn clause} to
\gls{code generation}.
%
A \gls!{Horn clause} is a disjunctive Boolean formula that contains at most one
positive (unnegated) literal, and \gls{IP}~models built using \glspl{Horn
  clause} can be solved in linear time~\cite{Hooker:1988}.
%
\citeauthor{Gebotys:1997} exploited this fact in developing an \gls{IP}~model
where \glspl{Horn clause} are applied to model \gls{register allocation}
\gls{instruction compaction}.
%
\Gls{pattern selection}, however, is modeled as in
\refEquation{pattern-selection-in-ip}.


\subsection{CP-based Approaches}

\glsreset{CP}

\Gls!{CP} is another method for solving combinatorial optimization problems,
which is discussed in detail in \refChapter{constraint-programming} (and
therefore only a brief introduction will be given here).
%
Like \gls{IP}, a \gls{constraint model} consists of a set of \glspl{decision
  variable}, a set of \glspl{constraint} over the \glspl{variable}, and
typically also an \gls{objective function} to either minimize or maximize.
%
A crucial difference, however, is that \glspl{constraint} are not limited to
linear equations.
%
Instead, relations among multiple \glspl{variable} are modeled using
\gls{global.c} \glspl{constraint}, which simplifies modeling and improves
solving.


\paragraph{Modeling Pattern Selection Using Global Constraints}

\Gls{pattern selection} can be modeled using a \gls{global.c} \gls{constraint}
called the \gls!{global cardinality constraint}.
%
The \gls{constraint}, referred to as $\mGCC$, constrains the number of
\glspl{decision variable} assigned a particular value (which may also be a
\gls{decision variable}).
%
We say that \mbox{$\mGCC(v, \mVar{x}_1, \ldots, \mVar{x}_k, \mVar{y})$} holds if
exactly $\mVar{y}$ \glspl{variable} in the set \mbox{$\mVar{x}_1, \ldots,
  \mVar{x}_k$} are assigned value~$v$ (for a formal definition, see
\refDefinition{gcc} on \refPageOfDefinition{gcc}).
%
For example, the constraint \mbox{$\mGCC(4, \mVar{x}_1 = \mSet{4}, \mVar{x}_2 =
  \mSet{5}, \mVar{x}_3 = \mSet{6}, \mVar{y} = \mSet{1})$} holds as exactly one
\glsshort{decision variable} is assigned the value~4.

To model \gls{pattern selection} using $\mGCC$, two new sets of
\glspl{decision variable} are needed.
%
Assume that $M$ denotes the \gls{match set} and $\mCovers(m)$ denotes the set of
\glspl{node} covered by \gls{match}~$m$.
%
Then, for each node~$n$ to be covered, \glsshort{decision variable}
\mbox{$\mVar{match}_n \in \mSetBuilder{m}{m \in M, i \in \mCovers(m)}$} decides
which \gls{match} covers~$n$.
%
Also, for each match~$m$, \glsshort{decision variable} \mbox{$\mVar{count}_m \in
  \mSet{0, |\mCovers(m)|}$} decides how many \glspl{node} are covered by $m$.
%
Hence each match covers either no \glspl{node} or all \glspl{node} in its
\gls{pattern}.
%
With these \glsplshort{decision variable}, \gls{pattern selection} can be
modeled as
%
\begin{equation}
  \forall m \in M :
  \mGCC(
    m,
    \cup_{n \in \mCovers(m)} \, \mVar{match}_n,
    \mVar{count}_m
  ),
  \labelEquation{pattern-selection-using-gcc}
\end{equation}
%
which is more restrictive than \refEquation{pattern-selection-in-ip} and thus
reduces solving time~\cite{FlochEtAl:2010}.


\paragraph{Applications}

The use of \gls{CP}-based \gls{instruction selection} appears to have been
pioneered by \textcite{BashfordLeupers:1999}.
%
To generate code for highly irregular \glspl{DSP},
\citeauthor{BashfordLeupers:1999} used \gls{CP} to model the interactions
between \gls{instruction selection} and the use of processor resources, such as
functional units and \glspl{register}.
%
Consequently, the approach essentially integrates \gls{instruction selection}
with a form of \gls{register allocation}.
%
Glossing over the details, the approach works as follows.
%
For each \gls{operation}, a so-called \gls!{FRT} is built which encodes the
resource requirements for the operands and result and the cost of every
\gls{instruction} that may be used to implement such \glspl{operation}.
%
Taking a \gls{block DAG} as input, the problem is to cover all \glspl{node}
using \glspl{FRT} such that all resource requirements are fulfilled.
%
Special resources are available for \glspl{instruction} that cover multiple
\glspl{node}, allowing adjacent \glspl{node} to be covered by the same
\gls{instruction}.
%
To solve this problem, \citeauthor{BashfordLeupers:1999} modeled it as a
\gls{constraint model}.

\textcite{MartinEtAl:2009, MartinEtAl:2012} developed a \gls{constraint model}
that integrates \gls{instruction selection} and \gls{instruction scheduling}.
%
Because they target \glspl!{ASIP}, the \glspl{pattern} are not predefined but
must be found prior to \gls{instruction selection}.
%
For this task \citeauthor{MartinEtAl:2009} applied a \gls{CP}-based
\glshyphened{pattern matching} algorithm, described
in~\cite{WolinskiKuchcinski:2007}.
%
\Gls{pattern selection} is modeled as \refEquation{pattern-selection-in-ip} in
another \gls{constraint model}, which was later improved by
\textcite{FlochEtAl:2010} who replaced the \gls{pattern selection}
\glspl{constraint} with \refEquation{pattern-selection-using-gcc}, and then
extended by \textcite{ArslanKuchcinski:2014} for targeting \gls{VLIW}
processors%
%
\footnote{%
  A \gls!{VLIW}[ processor] is a processor that executes multiple
  \glspl{instruction} in parallel, where the schedule has been computed by the
  \gls{compiler} and is part of the assembly code.%
}
%
with \gls{SIMD.i} \glspl{instruction}.
%
Selection of such \glspl{instruction} is done by first splitting the
\glspl{pattern} into multiple tree-shaped \glspl{pattern}, which are matched
individually, and then enforcing that two selected \glspl{match} belonging to
the same \gls{instruction} must be scheduled in the same cycle.

\textcite{Beg:2013} developed a \gls{constraint model} that, unlike the
approaches above, only concerns \gls{instruction selection}.
%
Both \gls{pattern matching} and \gls{pattern selection} are integrated into the
same \gls{constraint model}, and \citeauthor{Beg:2013} also applied
\refAlgorithm{aho-etal-cost-algorithm} in computing an upper bound on the cost
in order to improve solving.


\subsection{Limitations of DAG covering}

Although \glspl{DAG covering} addresses the issue of whether to \glsshort{edge
  splitting} or \glsshort{node duplication}[e] common subexpressions within a
\gls{block}, the problem still remains for expressions that are spread across
multiple \glspl{block}.
%
To fully address this problem, one must resort to \glspl{graph covering}.

\begin{filecontents*}{block-dags-limit-example.c}
int f(int* A, int* B, int N) {
  int s = 0;
  for (int i = 0; i < N; i++) {
    s = s + A[i] * B[i];
  }
  return s;
}
\end{filecontents*}

\begin{figure}
  \centering%
  \subcaptionbox{C code\labelFigure{block-dags-limit-example-c}}%
                {%
                  \begin{lstpage}{56mm}%
                    \lstinputlisting[language=c]{block-dags-limit-example.c}%
                  \end{lstpage}%
                }%
  \hfill%
  \subcaptionbox{%
                  Block graphs involving variable~\opVar{s}.
                  %
                  For brevity, the subtrees concerning \irCode*{A[i]}
                  and \irCode*{B[i]} are not included%
                  \labelFigure{block-dags-limit-dags}%
                }%
                [60mm]%
                {%
                  \input{figures/existing-isel-techniques-and-reps/%
                    block-dags-limit-example-dags%
                  }%
                }

  \vspace{\betweensubfigures}

  \subcaptionbox%
    {%
      Rules.
      %
      For brevity, the actions are not included.
      %
      $\mNT{Null}$ is a dummy nonterminal since \opRet*{} does not return
      anything, yet all productions must have a result.
      %
      All rules are assumed to have equal cost%
    }%
    [\textwidth]%
    {%
      \figureFontSize%
      \newcolumntype{L}{@{}l@{}}%
      \begin{tabular}{r@{ $\rightarrow$ }l@{\hspace{3em}}r@{ $\rightarrow$ }lc}
        \toprule
        \multicolumn{5}{c}{\tabhead rules}\\
        \midrule
        $\mNT{Reg}$ & \irCode{const}
          & $\mNT{SReg}$
          & \multicolumn{2}{L}{$\opMul \; \mNT{Reg} \; \mNT{Reg}$}\\
        $\mNT{SReg}$ & \irCode{const}
          & $\mNT{Null}$ & \multicolumn{2}{L}{$\opRet* \; \mNT{Reg}$}\\
        $\mNT{Reg}$ & $\opAdd \; \mNT{Reg} \; \mNT{Reg}$
          & $\mNT{Reg}$  & $\mNT{SReg}$ & $(r \ll 1)$\\
        $\mNT{SReg}$ & $\opAdd \; \mNT{SReg} \; \mNT{SReg}$
          & $\mNT{SReg}$ & $\mNT{Reg}$  & $(r \gg 1)$\\
        \bottomrule
      \end{tabular}%
    }

  \caption[Example illustrating the need for global instruction selection]%
          {%
            Example illustrating the need for global instruction selection%
          }
  \labelFigure{block-dags-limit-example}
\end{figure}

This also applies to other situations where decisions made for one \gls{block}
can inhibit subsequent decisions for other \glspl{block}, such as enforcing
specific storage locations or value modes.
%
For example, \refFigure{block-dags-limit-example} shows a \gls{function} that
multiplies the elements of two arrays and sums the results.
%
Assume that the arrays consist of fixed-point values.
%
For efficiency, a common idiosyncracy in many \glspl{DSP} is that multiplication
of two fixed-point values return a value that is shifted one bit to the left.
%
For such \glspl{target machine}, both the value~\irCode*{0} and the accumulator
\gls{variable}~\irVar*{s} should be in shifted mode throughout the entire
\gls{function}, and only restored into normal mode upon return.
%
Otherwise the accumulated value would be needlessly shifted back and forth
within the loop.
%
Achieving this, however, is difficult when limited to covering only a single
\gls{block DAG} at a time.
%
Assume for example that the function had no multiplication.
%
In that case, deciding to load value~\irCode*{0} in shifted mode would instead
lower code quality as the value would needlessly have to be shifted back before
returning, which takes an extra \gls{instruction}.

Lastly, most of these approaches are restricted to tree-shaped \glspl{pattern},
meaning they only support \gls{single-output.ic} \glspl{instruction}.
%
Many \glspl{instruction set}, however, contain \gls{multi-output.ic}
\glspl{instruction} and require \gls{DAG}-shaped \glspl{pattern}, which violate
underlying assumptions made by many of the aforementioned approaches.


\section{Graph Covering}
\labelSection{ex-isel-rep-graph-covering}

The most general principle is called \gls{graph covering}, where entire
\glspl{function} are modeled as \glspl{function graph} and \glspl{instruction}
are allowed to be modeled using any shape of \glspl{pattern}.
%
Compared to the other \glspl{principle}, there exist relatively few applications
of \gls{graph covering}, the most known of which appeared in the 2000s.

The main advantage is that \gls{graph covering} support selection of
\gls{inter-block.ic} \glspl{instruction}, whose behavior entail both control and
data flow and must therefore be captured as \glspl{pattern graph}.
%
The representations typically used, however, only model data flow, which
stresses the need for new representations in order to handle the
\glspl{instruction} of modern and forthcoming processors, which grow
increasingly complex.

First we will look at a few representations for capturing entire
\glspl{function} as \glspl{graph}.
%
Because they result in a relatively high \glspl{node} count, such
representations are colloquially referred to as \glspl!{sea-of-nodes IR}.


\subsection{Sea-of-nodes IRs}

In the context of \gls{instruction selection}, there are two \glspl{sea-of-nodes
  IR} that are of interest.
%
The first captures the data flow for entire \glspl{function}, and the second is
an extension of the first in order to also capture control flow.


\paragraph{Capturing Data Flow of Entire Functions}

In order to simply many \gls{compiler} tasks, \textcite{CytronEtAl:1991}
introduced a \gls{program} representation called \gls!{SSA} \glsEmph{form}.

\begin{filecontents*}{fact.c}
int factorial(int n) {
  entry:
    int f = 1;
  head:
    if (n <= 1) goto end;
  body:
    f = f * n;
    n = n - 1;
    goto head;
  end:
    return f;
}
\end{filecontents*}

\begin{filecontents*}{fact-ssa.c}
int factorial(int $\irVar{n}[1]$) {
  entry:
    int $\irVar{f}[1]$ = 1;
  head:
    int $\irVar{f}[2]$ = $\mPhi$($\irVar{f}[1]$:entry, $\irVar{f}[3]$:body);
    int $\irVar{n}[2]$ = $\mPhi$($\irVar{n}[1]$:entry, $\irVar{n}[3]$:body);
    if ($\irVar{n}[2]$ <= 1) goto end;
  body:
    int $\irVar{f}[3]$ = $\irVar{f}[2]$ * $\irVar{n}[2]$;
    int $\irVar{n}[3]$ = $\irVar{n}[2]$ - 1;
    goto head;
  end:
    return $\irVar{f}[2]$;
}
\end{filecontents*}

\begin{figure}
  \centering%
  \mbox{}%
  \hfill%
  \subcaptionbox{%
                  C implementation of factorial%
                  \labelFigure{sea-of-nodes-example-c}%
                }%
                [50mm]%
                {%
                  \begin{lstpage}{45mm}%
                    \lstinputlisting[language=c]{fact.c}%
                  \end{lstpage}%
                }%
  \hfill\hfill%
  \subcaptionbox{Code in SSA form\labelFigure{sea-of-nodes-example-ssa-c}}%
                {%
                  \begin{lstpage}{59mm}%
                    \lstinputlisting[language=c,mathescape]{fact-ssa.c}%
                  \end{lstpage}%
                }%
  \hfill%
  \mbox{}

  \vspace*{\betweensubfigures}

  \subcaptionbox{SSA graph\labelFigure{sea-of-nodes-example-ssa-graph}}%
                {%
                  \input{figures/existing-isel-techniques-and-reps/%
                    sea-of-nodes-example-ssa-graph%
                  }%
                }

  \caption{Example of an SSA graph}
  \labelFigure{ssa-example}
\end{figure}

A \gls{program} is said to be in \gls{SSA}~form if every \gls{variable} is
defined exactly once.
%
For example, the \gls{function} shown in \refFigure{sea-of-nodes-example-c}
is not in \gls{SSA}~form as \glspl{variable}~\irCode*{f} and~\irCode*{n} are
redefined within the loop.
%
By introducing new \glspl{variable} and connecting these using
\glspl!{phi-function} where the value depends on control flow, the
\gls{function} can be rewritten into \gls{SSA}~form, as shown in
\refFigure{sea-of-nodes-example-ssa-c}.

From an \gls{SSA}-based \gls{function}, we can construct a \gls{data-flow graph}
called the \gls!{SSA graph}~\cite{GerlekEtAl:1995}.
%
Like in \glspl{data-flow graph}, each \gls{operation} in the \gls{function}
(including the \glspl{phi-function}) is represented as a \gls{node}.
%
These nodes are connected using \glspl{data-flow edge}, ignoring the fact that
the \glspl{operation} may belong to different \glspl{block}.
%
For the example above, this results in the \gls{SSA graph} shown in
\refFigure{sea-of-nodes-example-ssa-graph}.

Since \glspl{operation} are not pre-assigned to specific \glspl{block}, the same
\gls{IR} can be used for performing \gls{global code motion}.
%
If an \gls{operation} should not be allowed to be moved to another \gls{block}
-- like with \glspl{phi-function} and returns -- then an \gls{edge} can be used
to connect the \gls{operation} with its original \gls{block} placement.


\paragraph{Capturing Both Data And Control Flow}

\textcite{ClickPaleczny:1995} introduced a \gls{sea-of-nodes IR} that captures
both data and control flow.
%
The data flow is modeled exactly as in the \gls{SSA graph}, and the control flow
is captured using \glspl{node} to represent the \glspl{block} in the
\gls{function} and \glspl{edge} to represent jumps between \glspl{block}.
%
To capture dependencies between the data and control flow -- for example, when
the target of a jump depends on a Boolean value -- such jumps flow through
special \emph{if}~\glspl{node}.
%
An example of the \gls{Click-Paleczny graph} is shown in
\refFigure{click-paleczny-graph-example}.

\begin{figure}
  \centering%
  \input{%
    figures/existing-isel-techniques-and-reps/%
    sea-of-nodes-example-click-paleczny-graph%
  }

  \caption[Example of a Click-Paleczny graph]%
          {%
            Example of a Click-Paleczny graph, corresponding to the program
            shown in \refFigure{ssa-example}.
            %
            Thin-lined nodes and edges denote data operations and data flow.
            %
            Thick-lined nodes and edges denote control operations and control
            flow.
            %
            Dashed edges indicate to which block an operation belongs%
          }
  \labelFigure{click-paleczny-graph-example}
\end{figure}


\subsection{Pattern Selection on the Click-Paleczny Graph}

\textcite{PalecznyEtAl:2001} introduced an approach for performing
\gls{instruction selection} based on the \gls{Click-Paleczny graph}.

The approach first divides the \gls{function graph} into a set of possibly
overlapping \glspl{expression tree}.
%
This is done by labeling certain \glspl{node} in the \gls{function graph} as
tree roots.
%
Root candidates are \glspl{node} representing \glspl{operation} whose result are
shared or \glspl{operation} with side effects and may therefore not be
\glsshort{node duplication}[ed].
%
The selection of roots is geared towards duplicating address computations and
other expressions that can be subsumed into a single \gls{instruction}.
%
Once labeled, each \gls{expression tree} is covered using a variant of
\refAlgorithm{opt-pat-sel-labeling-algorithm}.
%
The \glspl{instruction} are then emitted and placed in \glspl{block} using a
method described in~\cite{Click:1995}.

Although the \gls{function} is represented as a \gls{function graph}, the
\glspl{instruction} must still be modeled as \glspl{pattern tree}.
%
Consequently, only \gls{single-output.ic} \glspl{instruction} can be selected
using this approach.


\subsection{PBQP-based Approaches}

Similar to \gls{IP}-based approaches, another method of modeling
\gls{instruction selection} is to model it as a \gls!{PBQP}.
%
First introduced by \textcite{ScholzEckstein:2002} to model and solve
\gls{register allocation}, \gls{PBQP} is a variant of the \gls!{QAP}, which is a
fundamental combinatorial optimization problem in the field of operations
research (see \cite{LoiolaEtAl:2007} for a survey).
%
Although both problems are NP-complete in general, a subclass of \gls{PBQP} can
be solved in linear time which inspired \citeauthor{ScholzEckstein:2002} in
developing a greedy, linear-time solver.

Formally, a \gls{PBQP} is defined as follows.
%
\begin{definition}[PBQP]
  Let \mbox{$\mVector{c}_1, \ldots, \mVector{c}_n$} be integer vectors, and let
  \mbox{$\mMatrix{C}_{1,1}, \mMatrix{C}_{1,2}, \ldots, \mMatrix{C}_{1,n}$},
  \mbox{$\mMatrix{C}_{2,2}, \ldots, \mMatrix{C}_{n,n}$}
  be integer matrices of size \mbox{$|c_i| \times |c_j|$} for \mbox{$i = 1,
    \ldots, n$} and \mbox{$j = i, \ldots, n$}.
  %
  Let also \mbox{$\mVector{\mVar{x}}_1, \ldots, \mVector{\mVar{x}}_n$} be
  vectors of integer \glspl{decision variable}, where \mbox{$\mVar{x}_i \in
    \mSet{0, 1}^{|c_i|}$} for \mbox{$i = 1, \ldots, n$}.
  %
  Then
  %
  \begin{displaymath}
    \begin{array}{rl}
      \text{minimize}
        & \displaystyle
          \sum_{\mathclap{1 \leq i < j \leq n}}
          \mVector{\mVar{x}}_i^{\transp} \mMatrix{C}_{ij} \, \mVector{\mVar{x}}_j +
          \sum_{\mathclap{1 \leq i \leq n}}
          \mVector{c}_i^{\transp} \mVector{\mVar{x}}_i, \\
      \text{subject to}
        & \forall 1 \leq i \leq n :
          \mVector{1}^{\transp} \mVector{\mVar{x}}_i = 1. \\
    \end{array}
  \end{displaymath}
\end{definition}

Intuitively, one can interpret this definition as follows.
%
Assume a problem consists of $n$~decisions, each with $k$~choices.
%
Then $\mVector{\mVar{x}}_i$ is a \gls!{decision variable} with $k$~elements,
where \mbox{$\mVector{\mVar{x}}_i[j] = 1$} means that choice~$j$ has been
selected for decision~$i$.
%
For each \gls{decision variable}, the condition
\raisebox{0pt}[\height-2pt]{$\mVector{1}^{\transp} \mVector{\mVar{x}}_i = 1$}
ensures that exactly one choice is selected.
%
The cost of selecting a particular choice for decision~$i$ is represented
through a cost vector~$\mVector{c}_i$, and the cost of combining two
decisions~$i$ and~$j$ are represented through a cost matrix~$\mMatrix{C}_{ij}$.

In this context, $\mVector{\mVar{x}}_i$ decides whether to select a particular \gls{match}
to cover \gls{node}~$i$, $\mVector{c}_i$ contains the cost for each such
\gls{match}, and $\mMatrix{C}_{ij}$ contains the cost of additional
\glspl{instruction} that may need to be selected due to certain combinations of
\glspl{match}.
%
For example, assume two nodes~$i$ and~$j$ where $j$ depends on $i$.
%
Assume further that the \glspl{instruction} are represented as a \gls{machine
  grammar}, and that~$i$ and~$j$ can be covered using two \glspl{rule}, $r_i$
and $r_j$, with \glspl{production} \mbox{$\mNT{A} \rightarrow
  \irCode*{op}_{\mathit{i}} \; \mNT{A} \; \mNT{A}$} and \mbox{$\mNT{B}
  \rightarrow \irCode*{op}_{\mathit{j}} \; \mNT{B} \; \mNT{B}$}, respectively.
%
Since the result of $r_i$ does not match the operands of $r_j$, this \gls{rule}
combination requires a \gls{chain.r} \gls{rule} -- or a chain of these, if
necessary -- that derives $\mNT{B}$ from $\mNT{A}$.
%
Illegal combinations are prevented by assigning infinite cost.
%
An example of a \gls{PBQP} instance is shown in \refFigure{pbqp-example}.

\begin{figure}
  \centering%
  \mbox{}%
  \hfill%
  \subcaptionbox%
    {%
      Rules.
      %
      For brevity, the actions are not included%
    }%
    [68mm]%
    {%
      \figureFontSize%
      \begin{tabular}{r@{ $\rightarrow$ }lc}
        \toprule
        \multicolumn{2}{c}{\tabhead rules} & \tabhead cost\\
        \midrule
        $\mNT{Reg}$ & \irCode{var} & 0\\
        $\mNT{Reg}$ & $\opAdd \; \mNT{Reg} \; \mNT{Reg}$ & 1\\
        $\mNT{Reg}$ & $\opLoad \; \mNT{Addr}$ & 3\\
        $\mNT{Reg}$ & $\opLoad \; \opAdd \; \mNT{Reg} \; \mNT{Reg}$ & 5\\
        $\mNT{Addr}$ & $\mNT{Reg}$ & 2\\
        \bottomrule
      \end{tabular}%
    }%
  \hfill%
  \subcaptionbox{%
                  SSA graph%
                  \labelFigure{pbqp-example-ssa-graph}%
                }{%
                  \input{figures/existing-isel-techniques-and-reps/%
                    pbqp-example-ssa-graph%
                  }%
                }%
  \hfill%
  \mbox{}

  \vspace{\betweensubfigures}

  \subcaptionbox{%
                  PBQP instance.
                  %
                  The rows and columns in the cost vectors and matrices are
                  labeled with the matches they represent.
                  %
                  Cost matrices for uninteresting combinations are assumed to
                  consist of~0s%
                  \labelFigure{pbqp-example-instance}%
                }{%
                  \figureFontSize%
                  \apptocmd{\irFont}{\scriptsize}{}{}%
                  \newcommand{\lskip}{0.1ex}%
                  \newcommand{\newlineskip}{-1em}%
                  \newcolumntype{R}{r@{\;=\;}}%
                  \BAnewcolumntype{C}{>{\;}c<{\:}}%
                  \BAnewcolumntype{X}{@{\;\;\,}c}%
                  \newcommand{\mlabel}[1]{\raisebox{.2ex}{$\scriptstyle #1$}}%
                  \begin{minipage}{22mm}%
                    \begin{displaymath}
                      \begin{array}{r@{\;\in\;}l}
                        \mVector{\mVar{x}}_{\opVar{a}} & \mSet{0, 1}    \\[.7ex]
                        \mVector{\mVar{x}}_{\opVar{b}} & \mSet{0, 1}    \\[.7ex]
                        \mVector{\mVar{x}}_{\opAdd}   & \mSet{0, 1}^{2} \\[.7ex]
                        \mVector{\mVar{x}}_{\opLoad}  & \mSet{0, 1}^{2}
                      \end{array}
                    \end{displaymath}%
                  \end{minipage}%
                  \hspace{5mm}%
                  \trimbox{0 8pt 0 0}{%
                    \begin{minipage}{27mm}%
                      \begin{displaymath}
                        \begin{array}{Rl}
                          \mVector{c}_{\opVar{a}}
                            & \begin{adjblockarray}{cc}{1.3ex}
                                \begin{block}{[C]X}
                                  0 & \mlabel{m_1} \\
                                \end{block}
                              \end{adjblockarray} \\[1.5ex]
                          \mVector{c}_{\opVar{b}}
                            & \begin{adjblockarray}{cc}{1.3ex}
                                \begin{block}{[C]X}
                                  0 & \mlabel{m_2} \\
                                \end{block}
                              \end{adjblockarray} \\[1.5ex]
                          \mVector{c}_{\opAdd}
                            & \begin{adjblockarray}{cc}{1.3ex}
                                \begin{block}{[C]X}
                                  1 & \mlabel{m_3} \\
                                  5 & \mlabel{m_5} \\
                                \end{block}
                              \end{adjblockarray} \\[3ex]
                          \mVector{c}_{\opLoad}
                            & \begin{adjblockarray}{cc}{1.3ex}
                                \begin{block}{[C]X}
                                  3 & \mlabel{m_4} \\
                                  5 & \mlabel{m_5} \\
                                \end{block}
                              \end{adjblockarray}
                        \end{array}
                      \end{displaymath}%
                    \end{minipage}%
                  }%
                  \hspace{5mm}%
                  \begin{minipage}{33mm}%
                    \begin{displaymath}
                      \begin{array}{Rc}
                        \mMatrix{C}_{\opVar{a} \opAdd}
                          & \begin{adjblockarray}{ccc}{-0.1ex}
                              \scriptstyle m_3 & \scriptstyle m_5 & \\[\lskip]
                              \begin{block}{[cc]X}
                                0 & 0 & \mlabel{m_1} \\
                              \end{block}
                            \end{adjblockarray} \\[-2.2ex]
                        \mMatrix{C}_{\opVar{b} \opAdd}
                          & \begin{adjblockarray}{ccc}{-0.1ex}
                              \scriptstyle m_3 & \scriptstyle m_5 & \\[\lskip]
                              \begin{block}{[cc]X}
                                0 & 0 & \mlabel{m_1} \\
                              \end{block}
                            \end{adjblockarray} \\[-2.2ex]
                        \mMatrix{C}_{\opAdd\opLoad}
                          & \begin{adjblockarray}{ccc}{0ex}
                              \scriptstyle m_4 & \scriptstyle m_5 & \\[\lskip]
                              \begin{block}{[cc]X}
                                2      & \infty & \mlabel{m_3} \\
                                \infty & 0      & \mlabel{m_5} \\
                              \end{block}
                            \end{adjblockarray}
                      \end{array}
                    \end{displaymath}%
                  \end{minipage}%
                }

  \caption{Example of modeling instruction selection as a PBQP}
  \labelFigure{pbqp-example}
\end{figure}


\paragraph{Handling DAG-shaped Patterns}

The \gls{PBQP} model above assumes that all \glspl{pattern} are shaped as trees.
%
To handle \gls{DAG}-shaped \glspl{pattern}, the model must be extended.
%
First assume an extended \gls{grammar} where \gls{multi-output.ic}
\glspl{instruction} are described using \gls{complex.r} \glspl{rule} (described
on \refPage{extended-machine-grammars}, see also
\refFigure{extended-machine-grammar-rule-anatomy}).
%
For each combination of \glspl{match} derived from \gls{proxy.r} \glspl{rule}
that can be combined into an instance of a \gls{complex.r} \gls{rule}, a
\gls!{complex.m}[ \gls{match}] is created.
%
Each \gls{complex.m} \gls{match}~$i$ in turn introduces a \gls{decision
  variable}~\raisebox{0pt}[\height-2pt]{$\mVector{\mVar{x}}_i \in \mSet{0,
    1}^2$} to decide whether $i$ is selected.
%
Because of the \raisebox{0pt}[\height-2pt]{$\mVector{1}^{\transp}
  \mVector{\mVar{x}}_i = 1$} condition, every such \gls{variable} has exactly
two elements (one representing \emph{on} and the other \emph{off}).
%
Like with the \gls{simple.r} \glspl{rule}, the costs of selecting a
\gls{complex.m} \gls{rule} and interactions between these -- for example, two
\gls{complex.m} \glspl{match} are not allowed to overlap or cause cyclic data
dependencies -- are represented through the cost vectors and matrices.

In order to select a \gls{complex.r} \gls{rule}, all of its \gls{proxy.r}
\glspl{rule} must also be selected.
%
This is achieved by first extending, for each node~$i$, the domain of its
\gls{decision variable}~$\mVector{\mVar{x}}_i$ with \glspl{match} derived from
\gls{proxy.r} \glspl{rule}.
%
Then a new set of cost matrices~$D_{ij}$ is created such that, for a node~$i$
and \gls{complex.m} \gls{match}~$j$, the costs are 0 if
\mbox{$\mVector{\mVar{x}}_j = \text{\emph{off}}$} or $\mVector{\mVar{x}}_i$ is
set to a \gls{proxy.r} \gls{rule} associated with $j$.
%
Otherwise the costs are $\infty$.
%
Consequently, if a \gls{complex.m} \gls{match} covering some node~$n$ is
selected, then the only choice for $\mVector{\mVar{x}}_n$ with non-infinite cost
is an associated \gls{proxy.r} \gls{rule}.
%
The \gls{PBQP} model is thus augmented with another sum
%
\begin{equation}
  \sum_{\mathclap{i \,\in\, N, \: j \,\in\, M}}
  \mVector{\mVar{x}}_i^{\transp} D_{ij} \, \mVector{\mVar{x}}_j
\end{equation}
%
where $N$ denotes the set of \glspl{node} in the \gls{SSA graph} and $M$ denotes
the set of \gls{complex.m} \glspl{match}.

This alone, however, allows \glspl{solution} where all \gls{proxy.r}
\glspl{rule} but none of the \gls{complex.r} \glspl{rule} are selected.
%
This is resolved by assigning an artificially large cost~$K$ to the selection of
\gls{proxy.r} \glspl{rule}, which is offset when selecting the corresponding
\gls{complex.r} \gls{rule}.
%
For example, if a \gls{complex.r} \gls{rule}~$r$ with cost~2 consists of three
\gls{proxy.r} \glspl{rule}, then the new cost of selecting $r$ is \mbox{$2 -
  3K$}.


\paragraph{Applications}

\textcite{EcksteinEtAl:2003} were first with modeling \gls{instruction
  selection} as a \gls{PBQP}, and \textcite{EbnerEtAl:2008} extended their
approach to support \gls{DAG}-shaped \glspl{pattern}.
%
\textcite{BuchwaldZwinkau:2010} reused the \gls{PBQP} model but replaced the use
of \glspl{machine grammar} with rewrite rules based on algebraic graph
transformations~\cite{LoweEhrig:1991}.


\subsection{The VF2 Algorithm}
\labelSection{ex-isel-rep-vf2-algorithm}

For \gls{pattern matching}, most combinatorial approaches apply the
\gls{VF2}~algorithm~\cite{CordellaEtAl:2001}.
%
\def\fG{G_{\mathsc{f}}}%
\def\pG{G_{\mathsc{p}}}%
\def\fN{N_{\mathsc{f}}}%
\def\fE{E_{\mathsc{f}}}%
\def\pN{N_{\mathsc{p}}}%
\def\pE{E_{\mathsc{p}}}%
\def\mCandSet{P(s)}%
\def\mSynRuleCheck{F_{\mathsc{syn}}}%
\def\mSemRuleCheck{F_{\mathsc{sem}}}%
\def\fTout{T_{\!\mathsc{f}}^{\hspace{.5pt}\mathsc{out}}}%
\def\pTout{T_{\!\mathsc{p}}^{\hspace{.5pt}\mathsc{out}}}%
\def\fTin{T_{\!\mathsc{f}}^{\mathsc{in}}}%
\def\pTin{T_{\!\mathsc{p}}^{\mathsc{in}}}%
\begin{algorithm}[t]
  \DeclFunction{FindMatches}{function graph $\fG = \mPair{\fN}{\fE}$,
                             pattern graph $\pG = \mPair{\pN}{\pE}$}%
  {%
    $M$ \Assign $\emptyset$\;
    \Call{FindMatchRec}{$\emptyset$}\;
    \Return{$M$}\;
    \DeclFunction{FindMatchRec}{set $s$ of mappings}%
    {%
      \If(\tcp*[f]{if match found}){$\mCard{s} = \mCard{\pN}$}{%
        $M$ \Assign $M \cup s$\;
      }
      \Else{%
        compute mapping candidate set $\mCandSet$
        \tcp*{see \refDefinition{pm-cand-set}}
        \ForEach{$\mPair{n}{m} \in \mCandSet$}{%
          \If(\tcp*[f]{see \refDefinitionRange{pm-syntactic-rules}%
                                              {pm-semantic-rules}})%
             {$\mSynRuleCheck(s, n, m) \mAnd \mSemRuleCheck(s, n, m)$}%
          {%
            \Call{FindMatchRec}{$s \cup \mPair{n}{m}$}\;
          }
        }
      }
    }
  }

  \caption{The VF2 algorithm}
  \labelAlgorithm{vf2-algorithm}
\end{algorithm}
%
The algorithm, given in \refAlgorithm{vf2-algorithm}, recursively checks every
\gls{node}-mapping candidate and applies a set of rules for checking whether the
current mapping will yield a \gls{match}.
%
The rules are categorized into \emph{syntactic} and \emph{semantic} rules.
%
The syntactic rules check that the \gls{graph} structure is preserved, and the
semantic rules check that \gls{node} and \gls{edge} attributes are compatible.
%
In the worst case, \mbox{$\mBigO(N! N)$} mappings need to be checked.


\paragraph{Computing the Mapping Candidate Set}

Given a \gls{function graph}~\mbox{$\fG = \mPair{\fN}{\fE}$}, a \gls{pattern
  graph}~\mbox{$\pG = \mPair{\pN}{\pE}$}, and a set~$s$ of mappings from
\glspl{node} in $\fG$ to \glspl{node} in $\pG$, the set~$\mCandSet$ of mapping
candidates under consideration for being added to $s$ is computed as follows.
%
\begin{definition}[Mapping Candidate Set]
  Let $\fN(s)$ and $\pN(s)$ denote the sets of \glspl{node} in $\fG$ and $\pG$,
  respectively, that appear in $s$.
  %
  Also let $\fTout$ and $\pTout$ denote the set of \glspl{node} in $\fG$ and
  $\pG$, respectively, that are targets of \glspl{edge} from \glspl{node}
  appearing in~$s$.
  %
  Likewise, let $\fTin$ and $\pTin$ denote the set of \glspl{node} in $\fG$ and
  $\pG$, respectively, that are sources of \glspl{edge} to \glspl{node}
  appearing in~$s$.
  %
  Then
  %
  \begin{displaymath}
    \mCandSet \equiv
    \left\{
    \begin{array}{ll}
        \mSetBuilder{\mPair{n}{m}}{n \in \fTout, m \in \pTout}
      & \text{if} \ \fTout \neq \emptyset \mAnd \pTout \neq \emptyset
        \hspace{-1pt}, \\
        \mSetBuilder{\mPair{n}{m}}{n \in \fTin, m \in \pTin}
      & \text{if} \ \fTin \neq \emptyset \mAnd \pTin \neq \emptyset
        \hspace{-1pt}, \\
        \mSetBuilder{\mPair{n}{m}}{
                                    n \in \fN \setminus \fN(s),
                                    m \in \pN \setminus \pN(s)
                                  }
      & \text{otherwise}.
    \end{array}
    \right.
  \end{displaymath}%
  \labelDefinition{pm-cand-set}%
\end{definition}
%
The last clause is needed when $\fG$ or $\pG$ consists of disconnected
\glspl{subgraph}.


\paragraph{Syntactic Rules}

\def\mSynPredRule{R_{\mathsc{pred}}}
\def\mSynSuccRule{R_{\mathsc{succ}}}
\def\mSynInRule{R_{\mathsc{in}}}
\def\mSynOutRule{R_{\mathsc{out}}}
\def\mSynNewRule{R_\mathsc{new}}

The syntactic rule check $\mSynRuleCheck$ consists of five rules:
%
\begin{equation}
  \mSynRuleCheck(s, n, m) \equiv
  \mSynPredRule(\ldots) \mAnd \mSynSuccRule(\ldots) \mAnd
  \mSynInRule(\ldots) \mAnd \mSynOutRule(\ldots) \mAnd \mSynNewRule(\ldots).
\end{equation}
%
The first two rules, $\mSynPredRule$ and $\mSynSuccRule$, check the consistency
of the partial \gls{match} obtained when the candidate~\mbox{$\mPair{n}{m}$} is
added to~$s$.
%
Intuitively, if there exists an \gls{edge} between two mapped \glspl{node} in
the \gls{pattern graph}, then a corresponding \gls{edge} must also exist in the
\gls{function graph}.\!%
%
\footnote{%
  In the paper \cite{CordellaEtAl:2001}, $\mSynPredRule$ and $\mSynSuccRule$
  also check the inverse -- that is, an \gls{edge} between two mapped
  \glspl{node} in the \gls{function graph} must have a corresponding \gls{edge}
  in the \gls{pattern graph} -- thus requiring that $\pG$ is an induced
  \gls{subgraph} of $\fG$.
  %
  In this context, however, it is sufficient with only maintaining the structure
  of~$\pG$, and the condition above has therefore been removed from
  \refDefinition{pm-syntactic-rules}.
}
%
The next two rules, $\mSynInRule$ and $\mSynOutRule$, are 1-look-ahead rules
that check whether there exist a sufficient number of unmapped \glspl{node}
adjacent to $n$ in the \gls{function graph} for mapping the remaining
\glspl{node} adjacent to $m$ in the \gls{pattern graph}.
%
The last rule, $\mSynNewRule$, is similar to $\mSynInRule$ and $\mSynOutRule$
but perform a 2-look-ahead check.
%
Formally, the rules are defined as follows.
%
\begin{definition}[Syntactic Rules]
  \def\fT{T_{\!\mathsc{f}}}%
  \def\pT{T_{\!\mathsc{p}}}%
  \def\fNegN{\mkern2mu\overline{\mkern-2mu N}_{\mathsc{f}}}%
  \def\pNegN{\mkern2mu\overline{\mkern-2mu N}_{\mathsc{p}}}%

  Let $\mPred(G, n)$ and $\mSucc(G, n)$ denote the sets of predecessor and
  successor \glspl{node}, respectively, to \gls{node}~$n$ in \gls{graph}~$G$.
  %
  Also let \mbox{$\fT = \fTin \cup \fTout$} and \mbox{$\fNegN = \fN \setminus
    \fN(s) \setminus \fT$}, with similar definitions for $\pT$ and $\pNegN$.
  %
  Then
  %
  \begin{displaymath}
    \begin{array}{r@{}l}
        \mSynPredRule(s, n, m) \equiv \mbox{}
      & \forall m' \in \pN(s) \cap \mPred(\pG, m),
        \exists n' \in \fN(s) :
        \mPair{n'}{m'} \in s, \\[\abovedisplayskip]

        \mSynSuccRule(s, n, m) \equiv \mbox{}
      & \forall m' \in \pN(s) \cap \mSucc(\pG, m),
        \exists n' \in \fN(s) :
        \mPair{n'}{m'} \in s, \\[\abovedisplayskip]

        \mSynInRule(s, n, m) \equiv \mbox{}
      & \mCard{\mSucc(\fN, n) \cap \fTin} \geq
        \mCard{\mSucc(\pN, n) \cap \pTin} \mAnd \mbox{} \\
      & \mCard{\mPred(\fN, n) \cap \fTin} \geq
        \mCard{\mPred(\pN, n) \cap \pTin}, \\[\abovedisplayskip]

        \mSynOutRule(s, n, m) \equiv \mbox{}
      & \mCard{\mSucc(\fN, n) \cap \fTout} \geq
        \mCard{\mSucc(\pN, n) \cap \pTout} \mAnd \mbox{} \\
      & \mCard{\mPred(\fN, n) \cap \fTout} \geq
        \mCard{\mPred(\pN, n) \cap \pTout}, \\[\abovedisplayskip]

        \mSynNewRule(s, n, m) \equiv \mbox{}
      & \mCard{\fNegN \cap \mPred(\fG, n)} \geq
        \mCard{\pNegN \cap \mPred(\pG, m)} \mAnd \mbox{} \\
      & \mCard{\fNegN \cap \mSucc(\fG, n)} \geq
        \mCard{\pNegN \cap \mSucc(\pG, m)}.
    \end{array}
  \end{displaymath}%
  \labelDefinition{pm-syntactic-rules}%
\end{definition}


\paragraph{Semantic Rules}

\def\mSemNodeRule{R_{\mathsc{node}}}
\def\mSemEdgeRule{R_{\mathsc{edge}}}

The semantic rules check two properties:
%
\begin{equation}
  \mSemRuleCheck(s, n, m) \equiv
  \mSemNodeRule(\ldots) \mAnd \mSemEdgeRule(\ldots).
\end{equation}
%
The first rule checks that the \gls{node} types are compatible, while the second
rule checks compatibility between \glspl{edge}.
%
Formally, the rules are defined as follows.
%
\begin{definition}[Semantic Rules]
  Let $\mVFTwoAttrCmp$ represent a binary relation for comparing the
  compatibility between \glspl{node} and \glspl{edge}.
  %
  Also let $\fE(s)$ and $\pE(s)$ denote the sets of \glspl{edge} in $\fG$ and
  $\pG$, respectively, for all pairs of \glspl{node} appearing in $s$.
  %
  Then
  %
  \begin{displaymath}
    \begin{array}{r@{}l}
        \mSemNodeRule(s, n, m) \equiv \mbox{}
      & n \mVFTwoAttrCmp m, \\[\abovedisplayskip]
        \mSemEdgeRule(s, n, m) \equiv \mbox{}
      & \big(
        \forall \mPair{n'}{m'} \in \fE(s) :
        \mPair{n}{n'} \in \fE \mImp \mPair{n}{n'} \mVFTwoAttrCmp \mPair{m}{m'}
        \big) \mAnd \mbox{} \\
      & \big(
        \forall \mPair{n'}{m'} \in \fE(s) :
        \mPair{n'}{n} \in \fE \mImp \mPair{n'}{n} \mVFTwoAttrCmp \mPair{m'}{m}
        \big).
    \end{array}
  \end{displaymath}%
  \labelDefinition{pm-semantic-rules}%
\end{definition}


\section{Limitations of Existing Approaches}
\labelSection{ex-isel-rep-limitations-of-existing-approaches}

To solve the problems described in \refSection{intro-motivation}, none of the
approaches discussed in this chapter can be applied directly.
%
The greedy approaches are only concerned with \gls{pattern selection}, making it
unclear how to extend these to integrate other \gls{code generation} tasks.
%
The combinatorial approaches show more promise in that regards as they apply
generic solving techniques, but instead the models of these approaches are too
limited.

First, while many combinatorial approaches combine \gls{instruction
  selection} with \gls{instruction scheduling}, none combines \gls{instruction
  selection} with \gls{global code motion}, and for most of these it is not
clear how to extend the model to integrate this task.

Second, all combinatorial approaches only handle \gls{tree}- and
\gls{DAG}-shaped \glspl{pattern}, which excludes support for
\gls{inter-block.ic} \glspl{instruction} as these extend over multiple
\glspl{block} and must therefore be modeled as generic \glspl{graph} (one such
example is given in \refChapter{introduction}
on \refPage{saturated-arithmetic}).

Third, with the exception of \textcite{TanakaEtAl:2003}, no combinatorial
approach takes the cost of \gls{data copying} into account.
%
Failing to consider this cost could lead to greedy use of \gls{SIMD.i}
\glspl{instruction}, which in turn degrades code quality.

Fourth and last, all combinatorial approaches only deal with data flow.
%
Problems concerning control flow, such as selection of branch
\glspl{instruction} and \gls{block ordering}, must be handled separately, which
could potentially result in suboptimal code.

To summarize, we necessitate a more general combinatorial model that integrates
the problems described in \refSection{intro-motivation} and, to that end, we
necessitate a more powerful means of representing \glspl{function} and
\glspl{instruction}.
