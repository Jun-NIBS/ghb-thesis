% Copyright (c) 2017, Gabriel Hjort Blindell <ghb@kth.se>
%
% This work is licensed under a Creative Commons 4.0 International License (see
% LICENSE file or visit <http://creativecommons.org/licenses/by/4.0/> for a copy
% of the license).

\chapter{Constraint Programming}
\labelChapter{constraint-programming}

\glsreset{IP}

This chapter describes \gls{CP}, which is a method for solving combinatorial
problems.
%
Like similar methods such as \gls{IP} and \gls{SAT}~\cite{BiereEtAl:2009}, in
\gls{CP} we first \emph{model} the problem and then we \emph{solve} the model.
%
The modeling and solving aspects are described in \refSectionList{cp-modeling,
  cp-solving}, respectively.
%
A comprehensive overview of \gls{CP} is given in~\cite{RossiEtAl:2006}.

In terms of modeling, \gls{CP} offers a higher level of abstraction compared to
other methods.
%
For example, \gls{CP} provides dedicated \glspl{constraint} for capturing many
recurring problem structures that must be decomposed and reformulated in
\gls{IP} or \gls{SAT}.
%
This makes \gls{CP} particularly suited for modeling the problems introduced in
\refChapter{introduction} and is therefore the method on which we base our
approach.


\section{Modeling}
\labelSection{cp-modeling}

To solve a problem using \gls{CP}, it must first be formulated as a
\gls{constraint model}.
%
Modeling strategies are discussed in detail by \textcite{Smith:2006}.

A \gls!{constraint model} (or just \glsshort!{constraint model}) consists of two
components:%
%
\begin{inlinelist}[itemjoin={, }, itemjoin*={, and}]
  \item a set of \glspl{variable}
  \item a set of \glspl{constraint}
\end{inlinelist}.
%
\Glspl!{variable} represent problem decisions and take their values from a
finite \gls{domain}.
%
The \gls!{domain} of a variable~$\mVar{x}$, denoted $\mDomain(\mVar{x})$, is
typically is a set of integers, but it can also consist of real numbers and
complex structures such as string, sets, and \glspl{graph}~\cite{Gervet:2006}.
%
A \gls{variable}~$\mVar{x}$ is \gls!{assigned.v} if
\mbox{$|\!\mDomain(\mVar{x})| = 1$}.
%
\Glspl!{constraint} express relations between \glspl{variable} and forbid
assignments that are illegal in the problem.
%
Formally, we say that a \gls{constraint}~$C$ applied on a set of
\glspl{variable} returns a subset of the Cartesian product of the
\glspl{variable}' \glspl{domain}, i.e.\ \mbox{$C(\mVar{x}_1, \ldots, \mVar{x}_k)
  \subseteq \mDomain(\mVar{x}_1) \times \ldots \times \mDomain(\mVar{x}_k)$}.
%
A tuple $\mTuple{d_1, \ldots, d_k} \in C(\mVar{x}_1, \ldots, \mVar{x}_k)$ is
called a \gls!{solution}[ to $C$], and an assignment to all \glspl{variable}
that fulfills all \glspl{constraint} in a \gls{constraint model}~$M$ is called a
\gls!{solution}[ to $M$].
%
An example is shown in \refTable{cp-model-example}.

\begin{table}
  \centering%
  \figureFont\figureFontSize%
  \mbox{}%
  \hfill\hfill%
  \subcaptionbox{Constraint model\labelTable{cp-model-example-instance}}%
                {%
                  \begin{tabular}{lc}
                    \toprule
                      \multicolumn{1}{c}{\tabhead variables}
                        & \tabhead constraints\\
                    \midrule
                      $\mVar{x} \in \mSet{1, 2}$ & $\mVar{x} \neq \mVar{y}$\\
                      $\mVar{y} \in \mSet{1, 2}$ & $\mVar{x} \neq \mVar{z}$\\
                      $\mVar{z} \in \mSet{1, 2, 3, 4}$
                        & $\mVar{y} \neq \mVar{z}$\\
                    \bottomrule
                  \end{tabular}%
                }%
  \hfill%
  \subcaptionbox{Solutions\labelTable{cp-model-example-solutions}}%
                [21mm]%
                {%
                  \newcolumntype{C}{>{$}c<{$}}%
                  $\begin{tabular}{C@{\:}C@{\:}C@{\:}C@{\:}C}
                     \toprule
                       & \mVar{x}\phantom{,}
                       & \mVar{y}\phantom{,}
                       & \mVar{z}
                       & \\
                     \midrule
                       \langle & 1, & 2, & 3 & \rangle\\
                       \langle & 2, & 1, & 3 & \rangle\\
                       \langle & 1, & 2, & 4 & \rangle\\
                       \langle & 2, & 1, & 4 & \rangle\\
                     \bottomrule
                   \end{tabular}$%
                }%
  \hfill\hfill%
  \mbox{}

  \caption[Example of a constraint model]%
          {%
            Example of a constraint model, corresponding to a problem where
            three variables must be assigned values which are different from one
            another%
          }
  \labelTable{cp-model-example}
\end{table}

The use of \glspl{variable} and \glspl{constraint} results in \glspl{constraint
  model} that are \gls!{compositional.cm}, meaning they can be easily be
extended to capture additional problems to be solved in unison.


\subsubsection{Global Constraints}

If a \gls!{binary.c}[ \gls{constraint}] is a \gls{constraint} involving two
\glspl{variable}, then a \gls!{global.c}[ \gls{constraint}] is a
\gls{constraint} involving three or more
\glspl{variable}~\cite{VanHoeveKatriel:2006}.
%
\Gls{global.c} \glspl{constraint} capture recurring problem structures and
improve solving compared to relations modeled using multiple \gls{binary.c}
\glspl{constraint}.

Arguably, the most well-known \gls{global.c} \gls{constraint} is the
\gls!{all-different constraint}.
%
Typically referred to as $\mAllDifferent$ or $\mDistinct$, the \gls{constraint}
enforces all \glspl{variable} in a given set to take distinct values.
%
Formally, the \gls{constraint} is defined as follows.
%
\begin{definition}[All-different Constraint]%
  Let \mbox{$\mVar{x}_1, \ldots, \mVar{x}_k$} be a set of \glspl{variable}.
  %
  Then
  %
  \begin{displaymath}
    \mAllDifferent(\mVar{x}_1, \ldots, \mVar{x}_k)
    \equiv
    \mSetBuilder{\langle d_1, \ldots, d_k \rangle}%
                {\forall_{\! i} \: d_i \in \mDomain(\mVar{x}_i),
                 \forall_{\! i \neq j} \: d_i \neq d_j}.
  \end{displaymath}%
\end{definition}

Hence the \glspl{constraint} in \refTable{cp-model-example} can be replaced by
$\mAllDifferent(\mVar{x}, \mVar{y}, \mVar{z})$.

In \refChapter{existing-isel-techniques-and-reps} we saw another \gls{global.c}
\gls{constraint} -- the \gls{global cardinality constraint}, which is a
generalization of the \gls{all-different constraint} -- and how it can be used
to model the \gls{pattern selection} problem (see
\refEquation{pattern-selection-using-gcc} on
\refPageOfEquation{pattern-selection-using-gcc}).
%
For completeness, we provide the formal definition of the \gls{constraint}.
%
\begin{definition}[Global Cardinality Constraint]%
  Let $v$ be a value, and let \mbox{$\mVar{x}_1, \ldots, \mVar{x}_k, \mVar{y}$}
  be a set of \glspl{variable}.
  %
  Then
  %
  \begin{displaymath}
    \mGCC(v, \mVar{x}_1, \ldots, \mVar{x}_k, \mVar{y})
    \equiv
    \mSetBuilder*{\mTuple{d_1, \ldots, d_k, e}}%
                 {
                   \begin{array}{@{}l@{\,}}
                     \forall_i \: d_i \in \mDomain(\mVar{x}_i),
                     e \in \mDomain(\mVar{y}), \\
                     e = |\mSetBuilder{d_i}{\forall_i \: d_i = v}|
                   \end{array}
                 }.
  \end{displaymath}%
  \labelDefinition{gcc}%
\end{definition}

Another relevant example is the \gls!{circuit constraint}, typically referred to
as $\mCircuit$ or $\mCycle$, which enforces that the \glspl{variable}
representing adjacency forms a cycle.
%
Formally, the \gls{constraint} is defined as follows.
%
\begin{definition}[Circuit Constraint]
  Let \mbox{$\mVar{x}_1, \ldots, \mVar{x}_k$} be a set of \glspl{variable} with
  \glspl{domain} \mbox{$\mDomain(\mVar{x}_i) \subseteq \mSet{1, \ldots, k}$} for
  \mbox{$i = 1, \ldots, k$}.
  %
  Let also a permutation~\mbox{$P = d_1, \ldots, d_k$} of \gls{domain} values be
  considered \emph{cyclic} if the set $S_P$, defined as
  %
  \begin{displaymath}
    \begin{array}{c}
      1 \in S_P,
      i \in S_P \mImp d_i \in S_P,
    \end{array}
  \end{displaymath}
  %
  has $k$ elements.
  %
  Then
  %
  \begin{displaymath}
    \mCircuit(\mVar{x}_1, \ldots, \mVar{x}_k)
    \equiv
    \mSetBuilder{\langle d_1, \ldots, d_k \rangle}%
                {
                  \forall_{\! i} \: d_i \in \mDomain(\mVar{x}_i),
                  \text{$d_1, \ldots, d_k$ is cyclic}
                }.
  \end{displaymath}
  \labelDefinition{circuit}
\end{definition}

As will be seen in \refChapter{constraint-model}, $\mCircuit$ can be used
to model \gls{block ordering}.

The last \gls{global.c} \gls{constraint} relevant for this dissertation is the
\gls{table constraint}, which constrains a vector of \glspl{variable} such that
the values appear as a row in a given matrix.
%
By encoding legal \gls{variable} assignments into the matrix, any relation can
be expressed using a \gls{table constraint}.
%
Formally, the \gls{constraint} is defined as follows.
%
\begin{definition}[Table Constraint]
  Let \mbox{$\mVar{x}_1, \ldots, \mVar{x}_k$} be a set of \glspl{variable}, and
  let $T$ be an \mbox{$m \times k$} matrix, where \mbox{$m \in \mathbb{N}$}.
  %
  Then
  %
  \begin{displaymath}
    \mTable(\mVar{x}_1, \ldots, \mVar{x}_k, T)
    \equiv
    \mSetBuilder{\langle d_1, \ldots, d_k \rangle}%
                {
                  \forall_{\!i } \: d_i \in \mDomain(\mVar{x}_i),
                  d_1, \ldots, d_k \in T
                }.
  \end{displaymath}
\end{definition}

As will be seen in \refChapter{solving-techniques}, $\mTable$ can be used to
refining the modeling of cost.


\subsubsection{Optimization}

In \gls{CP}, an optimization problem is modeled by maximizing or minimizing a
\gls{variable}~$\mVar{c}$ whose value is constrained according to the
\gls{objective function}.
%
For example, if \mbox{$\mVar{x}_m \in \mSet{0, 1}$} is \gls{variable}
representing whether match~$m$ is selected and $c_m$ denotes the cost of
selecting $m$, then a \gls{CP} idiom for modeling \gls{optimal.ps} \gls{pattern
  selection} is
%
\begin{equation}
  \begin{array}{rl}
      \text{minimize} & \mVar{c} \\
    \text{subject to} & \mVar{c} = \displaystyle\sum_m c_m \mVar{x}_m
  \end{array}
  \labelEquation{pattern-selection-in-cp}
\end{equation}
%
In this context, $\mVar{c}$ is called a \gls!{cost variable}.
%
Note that the \gls{objective function} is orthogonal to the rest of the
\glsshort{constraint model}, thus allowing it to be easily customized to fit the
desired optimization criterion.


\section{Solving}
\labelSection{cp-solving}

A \gls!{constraint solver} (or just \gls!{solver}) finds \glspl{solution} to a
\gls{constraint model} by interleaving \gls{propagation} and \gls{search}.
%
\Gls!{propagation} removes \gls{domain} values that are known not to appear in
any \gls{solution}, and \gls!{search} attempts several alternatives when
\gls{propagation} is not enough to find a \gls{solution}.

In practice, however, this alone is often not enough for many problem instances
because the \gls{search space} is simply too large.
%
In such cases, the \gls{search space} can be further reduced by extending the
\gls{constraint model} with additional \glspl{constraint} to strengthen
propagation and remove uninteresting \glspl{solution} and by performing
\gls{presolving}.


\subsubsection{Propagation}

Performing \gls{propagation} requires an array of \gls{domain}-pruning
algorithms and a system that allows these algorithms to interact.
%
\Gls{propagation} theory is discussed in detail by \textcite{Bessiere:2006}, and
\glsdesc{CP} systems are throroughly discussed by
\textcite{SchulteCarlsson:2006}.

\Gls{constraint solver} typically keep track of \glspl{variable} and their
\glspl{domain} using \glspl{constraint store}.
%
A \gls!{constraint store} (or just \gls!{store}) is a data structure that maps a
set of \glspl{variable} to sets of \glspl{domain}.
%
A \gls{store}~$S_1$ is \gls!{stronger.cs} than another \gls{store}~$S_2$,
denoted \mbox{$S_1 \mStronger S_2$}, if \mbox{$\mDomain_1(\mVar{x}) \subseteq
  \mDomain_2(\mVar{x})$} for all \glspl{variable}~$\mVar{x}$, where
$\mDomain_i(\mVar{x})$ denotes the domain of \gls{variable}~$\mVar{x}$ in
store~$S_i$.

A function that takes a \gls{constraint store} as input and produces another
\gls{store} is called a \gls!{propagator} (or \gls!{filtering algorithm}).
%
A \gls{propagator} implements a \gls{constraint} if it does not remove any
\glspl{solution} to the \gls{constraint} and only keeps \gls{variable}
assignments that are part of a \gls{solution}.
%
For solving to be well-behaved, \glspl{propagator} are also expected to be
\gls!{decreasing.p} -- it does not add any values, hence \mbox{$p(S) \mStronger
  S$} -- and \gls!{monotonic.p} -- if \mbox{$S_1 \mStronger S_2$}, then
\mbox{$p(S_1) \mStronger p(S_2)$}.
%
A \gls{propagator} for which \mbox{$p(S) = S$} holds is said to be at
\gls!{fixpoint}, and a \gls{store} is at \gls{fixpoint} if all
\glspl{propagator} are at \gls{fixpoint} for that \gls{store}.
%
A \gls{propagator} that returns a \gls{store} with at least one empty
\gls{domain} has \glsshort!{failure}, meaning there are no \glspl{solution} in
this part of the \gls{search space}.

\Glspl{propagator} implementing the same \gls{constraint} can differ in the
amount of propagation they perform.
%
A \gls{propagator} is \glshyphened!{value consistency} if it only
\glsshort{propagation}[es] when one of its \glspl{variable} becomes
\gls{assigned.v}, \glshyphened!{bounds consistency} if it only reduces the
bounds of a \gls{domain}, and \glshyphened!{domain consistency} if it removes
all values that do not appear in any \gls{solution} to the \gls{constraint}.
%
\begin{table}
  \centering%
  \figureFont\figureFontSize%
  \newcolumntype{C}{>{$}c<{$}}%
  \subcaptionbox{%
                  Solving with value-consistent inequality constraints%
                  \labelTable{cp-prop-strengths-example-inequality-cons}%
                }{%
                  \begin{tabular}{%
                                   p{4.5cm}%
                                   C@{}C@{ }C@{}C%
                                   C@{}C@{ }C@{}C%
                                   C@{}C@{ }C@{ }C@{ }C@{}C%
                                 }
                    \toprule
                      \multicolumn{1}{c}{\tabhead event}
                        & \multicolumn{14}{c}{\tabhead store}\\
                      \cmidrule(lr){2-15}%
                        & \multicolumn{4}{c}{$\mVar{x}$}
                        & \multicolumn{4}{c}{$\mVar{y}$}
                        & \multicolumn{6}{c}{$\mVar{z}$}\\
                    \midrule
                      Initial store
                        & \{ & 1, & 2  & \}
                        & \{ & 1, & 2  & \}
                        & \{ & 1, & 2, & 3, & 4 & \}\\
                      Propagate until fixpoint
                        & \{ & 1, & 2  & \}
                        & \{ & 1, & 2  & \}
                        & \{ & 1, & 2, & 3, & 4 & \}\\
                      Search by attempting $\mVar{z} = 1$
                        & \{ & 1, & 2  & \}
                        & \{ & 1, & 2  & \}
                        & \{ & 1\phantom{,}
                                  &    &    &   & \}\\
                      Propagate $\mVar{y} \neq \mVar{z}$
                        & \{ & 1, & 2  & \}
                        & \{ &    & 2  & \}
                        & \{ & 1\phantom{,}
                                  &    &    &   & \}\\
                      Propagate $\mVar{x} \neq \mVar{z}$
                        & \{ &    & 2  & \}
                        & \{ &    & 2  & \}
                        & \{ & 1\phantom{,}
                                  &    &    &   & \}\\
                      Propagate $\mVar{x} \neq \mVar{y}$
                        & \{ &    &    & \}
                        & \{ &    & 2  & \}
                        & \{ & 1\phantom{,}
                                  &    &    &   & \}\\
                      Failure reached; backtrack
                        & \{ & 1, & 2  & \}
                        & \{ & 1, & 2  & \}
                        & \{ &    & 2, & 3, & 4 & \}\\
                      \qquad\raisebox{0pt}[10pt]{$\vdots$}
                        & & & & & & & & & & & & & & \\[-2pt]
                    \bottomrule
                  \end{tabular}%
                }

  \vspace{\betweensubfigures}

  \subcaptionbox{%
                  Solving with domain-consistent all-different constraint%
                  \labelTable{cp-prop-strengths-example-alldiff}%
                }{%
                  \begin{tabular}{%
                                   p{4.5cm}%
                                   C@{}C@{ }C@{}C%
                                   C@{}C@{ }C@{}C%
                                   C@{}C@{ }C@{ }C@{ }C@{}C%
                                 }
                    \toprule
                      \multicolumn{1}{c}{\tabhead event}
                        & \multicolumn{14}{c}{\tabhead store}\\
                      \cmidrule(lr){2-15}%
                        & \multicolumn{4}{c}{$\mVar{x}$}
                        & \multicolumn{4}{c}{$\mVar{y}$}
                        & \multicolumn{6}{c}{$\mVar{z}$}\\
                    \midrule
                      Initial store
                        & \{ & 1, & 2  & \}
                        & \{ & 1, & 2  & \}
                        & \{ & 1, & 2, & 3, & 4 & \}\\
                      Propagate $\mAllDifferent(\mVar{x}, \mVar{y}, \mVar{z})$
                        & \{ & 1, & 2  & \}
                        & \{ & 1, & 2  & \}
                        & \{ &    &    & 3, & 4 & \}\\
                      \qquad\raisebox{0pt}[10pt]{$\vdots$}
                        & & & & & & & & & & & & & & \\[-2pt]
                    \bottomrule
                  \end{tabular}%
                }

  \caption[Example illustrating propagation]%
          {%
            Example illustrating propagation for two versions of the model given
            in \refTable{cp-model-example}, one using the all-different
            constraint and the other using a binary decomposition%
          }
  \labelTable{cp-prop-strengths-example}
\end{table}
%
For example, \refTable{cp-prop-strengths-example} shows solving of two versions
of the \glsshort{constraint model} given in \refTable{cp-model-example}, one
using $\mAllDifferent$ and another using inequality \glspl{constraint}.
%
Because only \gls{value consistency} can be achieved for inequality
\glspl{constraint}, they cannot \glsshort{propagation}[e] anything until at
least one \gls{variable} becomes \gls{assigned.v}.
%
As all \glspl{propagator} are already at \gls{fixpoint}, the \gls{solver} must
resort to \gls{search}.
%
In this case, the \gls{solver} makes a wrong guess and is forced to backtrack.
%
In comparison, a \glshyphened{domain consistency} \gls{propagator} for the
\gls{all-different constraint} can remove values~1 and~2 from the \gls{domain}
of \gls{variable}~$\mVar{z}$ as these values do not appear in any
\glspl{solution} (see \refTable{cp-model-example-solutions}).
%
Maximizing \gls{propagation} is key in making solving tractable, as the
\gls{search space} grows exponentially with the number of \glspl{variable} and
size of the \glspl{domain}.

As to be expected, stronger \gls{propagation} comes at a price of greater
complexity.
%
For the \gls{all-different constraint}, there exist \glsshort{bounds
  consistency} and \glshyphened{domain consistency} \glspl{propagator} with
worst-case time complexities~$\mBigO(n \log n)$~\cite{Lopez-OrtizEtAl:2003}
and~$\mBigO(n^{2.5})$~\cite{Regin:1994}, respectively, where $n$ denotes the
number of \glspl{variable}.
%
The same can be achieved for the \gls{global cardinality constraint} at similar
cost~\cite{QuimperEtAl:2005, Regin:1996}.

Many \glshyphened{domain consistency} \gls{propagator} exist for the \gls{table
  constraint} \cite{LecoutreSzymanek:2006, Lecoutre:2011, MairyEtAl:2014,
  PerezRegin:2014, LecoutreEtAl:2015, DemeulenaereEtAl:2016}, although these
exhibit worst-case time complexity.
%
\Gls{domain consistency} for \gls{circuit constraint} cannot be achieved in
polynomial time as it involves finding Hamiltonian cycles, which is an
NP-complete problem~\cite{GareyJohnson:1979}, although an incomplete
polynomial-time \gls{filtering algorithm} is given in~\cite{KayaHooker:2006}.


\subsubsection{Search}

When no more \gls{propagation} can be performed -- that is, when all
\glspl{propagator} are at \gls{fixpoint} -- the \gls{solver} resorts to
\gls{search}, which is discussed in detail by \textcite{VanBeek:2006}.

In exploring the \gls{search space}, two decisions need to be made repeatedly:
%
\begin{enumerate*}[label=(\arabic*), itemjoin*={, and\ }]
  \item select a \gls{variable} on which to branch
  \item select a value from its \gls{domain}
\end{enumerate*}.
%
These decisions constitute a \gls!{branching strategy} which arranges the
\gls{search space} into a \gls!{search tree}, where each \gls{node} represents a
\gls{store} at \gls{fixpoint}.
%
An example is given in \refFigure{cp-search-tree-sat-example}.
%
Since the \glspl{solution} (and \glspl{failure}) appear at the leaf
\glspl{node}, the \gls{search tree} is typically explored depth-first.

\begin{figure}
  \centering%
  \input{figures/constraint-programming/search-tree-sat-example}

  \caption[Example of a search tree]%
          {%
            Example of a search tree for the model given in
            \refTable{cp-model-example}.
            %
            Diamond-shaped nodes represent solutions%
          }
  \labelFigure{cp-search-tree-sat-example}
\end{figure}

For \gls{search} to be well-behaved, a \gls{branching strategy} must preserve
all \glspl{solution} in the \gls{search space} and must not duplicate any
\gls{solution}.
%
A common strategy in choosing a \gls{variable}, called the \gls!{first-fail
  principle}, is to select the \gls{variable} most likely to cause a
\gls{failure}~\cite{HaralickElliott:1980}.
%
Other strategies involve selecting the \gls{variable} with the smallest or
largest value in its \gls{domain}, or selecting a random \gls{variable}.
%
Similar strategies are applied in value selection, which is typically done by
posting \gls{constraint} when branching.
%
Most common is to post unary \gls{constraint} that divides a \gls{domain} into
two.
%
For example, at the root \gls{node} in \refFigure{cp-search-tree-sat-example},
\gls{search} branches on \gls{variable}~$\mVar{x}$ by posting \mbox{$\mVar{x} =
  1$} in one branch and \mbox{$\mVar{x} \neq 1$} in the other.
%
Another strategy is to split the domain by posting inequality \glspl{constraint}
(for example, \mbox{$\mVar{x} \leq 3$} in one branch and \mbox{$\mVar{x} > 3$}
in the other), which is useful for solving \glsplshort{constraint model} with
arithmetic \glspl{constraint}.
%
More than one \gls{branching strategy} can be used for the same
\glsshort{constraint model} and, if needed, they can be customized by the user,
making it a key strength of \gls{CP}.

\Glspl{solution} to optimization problems are found using a method called
\gls!{branch and bound}.
%
During \gls{search}, the best \gls{solution} found so far is kept and a
\gls{constraint} is added to enforce all subsequent \glspl{solution} to have
strictly less (or greater) cost, allowing time can be traded for quality on a
continuous time scale.
%
When the entire \gls{search space} has been explored, the last found
\gls{solution} is guaranteed to be optimal.
%
\begin{figure}
  \centering%
  \input{figures/constraint-programming/search-tree-opt-example}

  \caption[Example of a search tree for an optimization problem]%
          {%
            Example of a search tree for the model given in
            \refTable{cp-model-example} with the additional requirement that
            the value of $\mVar{z}$ should be maximized.
            %
            The search tree is explored depth first, left to right.
            %
            Diamond-shaped nodes represent solutions and square-shaped nodes
            represent failures%
          }
  \labelFigure{cp-search-tree-opt-example}
\end{figure}
%
An example of shown in \refFigure{cp-search-tree-opt-example}.
%
Assuming the \gls{search tree} is explored depth first, left to right, the first
solution to be found is \mbox{$S_1 = \mTuple{\mVar{x} = \mSet{1}, \mVar{y} =
    \mSet{2}, \mVar{z} = \mSet{3}}$}.
%
Since the value of $\mVar{z}$ is to be maximized, this causes the
\gls{constraint} \mbox{$\mVar{z} > 3$} to be posted.
%
The next solution to be found is \mbox{$S_2 = \mTuple{\mVar{x} = \mSet{1},
    \mVar{y} = \mSet{2}, \mVar{z} = \mSet{4}}$}, which is clearly better than
$S_1$, causing the \gls{constraint} \mbox{$\mVar{z} > 4$} to be posted.
%
Since the \gls{domain} of $\mVar{z}$ has no value greater than 4, the
\gls{constraint} \mbox{$\mVar{z} > 4$} causes a \gls{failure} when the exploring
the other branch at the root.
%
At this point the entire \gls{search space} has been explored, making $S_2$ the
optimal \gls{solution}.


\subsubsection{Implied, Symmetry Breaking, and Dominance Breaking Constraints}

Solving can be improved by extending the \gls{constraint model} with additional
\glspl{constraint}.
%
These \glspl{constraint} can be divided into three categories --
\gls{implied.c}, \gls{symmetry breaking.c}, and \gls{dominance breaking.c} --
which are discussed in detail by \mbox{\textcite{Smith:2006}},
\textcite{GentEtAl:2006}, and \textcite{ChuStuckey:2015}, respectively.

An \gls!{implied.c}[ \gls{constraint}] is a \gls{constraint} that strengthens
\gls{propagation} without removing any \glspl{solution}.
%
For example, assume a naive \glsshort{constraint model} for solving the
\gls!{magic sequence problem}, which is defined as finding a sequence
\mbox{$x_0, \ldots, x_{n-1}$} of integers such that for all \mbox{$0 \leq i <
  n$}, the number~$i$ appears exactly $x_i$ times in the sequence.
%
Using the \gls{global cardinality constraint} and $n$~\glspl{variable}, this can
be modeled as
%
\begin{equation}
  \mGCC(i, \mVar{x}_0, \ldots, \mVar{x}_{n-1}, \mVar{x}_i)
  \mQuantSep
  \forall 0 \leq i < n.
\end{equation}
%
While this \gls{constraint} is sufficient in capturing the problem,
\gls{propagation} can be increased by adding the following \gls{constraint}:
%
\begin{equation}
  \sum_{i = 0}^{n - 1} \mVar{x}_i = n.
\end{equation}
%
This holds because the sum of all occurrences -- that is, the number of items in
the sequence -- must be equal to the length of the sequence.

A \gls!{symmetry breaking.c}[ \gls{constraint}] is a \gls{constraint} that
removes \glspl{solution} considered to be symmetric to one another.
%
For example, assume a \glsshort{constraint model} for solving a problem of
packing $n$ squares of sizes \mbox{$1, \ldots, n$} inside another, larger
square.
%
Given a \gls{solution} to this problem, more \glspl{solution} can found by
rotating, flipping, and mirroring the initial \gls{solution}.
%
As these \glspl{solution} are essentially the same, only one of them should be
kept in the \gls{search space}.
%
A simple method of removing most (but not all) symmetric \glspl{solution} is to
force one of the squares to be packed into one of the quadrants of the enclosing
square.

A \gls!{dominance breaking.c}[ \gls{constraint}] is a \gls{constraint} that
removes \glspl{solution} known to be dominated by another \gls{solution}.
%
\Gls{dominance breaking.c} is therefore a generalization of \gls{symmetry
  breaking.c}.
%
\begin{figure}
  \centering%
  \mbox{}%
  \hfill%
  \hfill%
  \input{figures/constraint-programming/square-packing-partial-sol1}%
  \hfill%
  \input{figures/constraint-programming/square-packing-partial-sol2}%
  \hfill%
  \hfill%
  \mbox{}

  \caption[Example of dominating solutions]%
          {%
            Example of two partial solutions to the square packing problem,
            where the left-most solution is dominated by the right-most
            solution%
          }
  \labelFigure{square-packing-partial-sol}
\end{figure}
%
For an example, let us revisit the \glsshort{constraint model} capturing the
square packing problem and assume the two partial \glspl{solution} shown in
\refFigure{square-packing-partial-sol}~\cite{Korf:2004}.
%
In the left-most \gls{solution}, the positioning of the two squares form an
empty \mbox{$2 \times 3$} rectangle in the corner.
%
If this is extended to a complete \gls{solution}, another \gls{solution} can be
found by sliding the \mbox{$3 \times 3$} square all the way up and moving any
squares packed above into the space created below.
%
Hence the left-most \gls{solution} is dominated by the right-most
\gls{solution}, and such \glspl{solution} can be removed from the \gls{search
  space} by forbidding each \mbox{$k \times k$} square from being placed a
certain distance away from the edge of the enclosing square such that the
\mbox{$k \times k$} square and the edge forms a rectangle wherein all smaller
squares can be packed.

The benefit of a given \gls{implied.c}, \gls{symmetry breaking.c}, or
\gls{dominance breaking.c} \gls{constraint} depends on the amount of \gls{search
  space} it prunes and the cost of \glsshort{propagation}[ing] the
\gls{constraint}.
%
For example, if a \gls{constraint} is expensive to run and only has marginal
effect on the \gls{variable} \gls{domain}, then adding it to a
\glsshort{constraint model} will \emph{increase} solving time instead of
decreasing it.
%
In addition, it is well known that such \glspl{constraint} often have synergy
effects among each other, meaning a \gls{constraint} may not be useful on its
own but may have a positive effect when combined with another \gls{constraint}.
%
Consequently, the decision of whether to add a \gls{implied.c}, \gls{symmetry
  breaking.c}, or \gls{dominance breaking.c} \gls{constraint} to a
\glsshort{constraint model} must be based on careful and thorough experimental
evaluation.


\subsubsection{Presolving}

\Gls!{presolving} is the process of applying problem-specific algorithms to
reduce the number of \glspl{variable} or to shrink the \gls{variable}
\glspl{domain} before solving.
%
Fewer \gls{variable} and smaller \glspl{domain} means smaller \glspl{constraint
  model}, which means shorter solving times.

\todo{give better example than bound tightening}


\subsubsection{Lazy Clause Learning}

\todo{write}
