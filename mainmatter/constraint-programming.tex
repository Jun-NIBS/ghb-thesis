% Copyright (c) 2017, Gabriel Hjort Blindell <ghb@kth.se>
%
% This work is licensed under a Creative Commons 4.0 International License (see
% LICENSE file or visit <http://creativecommons.org/licenses/by/4.0/> for a copy
% of the license).

\chapter{Constraint Programming}
\labelChapter{constraint-programming}

\glsreset{IP}

This chapter describes \gls{CP}, which is a method for solving combinatorial
problems.
%
Like similar methods such as \gls{IP} and \gls{SAT}~\cite{BiereEtAl:2009}, in
\gls{CP} we first \emph{model} the problem and then we \emph{solve} the model.
%
The modeling and solving aspects are described in \refSectionList{cp-modeling,
  cp-solving}, respectively.
%
A comprehensive overview of \gls{CP} is given in~\cite{RossiEtAl:2006}.

In terms of modeling, \gls{CP} offers a higher level of abstraction compared to
other methods.
%
For example, \gls{CP} provides dedicated \glspl{constraint} for capturing many
recurring problem structures that must be decomposed and reformulated in
\gls{IP} or \gls{SAT}.
%
This makes \gls{CP} particularly suited for modeling the problems introduced in
\refChapter{introduction} and is therefore the method on which we base our
approach.


\section{Modeling}
\labelSection{cp-modeling}

To solve a problem using \gls{CP}, it must first be formulated as a
\gls{constraint model}.
%
Modeling strategies are discussed in detail by \textcite{Smith:2006}.

A \gls!{constraint model} (or just \glsshort!{constraint model}) consists of two
components:%
%
\begin{inlinelist}[itemjoin={, }, itemjoin*={, and}]
  \item a set of \glspl{variable}
  \item a set of \glspl{constraint}
\end{inlinelist}.
%
\Glspl!{variable} represent problem decisions and take their values from a
finite \gls{domain}.
%
The \gls!{domain} of a variable~$\mVar{x}$, denoted $\mDomain(\mVar{x})$, is
typically is a set of integers, but it can also consist of real numbers and
complex structures such as string, sets, and \glspl{graph}~\cite{Gervet:2006}.
%
A \gls{variable}~$\mVar{x}$ is \gls!{assigned.v} if
\mbox{$|\!\mDomain(\mVar{x})| = 1$}.
%
\Glspl!{constraint} express relations between \glspl{variable} and forbid
assignments that are illegal in the problem.
%
Formally, we say that a \gls{constraint}~$C$ applied on a set of
\glspl{variable} returns a subset of the Cartesian product of the
\glspl{variable}' \glspl{domain}, i.e.\ \mbox{$C(\mVar{x}_1, \ldots, \mVar{x}_k)
  \subseteq \mDomain(\mVar{x}_1) \times \ldots \times \mDomain(\mVar{x}_k)$}.
%
A tuple $\mTuple{d_1, \ldots, d_k} \in C(\mVar{x}_1, \ldots, \mVar{x}_k)$ is
called a \gls!{solution}[ to $C$], and an assignment to all \glspl{variable}
that fulfills all \glspl{constraint} in a \gls{constraint model}~$M$ is called a
\gls!{solution}[ to $M$].
%
An example is shown in \refTable{cp-model-example}.

\begin{table}
  \centering%
  \figureFont\figureFontSize%
  \mbox{}%
  \hfill\hfill%
  \subcaptionbox{Constraint model\labelTable{cp-model-example-instance}}%
                {%
                  \begin{tabular}{lc}
                    \toprule
                      \multicolumn{1}{c}{\tabhead variables}
                        & \tabhead constraints\\
                    \midrule
                      $\mVar{x} \in \mSet{1, 2}$ & $\mVar{x} \neq \mVar{y}$\\
                      $\mVar{y} \in \mSet{1, 2}$ & $\mVar{x} \neq \mVar{z}$\\
                      $\mVar{z} \in \mSet{1, 2, 3, 4}$
                        & $\mVar{y} \neq \mVar{z}$\\
                    \bottomrule
                  \end{tabular}%
                }%
  \hfill%
  \subcaptionbox{Solutions\labelTable{cp-model-example-solutions}}%
                [21mm]%
                {%
                  \newcolumntype{C}{>{$}c<{$}}%
                  $\begin{tabular}{C@{\:}C@{\:}C@{\:}C@{\:}C}
                     \toprule
                       & \mVar{x}\phantom{,}
                       & \mVar{y}\phantom{,}
                       & \mVar{z}
                       & \\
                     \midrule
                       \langle & 1, & 2, & 3 & \rangle\\
                       \langle & 2, & 1, & 3 & \rangle\\
                       \langle & 1, & 2, & 4 & \rangle\\
                       \langle & 2, & 1, & 4 & \rangle\\
                     \bottomrule
                   \end{tabular}$%
                }%
  \hfill\hfill%
  \mbox{}

  \caption[Example of a constraint model]%
          {%
            Example of a constraint model, corresponding to a problem where
            three variables must be assigned values which are different from one
            another%
          }
  \labelTable{cp-model-example}
\end{table}

The use of \glspl{variable} and \glspl{constraint} results in \glspl{constraint
  model} that are \gls!{compositional.cm}, meaning they can be easily be
extended to capture additional problems to be solved in unison.


\subsection{Global Constraints}

If a \gls!{binary.c}[ \gls{constraint}] is a \gls{constraint} involving two
\glspl{variable}, then a \gls!{global.c}[ \gls{constraint}] is a
\gls{constraint} involving three or more
\glspl{variable}~\cite{VanHoeveKatriel:2006}.
%
\Gls{global.c} \glspl{constraint} capture recurring problem structures and
improve solving compared to relations modeled using multiple \gls{binary.c}
\glspl{constraint}.


\paragraph{The All-Different Constraint}

Arguably, the most well-known \gls{global.c} \gls{constraint} is the
\gls!{all-different constraint}~\cite{Lauriere:1978} (for a survey,
see~\cite{VanHoeve:2001}), which enforces all \glspl{variable} in a given set to
take distinct values.
%
We refer to this \gls{constraint} as $\mAllDifferent$, which is defined as
follows.

\begin{definition}[$\mAllDifferent$]
  Let \mbox{$\mVar{x}_1, \ldots, \mVar{x}_k$} be a set of \glspl{variable}.
  %
  Then
  %
  \begin{displaymath}
    \mAllDifferent(\mVar{x}_1, \ldots, \mVar{x}_k)
    \equiv
    \mSetBuilder{\langle d_1, \ldots, d_k \rangle}%
                {\forall_{\! i} \: d_i \in \mDomain(\mVar{x}_i),
                 \forall_{\! i \neq j} \: d_i \neq d_j}.
  \end{displaymath}
  \labelDefinition{all-different}
\end{definition}

Hence the \glspl{constraint} in \refTable{cp-model-example} can be replaced by
$\mAllDifferent(\mVar{x}, \mVar{y}, \mVar{z})$.


\paragraph{The Global Cardinality Constraint}

In \refChapter{existing-isel-techniques-and-reps} we saw another \gls{global.c}
\gls{constraint} -- the \gls{global cardinality
  constraint}~\cite{OplobeduEtAl:1989}, which is a generalization of the
\gls{all-different constraint} -- and how it can be used to model the
\gls{pattern selection} problem (see \refEquation{pattern-selection-using-gcc}
on \refPageOfEquation{pattern-selection-using-gcc}).
%
For completeness, we give here the formal definition of the \gls{constraint}.

\begin{definition}[$\mGCC$]
  Let $v$ be a value, and let \mbox{$\mVar{x}_1, \ldots, \mVar{x}_k, \mVar{y}$}
  be a set of \glspl{variable}.
  %
  Then
  %
  \begin{displaymath}
    \mGCC(v, \mVar{x}_1, \ldots, \mVar{x}_k, \mVar{y})
    \equiv
    \mSetBuilder*{\mTuple{d_1, \ldots, d_k, e}}%
                 {
                   \begin{array}{@{}l@{\,}}
                     \forall_i \: d_i \in \mDomain(\mVar{x}_i),
                     e \in \mDomain(\mVar{y}), \\
                     e = |\mSetBuilder{d_i}{\forall_i \: d_i = v}|
                   \end{array}
                 }.
  \end{displaymath}
  \labelDefinition{gcc}
\end{definition}


\paragraph{The Circuit Constraint}

Another relevant example is the \gls!{circuit constraint}~\cite{Lauriere:1978},
which enforces that the \glspl{variable} representing adjacency forms a
\gls{cycle}.
%
We refer to this \gls{constraint} as $\mCircuit$, which is defined as follows.

\begin{definition}[$\mCircuit$]
  Let \mbox{$\mVar{x}_1, \ldots, \mVar{x}_k$} be a set of \glspl{variable} with
  \glspl{domain} \mbox{$\mDomain(\mVar{x}_i) \subseteq \mSet{1, \ldots, k}$} for
  \mbox{$i = 1, \ldots, k$}.
  %
  Let also a permutation~\mbox{$P = d_1, \ldots, d_k$} of \gls{domain} values be
  considered \emph{cyclic} if the set $S_P$, defined as
  %
  \begin{displaymath}
    \begin{array}{c}
      1 \in S_P,
      i \in S_P \mImp d_i \in S_P,
    \end{array}
  \end{displaymath}
  %
  has $k$ elements.
  %
  Then
  %
  \begin{displaymath}
    \mCircuit(\mVar{x}_1, \ldots, \mVar{x}_k)
    \equiv
    \mSetBuilder{\langle d_1, \ldots, d_k \rangle}%
                {
                  \forall_{\! i} \: d_i \in \mDomain(\mVar{x}_i),
                  \text{$d_1, \ldots, d_k$ is cyclic}
                }.
  \end{displaymath}
  \labelDefinition{circuit}
\end{definition}

For example, \mbox{$\mCircuit(\mVar{x}_1 \in \mSet{2}, \mVar{x}_2 \in \mSet{4},
  \mVar{x}_3 \in \mSet{1}, \mVar{x}_4 \in \mSet{3})$} holds because the
assignment forms the following \gls{cycle}:
%
\begin{center}
  \figureFont\figureFontSize%
  \begin{tikzpicture}[%
      every node/.style={
        node distance=4mm,
      }
    ]

    \node (x1) {$\mVar{x}_1$};
    \node [right=of x1] (x2) {$\mVar{x}_2$};
    \node [below=of x1] (x3) {$\mVar{x}_3$};
    \node [right=of x3] (x4) {$\mVar{x}_4$};

    \begin{scope}[->, line width=\normalLineWidth]
      \draw (x1) -- (x2);
      \draw (x2) -- (x4);
      \draw (x4) -- (x3);
      \draw (x3) -- (x1);
    \end{scope}
  \end{tikzpicture}%
\end{center}
%
However, \mbox{$\mCircuit(\mVar{x}_1 \in \mSet{2}, \mVar{x}_2 \in \mSet{1},
  \mVar{x}_3 \in \mSet{4}, \mVar{x}_4 \in \mSet{3})$} does not hold because the
assignment forms two \glspl{cycle} instead of one:
%
\begin{center}
  \figureFont\figureFontSize%
  \begin{tikzpicture}[%
      every node/.style={
        node distance=4mm,
      }
    ]

    \node (x1) {$\mVar{x}_1$};
    \node [right=of x1] (x2) {$\mVar{x}_2$};
    \node [below=of x1] (x3) {$\mVar{x}_3$};
    \node [right=of x3] (x4) {$\mVar{x}_4$};

    \begin{scope}[->, line width=\normalLineWidth, bend left]
      \draw (x1) to (x2);
      \draw (x2) to (x1);
      \draw (x3) to (x4);
      \draw (x4) to (x3);
    \end{scope}
  \end{tikzpicture}%
\end{center}

As will be seen in \refChapter{constraint-model}, $\mCircuit$ is used to model
\gls{block ordering}.


\paragraph{The Table Constraint}

Another \gls{global.c} \gls{constraint}, belonging to the group of so-called
\glspl!{extensional constraint}~\cite[Sect.\thinspace11.5.4]{Smith:2006}, is the
\gls!{table constraint}, which constrains a vector of \glspl{variable} such that
the values appear as a row in a given matrix.
%
By encoding legal \gls{variable} assignments into the matrix, any relation can
be expressed using a \gls{table constraint}.
%
We refer to this \gls{constraint} as $\mTable$, which is defined as follows.

\begin{definition}[$\mTable$]
  Let \mbox{$\mVar{x}_1, \ldots, \mVar{x}_k$} be a set of \glspl{variable}, and
  let $T$ be an \mbox{$m \times k$} matrix, where \mbox{$m \in \mathbb{N}$}.
  %
  Then
  %
  \begin{displaymath}
    \mTable(\mVar{x}_1, \ldots, \mVar{x}_k, T)
    \equiv
    \mSetBuilder{\langle d_1, \ldots, d_k \rangle}%
                {
                  \forall_{\!i } \: d_i \in \mDomain(\mVar{x}_i),
                  d_1, \ldots, d_k \in T
                }.
  \end{displaymath}
  \labelDefinition{table}
\end{definition}

For example, assume we are given a matrix
%
\begin{displaymath}
    A =
    \begin{bmatrix}
      1 & 1 \\
      2 & 4
    \end{bmatrix}\!.
\end{displaymath}
%
Then \mbox{$\mTable(\mVar{x}_1 \in \mSet{2}, \mVar{x}_2 \in \mSet{4}, A)$} holds
because the tuple \mbox{$\mTuple{2, 4}$} appears as a row in~$A$.
%
Similarly, \mbox{$\mTable(\mVar{x}_1 \in \mSet{3}, \mVar{x}_2 \in \mSet{3}, A)$}
does not hold because the tuple \mbox{$\mTuple{3, 3}$} appears as a row in~$A$.

As will be seen in \refChapter{solving-techniques}, $\mTable$ is used to refine
the \glsshort{constraint model} and to implement serveral of the \gls{implied.c}
\glspl{constraint}.


\paragraph{The Value-Precede-Chain Constraint}
\labelSection{cp-vpc}

The \gls!{value-precede-chain constraint}~\cite{LawLee:2004} requires a sequence
of \glspl{variable} to be sorted according to a given chain of values.
%
We refer to this \gls{constraint} as $\mValuePrecChain$, which is defined as
follows.

\begin{definition}[$\mValuePrecChain$]
  Let \mbox{$\mVar{x}_1, \ldots, \mVar{x}_k$} be a permutation of
  \glspl{variable}.
  %
  Let also $c$ be a list with $n$ elements that can be indexed starting from~1.
  %
  Then
  %
  \begin{displaymath}
    \mValuePrecChain(c, \mVar{x}_1, \ldots, \mVar{x}_k)
    \equiv
    \bigcap_{1 \leq i < n}
    \mValuePrec(c[i], c[i+1], \mVar{x}[1], \ldots \mVar{x}[k]),
  \end{displaymath}
  %
  where
  %
  \begin{displaymath}
    \mValuePrec(s, t, \mVar{x}_1, \ldots, \mVar{x}_k)
    \equiv
    \mSetBuilder{\langle d_1, \ldots, d_k \rangle}%
                {
                  \forall_{\! i} \: d_i \in \mDomain(\mVar{x}_i),
                  \forall_{\! i} \: d_i = t
                  \mImp \exists_{j} \: j < i \mAnd d_j = s
                }.
  \end{displaymath}
  \labelDefinition{vpc}
\end{definition}

For example, \mbox{$\mValuePrecChain(\langle 6, 5, 4 \rangle, \mVar{x}_1 \in
  \mSet{6}, \mVar{x}_2 \in \mSet{1}, \mVar{x}_3 \in \mSet{5}, \mVar{x}_4 \in
  \mSet{4})$} holds because the~4 is preceded by a~5, which in turn is preceded
by a~6, in the permutation of $\mVar{x}$~\glspl{variable}.
%
Likewise, \mbox{$\mValuePrecChain(\langle 5, 4 \rangle, \mVar{x}_1 \in \mSet{5},
  \mVar{x}_2 \in \mSet{1})$} also holds because 4 does not appear among the
$\mVar{x}$~\glspl{variable} (the fact that 5 appears in the permutation does
not matter).
%
However, \mbox{$\mValuePrecChain(\langle 5, 4 \rangle, \mVar{x}_1 \in \mSet{1},
  \mVar{x}_2 \in \mSet{4})$} does not hold because the~4 is not preceded by a~5.

As will be seen in \refChapter{solving-techniques}, $\mValuePrec$ is used to
implement a \gls{dominance breaking.c} \gls{constraint}.


\paragraph{The Cumulative Constraint}
\labelSection{cp-cumulative}

The \gls!{cumulative constraint}~\cite{SchuttEtAl:2011} is used in scheduling to
constrains the scheduling times for a given set of tasks such that the capacity
of a given resource is not exceeded.
%
We refer to this \gls{constraint} as $\mCumulative$, which is defined as
follows.
%
\begin{definition}[$\mCumulative$]
  Let \mbox{$c \in \mNatNumSet$} represent the capacity of a resource to be used
  by $k$ optional tasks.
  %
  For each task~$i$, let \mbox{$\mVar{s}_i \in \mNatNumSet$} be a \gls{variable}
  representing the time at which $i$ is scheduled to start, \mbox{$l_i \in
    \mNatNumSet$} represent its latency, \mbox{$u_i \in \mNatNumSet$} represent
  the amount of resource required by~$i$, and \mbox{$\mVar{b}_i \in \mSet{0,
      1}$} be a \gls{variable} representing whether $i$ is scheduled.
  %
  Then
  %
  \begin{displaymath}
    \begin{array}{c}
      \mCumulative(
        c \hspace{-1pt},
        \mTuple{
          \mVar{s}_1 \hspace{-.8pt},
          l_1 \hspace{-.8pt},
          u_1 \hspace{-.8pt},
          \mVar{b}_1
        } \hspace{-1pt},
        \ldots,
        \mTuple{
          \mVar{s}_k \hspace{-.8pt},
          l_k \hspace{-.8pt},
          u_k \hspace{-.8pt},
          \mVar{b}_k
        }
      )
      \equiv \mbox{} \\
      \mSetBuilder*{%
                     \langle d_{\mVar{s}_1} \hspace{-.8pt}, \ldots, d_{\mVar{s}_k}
                             \hspace{-.8pt},
                             d_{\mVar{b}_1} \hspace{-.8pt}, \ldots, d_{\mVar{b}_k}
                             \hspace{-1pt}
                     \rangle
                   }%
                   {
                     \begin{array}{@{}l@{}}
                       \forall_{\! i} \: d_{\mVar{s}_i} \in \mDomain(\mVar{s}_i),
                       \forall_{\! i} \: d_{\mVar{b}_i} \in \mDomain(\mVar{b}_i)
                       \text{ \st} \\
                       \forall_{\! 1 \,\leq\, t \,<\, \mMax(\cup_i \mDomain(\mVar{s}_i))}
                         \displaystyle
                         \hspace{-1.2em}
                         \sum_{%
                                \substack{
                                  \forall{i} \text{ \st} \\
                                  d_{\mVar{s}_i} \,\leq\, t \,<\,
                                  d_{\mVar{s}_i} \hspace{-.8pt}+\, l_i
                                }
                              }
                         \hspace{-1.2em}
                           d_{\mVar{b}_i} \times u_i
                         \leq c
                     \end{array}
                   }\!.
    \end{array}
  \end{displaymath}
  \labelDefinition{cumulative}
\end{definition}

\begin{figure}
  \centering%
  \input{figures/future-work/cumulative-example}

  \caption[Example illustrating the cumulative constraint]%
          {%
            Example of a solution to the cumulative constraint.
            %
            Each box represents a task%
          }
  \labelFigure{cumulative-example}
\end{figure}

For example, the schedule shown in \refFigure{cumulative-example} is a
\gls{solution} to this \gls{constraint}.

As will be seen in \refChapter{future-work}, $\mCumulative$ is used to integrate
\gls{instruction scheduling}.


\paragraph{The No-Overlap Constraint}
\labelSection{cp-no-overlap}

The last \gls{global.c} \gls{constraint} we will look at is the \gls!{no-overlap
  constraint} (often also called the \gls!{diffn
  constraint}~\cite{BeldiceanuContejean:1994}), which is used in rectangle
packing problems to enforce that no two rectangles may overlap
%
We refer to this \gls{constraint} as $\mNoOverlap$, which is defined as follows.
%
\begin{definition}[$\mNoOverlap$]
  For each rectangle~$i$, let \mbox{$\mVar{xl}_i, \mVar{xr}_i, \mVar{yl}_i,
    \mVar{yu}_i \in \mNatNumSet$} be \glspl{variable} representing the
  rectangle's left, right, lower, respectively upper boundary.
  %
  Then
  %
  \begin{displaymath}
    \begin{array}{c}
      \mNoOverlap(
        \mTuple{
          \mVar{xl}_1 \hspace{-.8pt},
          \mVar{xr}_1 \hspace{-.8pt},
          \mVar{yl}_1 \hspace{-.8pt},
          \mVar{yu}_1 \hspace{-.8pt}
        } \hspace{-1pt},
        \ldots,
        \mTuple{
          \mVar{xl}_k \hspace{-.8pt},
          \mVar{xr}_k \hspace{-.8pt},
          \mVar{yl}_k \hspace{-.8pt},
          \mVar{yu}_k \hspace{-.8pt}
        }
      )
      \equiv \mbox{} \\
      \mSetBuilder*{%
                     \left\langle
                       \begin{array}{@{}c@{}}
                         d_{\mVar{xl}_1} \hspace{-.8pt}, \ldots, d_{\mVar{xl}_k}
                         \hspace{-.8pt},
                         d_{\mVar{xr}_1} \hspace{-.8pt}, \ldots, d_{\mVar{xr}_k}
                         \hspace{-.8pt}, \\
                         d_{\mVar{yl}_1} \hspace{-.8pt}, \ldots, d_{\mVar{yl}_k}
                         \hspace{-.8pt},
                         d_{\mVar{yu}_1} \hspace{-.8pt}, \ldots, d_{\mVar{yu}_k}
                       \end{array}
                     \right\rangle
                   }%
                   {
                     \begin{array}{@{}l@{}}
                       \forall_{\! i} \: d_{\mVar{xl}_i} \in \mDomain(\mVar{xl}_i),
                       \forall_{\! i} \: d_{\mVar{xr}_i} \in \mDomain(\mVar{xr}_i),
                       \\
                       \forall_{\! i} \: d_{\mVar{yl}_i} \in \mDomain(\mVar{yl}_i),
                       \forall_{\! i} \: d_{\mVar{yu}_i} \in \mDomain(\mVar{yu}_i)
                       \text{ \st} \\
                       \begin{array}{@{}r@{\:}l@{}}
                           \forall_{\! 1 \,\leq\, i \,<\, j \,\leq\, k}
                         & d_{\mVar{xr_i}} \leq d_{\mVar{xl_j}} \mOr
                           d_{\mVar{xl_i}} \geq d_{\mVar{xr_j}} \mOr \mbox{} \\
                         & d_{\mVar{yu_i}} \leq d_{\mVar{yl_j}} \mOr
                           d_{\mVar{yl_i}} \geq d_{\mVar{yu_j}}
                       \end{array}
                     \end{array}
                   }\!.
    \end{array}
  \end{displaymath}
  \labelDefinition{no-overlap}
\end{definition}

\todo{give example}

As will be seen in \refChapter{future-work}, $\mCumulative$ is used to integrate
\gls{register allocation}.


\subsection{Optimization}

In \gls{CP}, an optimization problem is modeled by maximizing or minimizing a
\gls{variable}~$\mVar{c}$ whose value is constrained according to the
\gls{objective function}.
%
For example, if \mbox{$\mVar{x}_m \in \mSet{0, 1}$} is \gls{variable}
representing whether match~$m$ is selected and $c_m$ denotes the cost of
selecting $m$, then a \gls{CP} idiom for modeling \gls{optimal.ps} \gls{pattern
  selection} is
%
\begin{equation}
  \begin{array}{rl}
      \text{minimize} & \mVar{c} \\
    \text{subject to} & \mVar{c} = \displaystyle\sum_m c_m \mVar{x}_m
  \end{array}
  \labelEquation{pattern-selection-in-cp}
\end{equation}
%
In this context, $\mVar{c}$ is called a \gls!{cost variable}.
%
Note that the \gls{objective function} is orthogonal to the rest of the
\glsshort{constraint model}, thus allowing it to be easily customized to fit the
desired optimization criterion.


\section{Solving}
\labelSection{cp-solving}

A \gls!{constraint solver} (or just \gls!{solver}) finds \glspl{solution} to a
\gls{constraint model} by interleaving \gls{propagation} and \gls{search}.
%
\Gls!{propagation} removes \gls{domain} values that are known not to appear in
any \gls{solution}, and \gls!{search} attempts several alternatives when
\gls{propagation} is not enough to find a \gls{solution}.

In practice, however, this alone is often not enough for many problem instances
because the \gls{search space} is simply too large.
%
In such cases, the \gls{search space} can be further reduced by extending the
\gls{constraint model} with additional \glspl{constraint} to strengthen
propagation and remove uninteresting \glspl{solution} and by performing
\gls{presolving}.


\subsection{Propagation}
\labelSection{cp-propagation}

Performing \gls{propagation} requires an array of \gls{domain}-pruning
algorithms and a system that allows these algorithms to interact.
%
\Gls{propagation} theory is discussed in detail by \textcite{Bessiere:2006}, and
\glsdesc{CP} systems are throroughly discussed by
\textcite{SchulteCarlsson:2006}.

\Gls{constraint solver} typically keep track of \glspl{variable} and their
\glspl{domain} using \glspl{constraint store}.
%
A \gls!{constraint store} (or just \gls!{store}) is a data structure that maps a
set of \glspl{variable} to sets of \glspl{domain}.
%
A \gls{store}~$S_1$ is \gls!{stronger.cs} than another \gls{store}~$S_2$,
denoted \mbox{$S_1 \mStronger S_2$}, if \mbox{$\mDomain_1(\mVar{x}) \subseteq
  \mDomain_2(\mVar{x})$} for all \glspl{variable}~$\mVar{x}$, where
$\mDomain_i(\mVar{x})$ denotes the domain of \gls{variable}~$\mVar{x}$ in
store~$S_i$.

A function that takes a \gls{constraint store} as input and produces another
\gls{store} is called a \gls!{propagator} (or \gls!{filtering algorithm}).
%
A \gls{propagator} implements a \gls{constraint} if it does not remove any
\glspl{solution} to the \gls{constraint} and only keeps \gls{variable}
assignments that are part of a \gls{solution}.
%
For solving to be well-behaved, \glspl{propagator} are also expected to be
\gls!{decreasing.p} -- it does not add any values, hence \mbox{$p(S) \mStronger
  S$} -- and \gls!{monotonic.p} -- if \mbox{$S_1 \mStronger S_2$}, then
\mbox{$p(S_1) \mStronger p(S_2)$}.
%
A \gls{propagator} for which \mbox{$p(S) = S$} holds is said to be at
\gls!{fixpoint}, and a \gls{store} is at \gls{fixpoint} if all
\glspl{propagator} are at \gls{fixpoint} for that \gls{store}.
%
A \gls{propagator} that returns a \gls{store} with at least one empty
\gls{domain} has \glsshort!{failure}, meaning there are no \glspl{solution} in
this part of the \gls{search space}.

\Glspl{propagator} implementing the same \gls{constraint} can differ in the
amount of propagation they perform.
%
A \gls{propagator} is \glshyphened!{value consistency} if it only
\glsshort{propagation}[es] when one of its \glspl{variable} becomes
\gls{assigned.v}, \glshyphened!{bounds consistency} if it only reduces the
bounds of a \gls{domain}, and \glshyphened!{domain consistency} if it removes
all values that do not appear in any \gls{solution} to the \gls{constraint}.
%
\begin{table}
  \centering%
  \figureFont\figureFontSize%
  \newcolumntype{C}{>{$}c<{$}}%
  \subcaptionbox{%
                  Solving with value-consistent inequality constraints%
                  \labelTable{cp-prop-strengths-example-inequality-cons}%
                }{%
                  \begin{tabular}{%
                                   p{4.5cm}%
                                   C@{}C@{ }C@{}C%
                                   C@{}C@{ }C@{}C%
                                   C@{}C@{ }C@{ }C@{ }C@{}C%
                                 }
                    \toprule
                      \multicolumn{1}{c}{\tabhead event}
                        & \multicolumn{14}{c}{\tabhead store}\\
                      \cmidrule(lr){2-15}%
                        & \multicolumn{4}{c}{$\mVar{x}$}
                        & \multicolumn{4}{c}{$\mVar{y}$}
                        & \multicolumn{6}{c}{$\mVar{z}$}\\
                    \midrule
                      Initial store
                        & \{ & 1, & 2  & \}
                        & \{ & 1, & 2  & \}
                        & \{ & 1, & 2, & 3, & 4 & \}\\
                      Propagate until fixpoint
                        & \{ & 1, & 2  & \}
                        & \{ & 1, & 2  & \}
                        & \{ & 1, & 2, & 3, & 4 & \}\\
                      Search by attempting $\mVar{z} = 1$
                        & \{ & 1, & 2  & \}
                        & \{ & 1, & 2  & \}
                        & \{ & 1\phantom{,}
                                  &    &    &   & \}\\
                      Propagate $\mVar{y} \neq \mVar{z}$
                        & \{ & 1, & 2  & \}
                        & \{ &    & 2  & \}
                        & \{ & 1\phantom{,}
                                  &    &    &   & \}\\
                      Propagate $\mVar{x} \neq \mVar{z}$
                        & \{ &    & 2  & \}
                        & \{ &    & 2  & \}
                        & \{ & 1\phantom{,}
                                  &    &    &   & \}\\
                      Propagate $\mVar{x} \neq \mVar{y}$
                        & \{ &    &    & \}
                        & \{ &    & 2  & \}
                        & \{ & 1\phantom{,}
                                  &    &    &   & \}\\
                      Failure reached; backtrack
                        & \{ & 1, & 2  & \}
                        & \{ & 1, & 2  & \}
                        & \{ &    & 2, & 3, & 4 & \}\\
                      \qquad\raisebox{0pt}[10pt]{$\vdots$}
                        & & & & & & & & & & & & & & \\[-2pt]
                    \bottomrule
                  \end{tabular}%
                }

  \vspace{\betweensubfigures}

  \subcaptionbox{%
                  Solving with domain-consistent all-different constraint%
                  \labelTable{cp-prop-strengths-example-alldiff}%
                }{%
                  \begin{tabular}{%
                                   p{4.5cm}%
                                   C@{}C@{ }C@{}C%
                                   C@{}C@{ }C@{}C%
                                   C@{}C@{ }C@{ }C@{ }C@{}C%
                                 }
                    \toprule
                      \multicolumn{1}{c}{\tabhead event}
                        & \multicolumn{14}{c}{\tabhead store}\\
                      \cmidrule(lr){2-15}%
                        & \multicolumn{4}{c}{$\mVar{x}$}
                        & \multicolumn{4}{c}{$\mVar{y}$}
                        & \multicolumn{6}{c}{$\mVar{z}$}\\
                    \midrule
                      Initial store
                        & \{ & 1, & 2  & \}
                        & \{ & 1, & 2  & \}
                        & \{ & 1, & 2, & 3, & 4 & \}\\
                      Propagate $\mAllDifferent(\mVar{x}, \mVar{y}, \mVar{z})$
                        & \{ & 1, & 2  & \}
                        & \{ & 1, & 2  & \}
                        & \{ &    &    & 3, & 4 & \}\\
                      \qquad\raisebox{0pt}[10pt]{$\vdots$}
                        & & & & & & & & & & & & & & \\[-2pt]
                    \bottomrule
                  \end{tabular}%
                }

  \caption[Example illustrating propagation]%
          {%
            Example illustrating propagation for two versions of the model given
            in \refTable{cp-model-example}, one using the all-different
            constraint and the other using a binary decomposition%
          }
  \labelTable{cp-prop-strengths-example}
\end{table}
%
For example, \refTable{cp-prop-strengths-example} shows solving of two versions
of the \glsshort{constraint model} given in \refTable{cp-model-example}, one
using $\mAllDifferent$ and another using inequality \glspl{constraint}.
%
Because only \gls{value consistency} can be achieved for inequality
\glspl{constraint}, they cannot \glsshort{propagation}[e] anything until at
least one \gls{variable} becomes \gls{assigned.v}.
%
As all \glspl{propagator} are already at \gls{fixpoint}, the \gls{solver} must
resort to \gls{search}.
%
In this case, the \gls{solver} makes a wrong guess and is forced to backtrack.
%
In comparison, a \glshyphened{domain consistency} \gls{propagator} for the
\gls{all-different constraint} can remove values~1 and~2 from the \gls{domain}
of \gls{variable}~$\mVar{z}$ as these values do not appear in any
\glspl{solution} (see \refTable{cp-model-example-solutions}).
%
Maximizing \gls{propagation} is key in making solving tractable, as the
\gls{search space} grows exponentially with the number of \glspl{variable} and
size of the \glspl{domain}.

As to be expected, stronger \gls{propagation} comes at a price of greater
complexity.
%
For the \gls{all-different constraint}, there exist \glsshort{bounds
  consistency} and \glshyphened{domain consistency} \glspl{propagator} with
worst-case time complexities~$\mBigO(n \log n)$~\cite{Lopez-OrtizEtAl:2003}
and~$\mBigO(n^{2.5})$~\cite{Regin:1994}, respectively, where $n$ denotes the
number of \glspl{variable}.
%
The same can be achieved for the \gls{global cardinality constraint} at similar
cost~\cite{QuimperEtAl:2005, Regin:1996}.

Many \glshyphened{domain consistency} \gls{propagator} exist for the \gls{table
  constraint} \cite{LecoutreSzymanek:2006, Lecoutre:2011, MairyEtAl:2014,
  PerezRegin:2014, LecoutreEtAl:2015, DemeulenaereEtAl:2016}, although these
exhibit worst-case time complexity.
%
\Gls{domain consistency} for \gls{circuit constraint} cannot be achieved in
polynomial time as it involves finding Hamiltonian cycles, which is an
NP-complete problem~\cite{GareyJohnson:1979}, although an incomplete
polynomial-time \gls{filtering algorithm} is given in~\cite{KayaHooker:2006}.


\subsection{Search}
\labelSection{cp-search}

When no more \gls{propagation} can be performed -- that is, when all
\glspl{propagator} are at \gls{fixpoint} -- the \gls{solver} resorts to
\gls{search}, which is discussed in detail by \textcite{VanBeek:2006}.

In exploring the \gls{search space}, two decisions need to be made repeatedly:
%
\begin{enumerate*}[label=(\arabic*), itemjoin*={, and\ }]
  \item select a \gls{variable} on which to branch
  \item select a value from its \gls{domain}
\end{enumerate*}.
%
These decisions constitute a \gls!{branching strategy} which arranges the
\gls{search space} into a \gls!{search tree}, where each \gls{node} represents a
\gls{store} at \gls{fixpoint}.
%
An example is given in \refFigure{cp-search-tree-sat-example}.
%
Since the \glspl{solution} (and \glspl{failure}) appear at the leaf
\glspl{node}, the \gls{search tree} is typically explored depth-first.

\begin{figure}
  \centering%
  \input{figures/constraint-programming/search-tree-sat-example}

  \caption[Example of a search tree]%
          {%
            Example of a search tree for the model given in
            \refTable{cp-model-example}.
            %
            Diamond-shaped nodes represent solutions%
          }
  \labelFigure{cp-search-tree-sat-example}
\end{figure}

For \gls{search} to be well-behaved, a \gls{branching strategy} must preserve
all \glspl{solution} in the \gls{search space} and must not duplicate any
\gls{solution}.
%
A common strategy in choosing a \gls{variable}, called the \gls!{first-fail
  principle}, is to select the \gls{variable} most likely to cause a
\gls{failure}~\cite{HaralickElliott:1980}.
%
Other strategies involve selecting the \gls{variable} with the smallest or
largest value in its \gls{domain}, or selecting a random \gls{variable}.
%
Similar strategies are applied in value selection, which is typically done by
posting \gls{constraint} when branching.
%
Most common is to post unary \gls{constraint} that divides a \gls{domain} into
two.
%
For example, at the root \gls{node} in \refFigure{cp-search-tree-sat-example},
\gls{search} branches on \gls{variable}~$\mVar{x}$ by posting \mbox{$\mVar{x} =
  1$} in one branch and \mbox{$\mVar{x} \neq 1$} in the other.
%
Another strategy is to split the domain by posting inequality \glspl{constraint}
(for example, \mbox{$\mVar{x} \leq 3$} in one branch and \mbox{$\mVar{x} > 3$}
in the other), which is useful for solving \glsplshort{constraint model} with
arithmetic \glspl{constraint}.
%
More than one \gls{branching strategy} can be used for the same
\glsshort{constraint model} and, if needed, they can be customized by the user,
making it a key strength of \gls{CP}.


\paragraph{Branch and Bound}

\Glspl{solution} to optimization problems are found using a method called
\gls!{branch and bound}.
%
During \gls{search}, the best \gls{solution} found so far is kept and a
\gls{constraint} is added to enforce all subsequent \glspl{solution} to have
strictly less (or greater) cost, allowing time can be traded for quality on a
continuous time scale.
%
When the entire \gls{search space} has been explored, the last found
\gls{solution} is guaranteed to be optimal.
%
\begin{figure}
  \centering%
  \input{figures/constraint-programming/search-tree-opt-example}

  \caption[Example of a search tree for an optimization problem]%
          {%
            Example of a search tree for the model given in
            \refTable{cp-model-example} with the additional requirement that
            the value of $\mVar{z}$ should be maximized.
            %
            The search tree is explored depth first, left to right.
            %
            Diamond-shaped nodes represent solutions and square-shaped nodes
            represent failures%
          }
  \labelFigure{cp-search-tree-opt-example}
\end{figure}
%
An example of shown in \refFigure{cp-search-tree-opt-example}.
%
Assuming the \gls{search tree} is explored depth first, left to right, the first
solution to be found is \mbox{$S_1 = \mTuple{\mVar{x} = \mSet{1}, \mVar{y} =
    \mSet{2}, \mVar{z} = \mSet{3}}$}.
%
Since the value of $\mVar{z}$ is to be maximized, this causes the
\gls{constraint} \mbox{$\mVar{z} > 3$} to be posted.
%
The next solution to be found is \mbox{$S_2 = \mTuple{\mVar{x} = \mSet{1},
    \mVar{y} = \mSet{2}, \mVar{z} = \mSet{4}}$}, which is clearly better than
$S_1$, causing the \gls{constraint} \mbox{$\mVar{z} > 4$} to be posted.
%
Since the \gls{domain} of $\mVar{z}$ has no value greater than 4, the
\gls{constraint} \mbox{$\mVar{z} > 4$} causes a \gls{failure} when the exploring
the other branch at the root.
%
At this point the entire \gls{search space} has been explored, making $S_2$ the
optimal \gls{solution}.


\subsection{Solving Techniques}
\labelSection{cp-solving-techniques}

Solving can be improved by applying various solving techniques, which can be
divided into two categories.
%
The first category involves additional \glspl{constraint} that are added to the
\glsshort{constraint model} in order to increase \gls{propagation} and also
reduce the \gls{search space}.
%
These \glspl{constraint} can be divided into three categories --
\gls{implied.c}, \gls{symmetry breaking.c}, and \gls{dominance breaking.c} --
which are discussed in detail by \mbox{\textcite{Smith:2006}},
\textcite{GentEtAl:2006}, and \textcite{ChuStuckey:2015}, respectively.

The second category involves applying methods that reduce the number of
\glspl{variable} or shrink the \gls{variable} \glspl{domain}, thereby reducing
the \gls{search space}.


\paragraph{Implied Constraints}

An \gls!{implied.c}[ \gls{constraint}] is a \gls{constraint} that strengthens
\gls{propagation} without removing any \glspl{solution}.
%
For example, assume a naive \glsshort{constraint model} for solving the
\gls!{magic sequence problem}, which is defined as finding a sequence
\mbox{$x_0, \ldots, x_{n-1}$} of integers such that for all \mbox{$0 \leq i <
  n$}, the number~$i$ appears exactly $x_i$ times in the sequence.
%
Using the \gls{global cardinality constraint} and $n$~\glspl{variable}, this can
be modeled as
%
\begin{equation}
  \mGCC(i, \mVar{x}_0, \ldots, \mVar{x}_{n-1}, \mVar{x}_i)
  \mQuantSep
  \forall 0 \leq i < n.
\end{equation}
%
While this \gls{constraint} is sufficient in capturing the problem,
\gls{propagation} can be increased by adding the following \gls{constraint}:
%
\begin{equation}
  \sum_{i = 0}^{n - 1} \mVar{x}_i = n.
\end{equation}
%
This holds because the sum of all occurrences -- that is, the number of items in
the sequence -- must be equal to the length of the sequence.


\paragraph{Symmetry Breaking Constraints}

A \gls!{symmetry breaking.c}[ \gls{constraint}] is a \gls{constraint} that
removes \glspl{solution} considered to be symmetric to one another.
%
For example, assume a \glsshort{constraint model} for solving a problem of
packing $n$ squares of sizes \mbox{$1, \ldots, n$} inside another, larger
square.
%
Given a \gls{solution} to this problem, more \glspl{solution} can found by
rotating, flipping, and mirroring the initial \gls{solution}.
%
As these \glspl{solution} are essentially the same, only one of them should be
kept in the \gls{search space}.
%
A simple method of removing most (but not all) symmetric \glspl{solution} is to
force one of the squares to be packed into one of the quadrants of the enclosing
square.


\paragraph{Dominance Breaking Constraints}

A \gls!{dominance breaking.c}[ \gls{constraint}] is a \gls{constraint} that
removes \glspl{solution} known to be dominated by another \gls{solution}.
%
\Gls{dominance breaking.c} is therefore a generalization of \gls{symmetry
  breaking.c}.
%
\begin{figure}
  \centering%
  \mbox{}%
  \hfill%
  \hfill%
  \input{figures/constraint-programming/square-packing-partial-sol1}%
  \hfill%
  \input{figures/constraint-programming/square-packing-partial-sol2}%
  \hfill%
  \hfill%
  \mbox{}

  \caption[Example of dominating solutions]%
          {%
            Example of two partial solutions to the square packing problem,
            where the left-most solution is dominated by the right-most
            solution%
          }
  \labelFigure{square-packing-partial-sol}
\end{figure}
%
For an example, let us revisit the \glsshort{constraint model} capturing the
square packing problem and assume the two partial \glspl{solution} shown in
\refFigure{square-packing-partial-sol}~\cite{Korf:2004}.
%
In the left-most \gls{solution}, the positioning of the two squares form an
empty \mbox{$2 \times 3$} rectangle in the corner.
%
If this is extended to a complete \gls{solution}, another \gls{solution} can be
found by sliding the \mbox{$3 \times 3$} square all the way up and moving any
squares packed above into the space created below.
%
Hence the left-most \gls{solution} is dominated by the right-most
\gls{solution}, and such \glspl{solution} can be removed from the \gls{search
  space} by forbidding each \mbox{$k \times k$} square from being placed a
certain distance away from the edge of the enclosing square such that the
\mbox{$k \times k$} square and the edge forms a rectangle wherein all smaller
squares can be packed.

The benefit of a given \gls{implied.c}, \gls{symmetry breaking.c}, or
\gls{dominance breaking.c} \gls{constraint} depends on the amount of \gls{search
  space} it prunes and the cost of \glsshort{propagation}[ing] the
\gls{constraint}.
%
For example, if a \gls{constraint} is expensive to run and only has marginal
effect on the \gls{variable} \gls{domain}, then adding it to a
\glsshort{constraint model} will \emph{increase} solving time instead of
decreasing it.
%
In addition, it is well known that such \glspl{constraint} often have synergy
effects among each other, meaning a \gls{constraint} may not be useful on its
own but may have a positive effect when combined with another \gls{constraint}.
%
Consequently, the decision of whether to add a \gls{implied.c}, \gls{symmetry
  breaking.c}, or \gls{dominance breaking.c} \gls{constraint} to a
\glsshort{constraint model} must be based on careful and thorough experimental
evaluation.


\paragraph{Presolving}

\Gls!{presolving} is the process of applying problem-specific algorithms to
reduce the number of \glspl{variable} or to shrink the \gls{variable}
\glspl{domain} before solving.
%
Fewer \gls{variable} and smaller \glspl{domain} means smaller \glspl{constraint
  model}, which means shorter solving times.

When dealing with optimization problems, a common \gls{presolving} technique is
to precompute lower and upper bounds on the \gls{cost variable}.
%
A lower bound can be found by solving a relaxed, and hence simpler, version of
the \gls{constraint model}, which enables pruning of parts in the \gls{search
  space} that contain no \glspl{solution}.
%
An upper bound can be found by solving the problem using a greedy but fast
heuristic, which enables pruning of parts in the \gls{search space} that contain
inferior \glspl{solution}.
%
If applying the upper bound yields a \gls{search space} with no
\glspl{solution}, then we know that the heuristic has already found the optimal
\gls{solution}.

In the context of \gls{instruction selection}, another \gls{presolving}
technique is to remove \glspl{match} that we know cannot participate in any
\gls{solution}.
%
Several such techniques are introduced in \refChapter{solving-techniques}.


\subsection{Lazy Clause Learning}

\todo{write}
