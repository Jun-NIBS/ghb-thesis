% Copyright (c) 2017, Gabriel Hjort Blindell <ghb@kth.se>
%
% This work is licensed under a Creative Commons 4.0 International License (see
% LICENSE file or visit <http://creativecommons.org/licenses/by/4.0/> for a copy
% of the license).

\chapter{Solving Techniques}
\labelChapter{solving-techniques}

This chapter introduces the techniques applied for improving solving of the
\gls{constraint model} presented in the previous chapter.
%
\todo{rewrite}
%
\RefSection{st-model-refinements} introduces \glsshort{constraint model}
refinements to strenghten \gls{propagation}.
%
\RefSection{st-cost-bounds} introduces techniques for bounding the \gls{cost
  variable}.
%
\RefSection{st-branching-strategies} introduces \glspl{branching strategy}.
%
\RefSectionList{st-impl-constraints, st-dom-breaking-constraints} introduce
\gls{implied.c} respectively \glsshort{symmetry breaking.c} and \gls{dominance
  breaking.c} \glspl{constraint}.
%
\RefSection{st-presolving} introduces \gls{presolving} techniques.
%
\RefSection{st-experimental-evaluation} presents an experimental evaluation of
the above techniques.
%
Lastly, \refSection{st-summary} summarizes the chapter.

The material presented in \refSectionList{st-refining-define-before-use
  constraint, st-branching-strategies, st-impl-constraints,
  st-dom-breaking-constraints, st-pre-dom-matches, st-canonical-locations} is
based on ideas conceived by Mats~Carlsson, which have then been improved by the
author of this dissertation.
%
All other materials in this chapter, including the text and figures, are due
entirely to the author.


\section{Refining the Define-Before-Use Constraint}
\labelSection{st-refining-define-before-use constraint}

In \refChapter{constraint-model}, a simple but naive implementation is used for
implementing the \gls{constraint} that all \glspl{datum} must be
\gls{define.d}[d] before \gls{use.d}[d] (\refEquation{naive-dom-alt}).
%
To begin with, it requires the use of set~\glspl{variable},\!%
%
\footnote{%
  This is because the $\mVar{dplace}$~\glspl{variable} need to be members of the
  $\mDom$~\gls{function}, which is implemented as an array into which the
  $\mVar{oplace}$~\glspl{variable} are used as indices.%
}
%
which are expensive to use and necessarily supported by all \glspl{constraint
  solver}.
%
This is for example the case of \gls{Chuffed}, which is used in the experimental
setup applied in this dissertation, thus preventing us from evaluating the
impact of the naive implementation.
%
In addition, if we know in which \glspl{block} a \gls{datum} is \gls{use.d}[d],
then many \gls{implied.c} \glspl{constraint} can be applied to strengthen
\gls{propagation}.

We first eliminate the set~\glspl{variable} by capturing the information in the
$\mDom$~\gls{function} as a dominance relation matrix
%
\begin{equation}
  \begin{bmatrix}
    \begin{array}{@{\:}c|c@{\:}}
        b_1, b_2
      & b_1, b_2 \in \mBlockSet, b_1 \in \mDom(b_2)
    \end{array}
  \end{bmatrix}
  \labelEquation{dominance-matrix}
\end{equation}
%
where each row denotes the fact that a \gls{block}~$b_1$ is \gls{dominate.b}[d]
by another \gls{block}~$b_2$.
%
Next, we introduce the \glspl{variable} needed for capturing \gls{use.d}[s] of
\glspl{datum}.


\paragraph{Variables}

The set of \glspl{variable} \mbox{$\mVar{uplace}[p] \in \mBlockSet$} models in
which \gls{block} the \gls{datum} connected to \gls{operand}~$p$ is used.
%
Note that unlike the $\mVar{dplace}$~\glspl{variable}, which are indexed using a
\gls{datum}, the $\mVar{uplace}$~\glspl{variable} are indexed using an
\gls{operand}.


\paragraph{Constraints}

Obviously, every \gls{use.d} of \glspl{datum} must be \gls{dominate.b}[d] by its
definition.
%
Given a \glsshort{dominate.b}[ance] relation matrix~$\mDomMatrix$, this
\gls{constraint} is modeled as
%
\begin{equation}
  \mTable(\mVar{uplace}[p], \mVar{dplace}[\mVar{alt}[p]], \mDomMatrix)
  \mQuantSep
  \forall p \in \mPhiOperandCompSet.
  \labelEquation{refined-dom}
\end{equation}
%
where \mbox{$\mPhiOperandCompSet \subseteq \mOperandSet$} denotes the set of
\glspl{operand} not appearing in any \gls{phi-match}.
%
We exclude such \glspl{operand} for the same reasons \glspl{phi-match} are
excluded in \refEquation{naive-dom}.

Next, if a \gls{match}~$m$ is selected and placed in \gls{block}~$b$, then all
\gls{use.d}[s] of \glspl{datum} made by $m$ should also occur in~$b$.
%
This is modeled as
%
\begin{equation}
  \begin{array}{c}
    \mVar{sel}[m] \mImp \mVar{oplace}[o] = \mVar{uplace}[p] \\
    \forall m \in \mPhiMatchCompSet,
    \forall o \in \mCovers(m),
    \forall p \in \mUses(m).
  \end{array}
  \labelEquation{refined-dom-selected}
\end{equation}
%
An example illustrating the interaction between \refEquation{refined-dom} and
\refEquation{refined-dom-selected} is shown in \refFigure{refined-dom-example}.

\begin{figure}
  \centering%
  \input{figures/solving-techniques/dom-example}%

  \caption[Example illustrating the refined define-before-use constraint]%
          {%
            Example illustrating the refined define-before-use constraint.
            %
            The assignments on the right-hand side show how these
            variables should be set if the two matches~$m_1$ and~$m_2$ are
            placed in blocks~$b_1$ respectively $b_2$, where $b_1$ is assumed to
            dominate~$b_2$%
          }
  \labelFigure{refined-dom-example}
\end{figure}

Due to \refEquation{refined-dom-selected}, the assignment to the
$\mVar{uplace}$~\glspl{variable} for non-selected \glspl{match} does not matter
as long as \refEquation{refined-dom} is satisfied, which gives rise to symmetric
\glspl{solution}.
%
To break these symmetries, we fix the assignments in such cases to the
\gls{block} wherein the \gls{datum} is \gls{define.d}[d] since a \gls{block}
always \gls{dominate.b}[s] itself.
%
This is modeled as
%
\begin{equation}
  \begin{array}{c}
    \neg\mVar{sel}[m] \mImp \mVar{uplace}[p] = \mVar{dplace}[\mVar{alt}[p]] \\
    \forall m \in \mPhiMatchCompSet,
    \forall p \in \mUses(m).
  \end{array}
  \labelEquation{refined-dom-not-selected}
\end{equation}

Since neither of the above \glspl{constraint} apply to \glspl{operand} belonging
to \glspl{phi-match}, the assignment to their $\mVar{uplace}$~\glspl{variable}
does not matter, again giving rise to symmetric \glspl{solution}.
%
We therefore fix the assignment in such cases, which is modeled as
%
\begin{equation}
  \mVar{uplace}[p] = \mMin(\mBlockSet)
  \mQuantSep
  \forall p \in \mPhiOperandSet.
  \labelEquation{refined-dom-phi-operands}
\end{equation}


\section{Refining the Objective Function}
\labelSection{st-refined-objective-function}

The straightforward implementation of the \gls{objective function}
(\refEquation{naive-objective-function}) is naive because it fails to reason on
how cost is distributed across the \glspl{operation} that need to be covered,
which in turn results in poor \gls{propagation}.
%
\begin{figure}
  \mbox{}%
  \hfill%
  \subcaptionbox{UF graph\labelFigure{cost-example-graph}}%
                {%
                  \input{figures/solving-techniques/cost-example-graph}%
                }%
  \hfill%
  \subcaptionbox{Cost matrix\labelFigure{cost-example-matrix}}%
                {%
                  \figureFontSize%
                  \begin{minipage}{50mm}%
                    \centering%
                    \begin{displaymath}
                      \begin{adjblockarray}{cccccc}{1ex}
                          \text{\tabhead op}
                        & \text{\tabhead match}
                        & \text{\tabhead block}
                        & \multicolumn{3}{c}{%
                            \text{%
                              \parbox{20.5mm}{%
                                % The parbox is used to create a bit of space
                                % between the content and the right bracket
                                \centering\tabhead opcost%
                              }%
                            }%
                          } \\[-.5ex]
                        \begin{block}{[cccc@{\;\times\;}c@{\;=\;}c]}
                          \rule{0pt}{2.5ex}
                          o_1 & m_1 & b_1 & 4 & 10 & 40 \\
                          o_1 & m_1 & b_2 & 4 &  1 &  4 \\
                          o_1 & m_3 & b_1 & 3 & 10 & 30 \\
                          o_1 & m_3 & b_2 & 3 &  1 &  3 \\
                          o_2 & m_2 & b_1 & 1 & 10 & 10 \\
                          o_2 & m_2 & b_2 & 1 &  1 &  1 \\
                          o_2 & m_3 & b_1 & 2 & 10 & 20 \\
                          o_2 & m_3 & b_2 & 2 &  1 &  2 \\[.55ex]
                        \end{block}
                      \end{adjblockarray}
                    \end{displaymath}
                  \end{minipage}%
                }%
  \hfill%
  \mbox{}

  \caption[Example illustrating match cost distributed over operations]%
          {%
            Example illustrating match cost distributed over operations.
            %
            It is assumed that matches~$m_1$, $m_2$, and~$m_3$ have costs~4,
            1, and~5, respectively, and that they can be placed in one of two
            blocks, $b_1$ and $b_2$, with execution frequencies~10 and~1,
            respectively.
            %
            The cost of $m_3$ distributed over $o_1$ and $o_2$ is~3 and~2,
            respectively%
          }
  \labelFigure{cost-example}
\end{figure}
%
See for example \refFigure{cost-example}.
%
Assume a \gls{UF graph} that can be covered by three \glspl{match} -- $m_1$,
$m_2$, and~$m_3$ -- which have costs~4, 1, and~5, respectively, and that they
can be placed in one of two blocks, $b_1$ and $b_2$, with execution
frequencies~10 and~1, respectively (\refFigure{cost-example-graph}).
%
Because \refEquation{naive-objective-function} is modeled as a summation, it can
only \glsshort{propagation}[e] the bounds of the \gls{cost variable}.
%
Consequently, for any \glspl{match} where their $\mVar{sel}$~\gls{variable} have
yet to be decided, either all those \glspl{match} are selected and placed in the
\gls{block} with highest execution frequency, or none are selected.
%
In the example above, this means the \gls{cost variable} is initially bounded as
\mbox{$0 < \mVar{cost} < 100$}, which are very weak bounds as we know that the
\gls{UF graph} must at least be covered at a cost of~5 (either both $m_1$ and
$m_2$ are selected and placed in $b_2$, or $m_3$ is selected and placed in
$b_2$) and can at most be covered at a cost of~\num{50} (the selected
\glspl{match} are placed in $b_1$).

Instead of reasoning about the cost incurred by the \glspl{match} -- which may
or may not be selected -- a better approach is to infer the cost incurred on the
\glspl{operation} since these must always be covered.
%
The idea is as follows.
%
First, for each \gls{match}~$m$, evenly divide the cost of $m$ over each
\gls{operation}~$o$ covered by $m$.
%
If a strict partial order $<$ exists over the set of \glspl{operation}, and
$\mCovers(m)$ returns an ordered list that can be indexed starting from~$1$,
then the cost can be divided as follows.
%
Given a match~$m$, if \mbox{$q = \lfloor \mCost(m) / \mCard{\!\mCovers(m)}
  \rfloor$} and \mbox{$r = \mCost(m) \! \mod \mCard{\!\mCovers(m)}$}, then
%
\begin{equation}
  \mOpCost(m, o) =
  \left\{
  \begin{array}{ll}
    q + 1 & \text{if $o < \mCovers(m)[r+1]$}, \\
    q     & \text{otherwise}. \\
  \end{array}
  \right.
  \labelEquation{op-cost-function}
\end{equation}
%
Then, for each \gls{block}~$b$, weigh \mbox{$\mOpCost(m, o)$} with the execution
frequency of~$b$.
%
This information can be represented as a cost matrix
%
\begin{equation}
  \begin{bmatrix}
    \begin{array}{@{\:}c|c@{\:}}
        o, m, b, \big( \mOpCost(m, o) \times \mFreq(b) \big)
      & m \in \mMatchSet, o \in \mCovers(m), b \in \mBlockSet
    \end{array}
  \end{bmatrix}
  \labelEquation{cost-matrix}
\end{equation}
where each row denotes the cost of an \gls{operation}~$o$ if covered by a
\gls{match}~$m$ and placed in a \gls{block}~$b$.
%
An example of this cost matrix is given in \refFigure{cost-example}, from which
we can deduce that the cost of covering operations~$o_1$ and~$o_2$ is between~3
and~40 respectively between~1 and~20 (\refFigure{cost-example-matrix}).
%
Hence the total cost can be bounded as \mbox{$4 < \mVar{cost} < 60$}, which is a
much tighter bound compared to that achieved using the naive \gls{objective
  function}.
%
This in turn has a tremendous impact on solving time, for reasons that will
become clear in \refSection{st-cost-bounds}.

Instead of first evenly dividing the \gls{match} cost over the covered
\glspl{operation} and then multiplying each \gls{operation} cost with the
execution frequency, an alternative method is to first multiply the \gls{match}
cost with the execution frequency and then evenly divide the product over the
covered \glspl{operation}.
%
We call the first method the \gls!{divide-then-multiply method}, and the second
method the \gls!{multiply-then-divide method}.
%
At first glace this design decision would appear to make no difference, but as
will be seen later in the experimental evaluation, they unexpectedly exhibit
significantly different solving time characteristics.
%
One possible explanation is that they may yield different bounds.
%
For instance, for the example given in \refFigure{cost-example} the
\gls{multiply-then-divide method} bounds the total cost as \mbox{$4 <
  \mVar{cost} < 65$}, whereas the \gls{divide-then-multiply method} bounds it to
\mbox{$4 < \mVar{cost} < 60$}.
%
Another possible explanation is that the \gls{divide-then-multiply method}
results in \gls{operation} costs that are even multiples of the execution
frequencies, whereas the \gls{multiply-then-divide method} potentially results
in arbitrary cost values and consequently leads to larger \glspl{domain} for the
\gls{cost variable}.


\paragraph{Variables}

The set of \glspl{variable} \mbox{$\mVar{ocost}[o] \in \mNatNumSet$} models the
cost incurred by covering \gls{operation}~$o$\hspace{-.8pt}.
%
It is assumed the \gls{domain} is the same as for the \gls{cost variable}.


\paragraph{Constraints}

For each \gls{operation}~$o$\hspace{-.8pt}, the combination
\mbox{$o\hspace{-.8pt}, \mVar{omatch}[o], \mVar{oplace}[o], \mVar{ocost}[o]$}
must appear as a row in the cost matrix.
%
Hence, given a cost matrix~$\mCostMatrix$ this \gls{constraint} is modeled as
%
\begin{equation}
  \mTable(
    o\hspace{-1pt},
    \mVar{omatch}[o],
    \mVar{oplace}[o],
    \mVar{ocost}[o],
    \mCostMatrix
  )
  \mQuantSep
  \forall o \in \mOpSet \hspace{-.8pt}.
  \labelEquation{omatch-oplace-ocost-connection}
\end{equation}
%
The total cost is then modeled as
%
\begin{equation}
  \mVar{cost} = \sum_{o \in \mOpSet} \mVar{ocost}[o].
  \labelEquation{total-cost}
\end{equation}


\section{Tightening the Cost Bounds}
\labelSection{st-cost-bounds}

As explained in \refChapter{constraint-programming}, in \gls{CP} optimization
problems are solved using \gls{branch and bound}.
%
In other words, when a \gls{solution} is found a \gls{constraint} is added to
the \glsshort{constraint model}, forcing any subsequently found \glspl{solution}
to be strictly better.
%
This means that if a part of the \gls{search space} is known not to contain any
better \glspl{solution} -- this is achieved by checking the current lower bound
of the \gls{cost variable} -- then this part need not be explored during
\gls{search}.
%
For this to be effective, however, it must be possible to infer reasonable
bounds on the \gls{cost variable}, which asserts the need for the refined
\gls{objective function} introduced in
\refSection{st-refined-objective-function}.

By the same mechanism, solving can improved by tightening the lower and upper
bounds of the \gls{cost variable} before commencing \gls{search}.
%
A tight upper bound helps the \gls{constraint solver} to prune away parts of the
\gls{search space} that contains inferior \glspl{solution}, while a tight lower
bound helps the \gls{constraint solver} to prune away parts of the \gls{search
  space} that contains no \glspl{solution}.
%
A decent upper bound can be computed by solving the same problem using a greedy
but fast heuristic.
%
To this end, any modern \gls{compiler} can be used.
%
A decent lower bound be computed by solving a relaxed version of the
\gls{constraint model} that only models the \gls{global.is} \gls{instruction
  selection} and \gls{block ordering} problems, which obviously is simpler to
solve than the complete \glsshort{constraint model}.
%
Hence the relaxed \glsshort{constraint model} consists of only the
$\mVar{omatch}$, $\mVar{opcosts}$, $\mVar{sel}$, $\mVar{succ}$, and
$\mVar{cost}$ variables, \refEquation{operation-coverage}, relaxed versions of
\refEquationList{block-order, fall-through} that allow fall-throughs via
non-empty \glspl{block}, and modified versions of
\refEquationRange{cost-matrix}{total-cost} that do not take the execution
frequencies into account.

If $\mRelaxedCost$ and $\mHeuristicCost$ denote the cost computed from the
relaxation and by the heuristic, respectively, then the \gls{cost variable} is
bounded as
%
\begin{equation}
  \mRelaxedCost \leq \mVar{cost} < \mHeuristicCost.
  \labelEquation{lower-bound}
\end{equation}


\section{Implied Constraints}
\labelSection{st-impl-constraints}

As explained in \refChapter{constraint-programming}, \gls{implied.c}
\glspl{constraint} are \glspl{constraint} that strengthen the \gls{propagation}
while preserving all \glspl{solution}.
%
Stronger \gls{propagation} leads to less \gls{search}, which in turn leads to
shorter solving times.
%
In this section, we discuss such \glspl{constraint} that have been added to the
\glsshort{constraint model}.


\subsection{Implied Operation and Data Placements}

Due to \refEquationRange{naive-dom}{spanning}, if a selected \gls{match}
\gls{define.d}[s] some \gls{datum}~$d_1$ and \gls{use.d}[s] some other
\gls{datum}~$d_2$, then the \gls{block} wherein $d_2$ is \gls{define.d}[d] must
\gls{dominate.b} the \gls{block} wherein $d_1$ is \gls{define.d}[d].
%
From this observation, we can infer that if all \glspl{match} covering a
non-\gls{phi-node} \gls{operation}~$o$ do not \gls{span.b} any \glspl{block},
\gls{define.d} some \gls{datum}~$d_1$, and \gls{use.d} some \gls{datum}~$d_2$,
then the \gls{block} wherein $d_2$ is \gls{define.d}[d] must \gls{dominate.b}
the \gls{block} wherein $d_1$ is \gls{define.d}[d].
%
In addition, $o$ must be placed in the same \gls{block} wherein $d_1$ is
\gls{define.d}[d].
%
This is modeled as
%
\begin{equation}
  \begin{array}{c}
    \mTable(\mVar{dplace}[d_1], \mVar{dplace}[d_2], \mDomMatrix)
    \mAnd
    \mVar{oplace}[o] = \mVar{dplace}[d_1] \\
    \forall o \in
      \mSetBuilder{o'}%
                  {%
                    o' \in \mOpCompSet{\mPhi},
                    m \in \mMatchSet[o'] \hspace{-1pt}
                    \text{ \st }
                    \mConsumes(m) = \mEmptySet
                  }, \\
    \forall d_1 \in
      \mSetBuilder{d}%
                  {%
                    d \in \mDataOf(o \hspace{-1pt}, \mDefines),
                    m \in \mMatchSet[o] \hspace{-1pt},
                    \exists p \in \mDefines(m)
                    \text{ \st }
                    \mDataSet[p] = \mSet{d}
                  }, \\
    \forall d_2 \in
      \mSetBuilder{d}%
                  {%
                    d \in \mDataOf(o \hspace{-1pt}, \mUses),
                    m \in \mMatchSet[o] \hspace{-1pt},
                    \exists p \in \mUses(m)
                    \text{ \st }
                    \mDataSet[p] = \mSet{d}
                  }.
  \end{array}
  \labelEquation{impl-cons-defs-dominate-defs}
\end{equation}
%
where
%
\begin{equation}
  \mDataOf(o \hspace{-1pt}, f)
  \equiv
  \hspace{-1.5em}
  \bigcup_{\substack{%
                   m \, \in \, \mMatchSet[o] \hspace{-.8pt}, \\
                   p \, \in \, f\hspace{-1pt}(m) \text{ \st} \\
                   \mCovers(m) \, = \, \mSet{o}
                 }}
  \hspace{-1.5em}
  \mDataSet[p]
  \labelEquation{data-of-function}
\end{equation}

Furthermore, we can infer that if all \glspl{match} covering the same
non-\gls{phi-node} \gls{operation} \gls{span.b} a set~$S$ of \glspl{block} and
\gls{define.d} some \gls{datum}~$d$\hspace{-1pt}, then $d$ must be
\gls{define.d}[d] in one of the \glspl{block} in~$S$\hspace{-.8pt}.
%
This is modeled as
%
\begin{equation}
  \begin{array}{c}
    \mVar{dplace}[d] \in S \\
    \forall S \in \mPowerset{\mBlockSet} \hspace{-2pt},
    \forall d \in \mDataSet \!,
    \forall o \in
      \mSetBuilder*{o'}%
                   {%
                     \begin{array}{@{}l@{}}
                       o' \in \mOpCompSet{\mPhi},
                       m \in \mMatchSet[o'] \hspace{-1pt},
                       \exists p \in \mDefines(m) \\
                       \text{\st }
                       \mSpans(m) = S \mAnd \mDataSet[p] = \mSet{d}
                     \end{array}
                   } \!.
  \end{array}
  \labelEquation{impl-cons-defs-in-spanned-blocks}
\end{equation}

From \refEquation{preventing-control-flow-op-moves}, we can infer that if all
non-\glspl{phi-match} covering \gls{operation}~$o$ have \gls{entry
  block}~$b$\hspace{-1pt}, then $o$ must for sure be placed in~$b$\hspace{-1pt}.
%
This is modeled as
%
\begin{equation}
  \begin{array}{c}
    \mVar{oplace}[o] = b \\
    \forall b \in \mBlockSet \hspace{-1pt},
    \forall o \in
      \mSetBuilder{o'}%
                  {%
                    o' \in \mOpSet,
                    m \in \mMatchSet[o'] \setminus \mPhiMatchSet
                    \text{ \st }
                    \mEntry(m) = \mSet{b}
                  }.
  \end{array}
  \labelEquation{impl-cons-identical-entry-blocks}
\end{equation}

Furthermore, we can infer that if the \glspl{match} covering the same
non-\gls{phi-node} \gls{operation} all have identical \glspl{entry block}, say
$b$\hspace{-1pt}, and make \gls{use.d} of some \gls{datum}~$d$\hspace{-1pt},
then the \gls{block} wherein $d$ is \gls{define.d}[d] must dominate
$b$\hspace{-1pt}.
%
This is modeled as
%
\begin{equation}
  \begin{array}{c}
    \mTable(b \hspace{-.8pt}, \mVar{dplace}[d], \mDomMatrix) \\
    \forall b \in \mBlockSet \hspace{-1pt},
    \forall d \in
      \mSetBuilder*{d'}%
                   {%
                     \begin{array}{@{}l@{}}
                       o' \in \mOpCompSet{\mPhi},
                       m \in \mMatchSet[d'] \hspace{-1pt},
                       \exists p \in \mUses(m) \\
                       \text{ \st}
                       \mEntry(m) = \mSet{b} \mAnd \mDataSet[p] = \mSet{d}
                     \end{array}
                   } \!.
  \end{array}
  \labelEquation{impl-cons-defs-dominate-entry-blocks}
\end{equation}

From \refEquation{def-edges}, we can infer that if a \gls{datum}~$d$ appears in
a \gls{definition edge}~$\mEdge{b}{d}$ and is \gls{define.d}[d] by
\glspl{phi-match} only, then the \gls{operation} covered by these \glspl{match}
must be placed $b$.
%
This is modeled as
%
\begin{equation}
  \begin{array}{c}
    \mVar{oplace}[o] = b \\
    \forall \mEdge{d}{b} \in \mFunctionDefEdgeSet,
    \forall o \in
      \mSetBuilder{o'}%
                  {%
                    \begin{array}{@{}l@{}}
                      m \in \mMatchSet[d] \cap \mPhiMatchSet,
                      o' \in \mCovers(m)
                    \end{array}
                  }.
  \end{array}
  \labelEquation{impl-cons-place-phi-ops-same-as-def-edges}
\end{equation}
%
It is assumed that the \glspl{edge} in $\mFunctionDefEdgeSet$ have been
reoriented such that all \glspl{source} are either \glsshort{state node} or
\glspl{value node} and all \glspl{target} are \glspl{block node}.


\subsection{Implied Constraints due to the Define-Before-Use Refinement}

From \refEquationRange{refined-dom}{refined-dom-phi-operands}, we can infer the
following \gls{implied.c} \glspl{constraint}.

If a non-\gls{phi-match} that \gls{span.b}[s] no \glspl{block} is selected, then
all \glspl{datum} it \gls{use.d}[s] and \gls{define.d}[s] must take place in the
same \gls{block}.
%
This is modeled as
%
\begin{equation}
  \begin{array}{c}
    \mVar{sel}[m] \mImp \mVar{uplace}[p_1] = \mVar{uplace}[p_2] \\
    \forall m \in
      \mSetBuilder{m'}%
                  {%
                    m \in \mPhiMatchCompSet \hspace{-1pt},
                    \mSpans(m) = \mEmptySet
                  },
    \forall p_1, p_2 \in \mUses(m) \\
    \text{\st } p_1 < p_2,
  \end{array}
  \labelEquation{impl-cons-no-span-uses}
\end{equation}
%
\begin{equation}
  \begin{array}{c}
    \mVar{sel}[m]
    \mImp
    \mVar{dplace}[\mVar{alt}[p_1]] = \mVar{dplace}[\mVar{alt}[p_2]] \\
    \forall m \in
      \mSetBuilder{m'}%
                  {%
                    m \in \mPhiMatchCompSet \hspace{-1pt},
                    \mSpans(m) = \mEmptySet
                  },
    \forall p_1, p_2 \in \mDefines(m) \text{ \st } p_1 < p_2,
  \end{array}
  \labelEquation{impl-cons-no-span-defs}
\end{equation}
%
\begin{equation}
  \begin{array}{c}
    \mVar{sel}[m]
    \mImp
    \mVar{uplace}[p_1] = \mVar{dplace}[\mVar{alt}[p_2]] \\
    \forall m \in
      \mSetBuilder{m'}%
                  {%
                    m \in \mPhiMatchCompSet \hspace{-1pt},
                    \mSpans(m) = \mEmptySet
                  }, \\
    \forall p_1 \in \mUses(m) \setminus \mDefines(m),
    \forall p_2 \in \mDefines(m).
  \end{array}
  \labelEquation{impl-cons-no-span-use-defs}
\end{equation}

If a non-\gls{phi-match} \gls{span.b}[ning] some \glspl{block} is selected, then
all \gls{use.d}[s] of the input \glspl{datum} must occur in the same
\gls{block}.
%
This is modeled as
%
\begin{equation}
  \begin{array}{c}
    \mVar{sel}[m] \mImp \mVar{uplace}[p_1] = \mVar{uplace}[p_2] \\
    \forall m \in
      \mSetBuilder{m'}%
                  {%
                    m \in \mPhiMatchCompSet \hspace{-1pt},
                    \mSpans(m) \neq \mEmptySet
                  },
    \forall p_1, p_2 \in \mUses(m) \setminus \mDefines(m) \\
    \text{\st } p_1 < p_2.
  \end{array}
  \labelEquation{impl-cons-spanned-input}
\end{equation}


\subsection{Implied Data Locations}

Several \gls{implied.c} \glspl{constraint} can be due to
\refEquation{compatible-locations}.

If all non-\glspl{kill match} covering some \gls{operation} require some
non-\glsshort{state node} \gls{datum}~$d$ as input, then $d$ cannot be an
intermediate value nor be \gls{killed.d}.
%
Such \glspl{datum} is said to be \gls!{available.d}, meaning they cannot be
located in either $\mIntLocation$ or $\mKilledLocation$.
%
If the input can be one of several values (due to \glspl{alternative value}),
then at least one of those values must be made \gls{available.d}.
%
This is modeled as
%
\begin{equation}
  \begin{array}{c}
    \exists d \in S,
    \mVar{loc}[d] \notin \mSet{\mIntLocation, \mKilledLocation} \\
    \forall S \in \mPowerset{\mStateDataCompSet} \hspace{-2pt},
    \forall o \in
      \mSetBuilder*{o'}%
                   {%
                     \begin{array}{@{}l@{}}
                       o' \in \mOpSet \!,
                       m \in \mMatchSet[o'] \hspace{-1pt},
                       \exists p \in \mUses(m) \setminus \mDefines(m) \\
                       \text{\st }
                       \mDataSet[p] = S
                     \end{array}
                   } \!.
  \end{array}
  \labelEquation{impl-cons-used-data-must-be-available}
\end{equation}
%
where $\mStateDataCompSet$ denotes the set of \glspl{datum} without the
\glspl{state node}.

If all non-\glspl{kill match} defining a non-\glsshort{state node}
\gls{datum}~$d$ have $d$ as an \gls{exterior value}, then $d$ must be made
\gls{available.d}.
%
This is modeled as
%
\begin{equation}
  \begin{array}{c}
    \mVar{loc}[d] \notin \mSet{\mIntLocation, \mKilledLocation} \\
    \forall d \in
      \mSetBuilder*{d'}%
                   {%
                     \begin{array}{@{}l@{}}
                       d' \in \mStateDataCompSet,
                       m \in \mMatchSet[d'] \setminus \mKillMatchSet,
                       \exists p \in \mDefines(m) \text{ \st} \\
                       \mDataSet[p] = \mSet{d'}
                       \mAnd
                       \mIsExt(m, p)
                     \end{array}
                   } \!.
  \end{array}
  \labelEquation{impl-cons-exterior-data-must-be-available}
\end{equation}
%
where \mbox{$\mIsExt(m, p)$} denotes whether an \gls{operand}~$p$ in a
\gls{match}~$m$ represents an \gls{exterior value}.

We can always constrain the \glspl{location} of a non-\glsshort{state node}
\gls{datum}~$d$ to those \glspl{location} where the definers can put $d$.
%
The intuition here is to take the union of all those \glspl{location}, which is
modeled as
%
\begin{equation}
  \begin{array}{c}
    \mVar{loc}[d] = S \\
    \forall d \in \mStateDataCompSet,
    \forall S \in
      \mPowerset{\mLocationSet
      \, \cup \,
      \mSet{\mIntLocation, \mKilledLocation}} \text{ \st} \\
    S = \mSetBuilder{l}%
                    {%
                      m \in \mDataSet[d] \setminus \mKillMatchSet,
                      p \in \mDefines(m),
                      l \in \mStores(m, p)
                      \text{ \st }
                      d \in \mDataSet[p]
                    },
  \end{array}
  \labelEquation{impl-cons-locs-of-uses}
\end{equation}

Likewise, we can always constrain the \glspl{location} of a non-\glsshort{state
  node} \gls{datum}~$d$ to those \glspl{location} where the users can access $d$
(assuming there is always at least one \gls{match} making use of $d$).
%
This is modeled similarly as
%
\begin{equation}
  \begin{array}{c}
    \mVar{loc}[d] = S \\
    \forall d \in \mStateDataCompSet,
    \forall S \in
      \mPowerset{\mLocationSet
      \, \cup \,
      \mSet{\mIntLocation, \mKilledLocation}} \text{ \st} \\
    S = \mSetBuilder{l}%
                    {%
                      m \in \mKillMatchCompSet,
                      p \in \mUses(m),
                      l \in \mStores(m, p)
                      \text{ \st }
                      d \in \mDataSet[p]
                    }
    \mAnd
    S \neq \mEmptySet.
  \end{array}
  \labelEquation{impl-cons-locs-of-defs}
\end{equation}


\subsection{Implied Fall-Through}

Due to \refEquation{fall-through}, if for any two \glspl{block}~$b_1$ and~$b_2$
there exists a \gls{match} requiring $b_2$ to follow $b_1$ but none requiring
any other \gls{block} to follow $b_1$ nor $b_2$ to follow any other \gls{block},
then it is always safe to force $b_2$ to follow $b_1$.
%
This is modeled as
%
\begin{equation}
  \begin{array}{c}
    \mVar{succ}[b_1] = b_2 \\
    \forall b_1, b_2 \in \mBlockSet
    \text{ \st }
    \mSetBuilder{\mEntry(m)}{\mPair{m}{b_2} \in \mFallThroughSet} = \mSet{b_1}
    \mAnd \mbox{} \\
    \mSetBuilder{b}%
                {%
                  \mPair{m}{b} \in \mFallThroughSet
                  \text{ \st }
                  \mEntry(m) = \mSet{b_1}%
                } = \mSet{b_2}.
  \end{array}
  \labelEquation{impl-cons-fix-fall-throughs}
\end{equation}


\section{Symmetry and Dominance Breaking Constraints}
\labelSection{st-dom-breaking-constraints}

As explained in \refChapter{constraint-programming}, \glsshort{symmetry
  breaking.c} and \gls{dominance breaking.c} \glspl{constraint} are
\glspl{constraint} that remove \glspl{solution} from the \gls{search space} that
are either symmetric to one another or dominated by some other \gls{solution}.
%
Since this leads to a smaller \gls{search space}, the solving time is reduced.
%
In this section, we discuss such \glspl{constraint} that have been added to the
\glsshort{constraint model}.


\subsection{Location of State Nodes}

Since \glspl{datum} also includes the \glspl{state node}, this means a
$\mVar{loc}$~\gls{variable} will be allocated for each individual \gls{state
  node}.
%
However, since \glspl{state node} are abstract entities used only to capture
implicit dependencies between certain \gls{operation}, the assignment to these
\glspl{variable} has no actual bearing on the \gls{solution}, thus giving rise
to many symmetric \glspl{solution}.
%
Consequently, it makes sense to fix the \gls{location} for each \gls{state
  node}, which is modeled as
%
\begin{equation}
  \mVar{loc}[d] = \mIntLocation
  \mQuantSep
  \forall d \in \mStateDataSet,
  \labelEquation{dom-cons-locs-of-states}
\end{equation}
%
where \mbox{$\mStateDataSet \subseteq \mDataSet$} denotes the set of
\glspl{state node}.


\subsection{Operands of Non-Selected Matches}

The $\mVar{alt}$~\glspl{variable} of \glspl{match} that are not selected still
need to be assigned a value.
%
Since this assignment does not matter for the \gls{solution}, it gives rise to
many symmetric \glspl{solution}.
%
We therefore fix the $\mVar{alt}$ assignments in such cases, which is modeled as
%
\begin{equation}
  \begin{array}{c}
    \neg\mVar{sel}[m] \mImp \mVar{alt}[p] = \mMin(\mDataSet[\hspace{-1pt}p]) \\
    \forall m \in \mMatchSet,
    \forall p \in \mDefines(m) \cup \mUses(m).
  \end{array}
  \labelEquation{dom-cons-operands-of-non-selected-matches}
\end{equation}

The \gls{symmetry breaking.c} \gls{constraint} above also implies that if an
\gls{operand} representing input with multiple \glspl{datum} does not take its
minimum value, then the corresponding \gls{match} must be selected, which means
that the corresponding \gls{datum} must be in a usable \gls{location}.
%
This is modeled as
\begin{equation}
  \begin{array}{c}
    \mVar{alt}[p] \neq \mMin(\mDataSet[p])
    \mImp
    \mVar{alt}[p] \notin \mSet{\mIntLocation, \mKilledLocation} \\
    \forall m \in \mMatchSet,
    \forall p \in \mUses(m) \setminus \mDefines(m)
    \text{ \st }
    \mCard{\mDataSet[p]} > 1.
  \end{array}
  \labelEquation{impl-cons-input-operands-not-taking-min-value}
\end{equation}


\subsection{Interchangeable Data}

As described in \refSection{modeling-value-reuse}, \glspl{datum} in the \gls{UF
  graph} that are copies of the same value are \gls{copy-related.d} and
therefore interchangeable.
%
\begin{figure}
  \begin{minipage}[b]{58mm}%
    \centering%
    \subcaptionbox{%
                    UF graph, where the values \irVar{v}[1] and \irVar{v}[2]
                    constitute a chain of interchangeable data%
                    \labelFigure{interchangeable-data-example-annotated-graph}%
                  }%
                  [\textwidth]%
                  {%
                    \input{%
                      figures/solving-techniques/%
                      interchangeable-data-example-annotated-graph%
                    }%
                  }

    \vspace{\betweensubfigures}

    \subcaptionbox{%
                    Symmetries due to how data can be connected to operands%
                    \labelFigure{interchangeable-data-example-alt-solutions}%
                  }%
                  [\textwidth]%
                  {%
                    \input{%
                      figures/solving-techniques/%
                      interchangeable-data-example-alt-1%
                    }%
                    \hspace{8mm}%
                    \input{%
                      figures/solving-techniques/%
                      interchangeable-data-example-alt-2%
                    }%
                  }%
  \end{minipage}%
  \hfill%
  \begin{minipage}[b]{58mm}%
    \centering%
    \subcaptionbox{%
                    Symmetries due to how null-copy matches can be selected%
                    \labelFigure{interchangeable-data-example-null-solutions}%
                  }%
                  [\textwidth]%
                  {%
                    \input{%
                      figures/solving-techniques/%
                      interchangeable-data-example-null-1%
                    }%
                    \hspace{8mm}%
                    \input{%
                      figures/solving-techniques/%
                      interchangeable-data-example-null-2%
                    }%
                  }

    \vspace{\betweensubfigures}

    \subcaptionbox{%
                    Symmetries due to how kill matches can be selected%
                    \labelFigure{interchangeable-data-example-kill-solutions}%
                  }%
                  [\textwidth]%
                  {%
                    \input{%
                      figures/solving-techniques/%
                      interchangeable-data-example-kill-1%
                    }%
                    \hspace{8mm}%
                    \input{%
                      figures/solving-techniques/%
                      interchangeable-data-example-kill-2%
                    }%
                  }%
  \end{minipage}

  \caption[Example of interchangeable data]%
          {%
            Example of interchangeable data and how these give rise to
            symmetries%
          }
  \labelFigure{interchangeable-data-example}
\end{figure}
%
This is another source for symmetric \glspl{solution}, which is illustrated in
\refFigure{interchangeable-data-example}.

Assume a \gls{UF graph} containing two \gls{copy-related.d} values, \irVar{v}[1]
and~\irVar{v}[2], that may both be connected to two \glspl{operand}~$p_1$
and~$p_2$ (\refFigure{interchangeable-data-example-annotated-graph}).
%
We say that a set of values constitute a chain of \gls!{interchangeable.d} if
they can be swapped in a \gls{solution} without affecting the \gls{program}
semantics, which is the case if the values are all \gls{copy-related.d} and none
is both \gls{define.d}[d] and \gls{use.d}[d] by some \gls{match}.
%
In the above example, \irVar{v}[1] and~\irVar{v}[2] constitute such a chain and
can therefore be swapped for $p_1$ and~$p_2$, giving rise to unwanted symmetric
\glspl{solution} (\refFigure{interchangeable-data-example-alt-solutions}).
%
The intuition here is to forbid \glspl{solution} containing ``cross-over''
connections between the values in a chain and the $\mVar{alt}$~\glspl{variable}.
%
As a precaution, however, we will exclude \glspl{operand} used by
\glspl{phi-match} due to the \gls{definition edge} which may require such
cross-over connections.

If we assume that there exists a partial order $\leq$ for~$\mDataSet$, then we
can remove these symmetries by enforcing an order of the values assigned to the
$\mVar{alt}$~\glspl{variable}.
%
To this end, we use the \gls{value-precede-chain constraint} which was
introduced in \refChapter{constraint-programming} on
\refPageOfSection{cp-vpc}.
%
Hence, if $\mInterchDataSet$ denotes the set of chains of
\gls{interchangeable.d} \glspl{datum} and \mbox{$\mOperandSet[\mPhi] \subseteq
  \mOperandSet$} denotes the set of \glspl{operand} used by \glspl{phi-match},
then the \gls{dominance breaking.c} \gls{constraint} described above is modeled
as
%
\begin{equation}
  \begin{array}{c}
    \mValuePrecChain(c, \mVar{alt}[p_1], \ldots, \mVar{alt}[p_k]) \\
    \forall c \in \mInterchDataSet,
    \forall p_1, \ldots, p_k \in \mOperandSet \setminus \mOperandSet[\mPhi]
    \text{ \st }
    p_1 \neq \cdots \neq p_k
    \mAnd
    \forall_{\! i} \: \mDataSet[p_i] \! = c.
  \end{array}
  \labelEquation{dom-cons-interch-data-chains}
\end{equation}

Additional symmetries may appear due to \glspl{null-copy match}.
%
Returning to the previous example, if one of the two \glspl{copy node} need to
be covered using a \gls{copy match} derived from actual copy \gls{instruction},
then we are free to decide which.
%
Intuitively, we want to forbid \glspl{solution} where selected \glspl{null-copy
  match} ``appear to the left'' of a non-\gls{null-copy match}.
%
Let $\mInterchDataSet[\mCopy]$ denote the set of chains of \glspl{datum} that
can only be defined by \glspl{copy match}, \mbox{$\mNullCopyMatchSet \subseteq
  \mMatchSet$} denote the set of \glspl{null-copy match}, and
\mbox{$\mMatchSet[d] \subseteq \mMatchSet$} denote the set of \glspl{match} that
can define a \gls{datum}~$d$.
%
Using these definitions, this \gls{dominance breaking.c} \glspl{constraint} is
modeled as
%
\begin{equation}
  \begin{array}{c}
    \mIncreasing(\mVar{sel}[m_1], \ldots, \mVar{sel}[m_k]) \\
    \forall c \in \mInterchDataSet[\mCopy],
    \forall 1 \leq i < k,
    \exists m_i \in \mMatchSet[c[i]] \cap \mNullCopyMatchSet \hspace{-.8pt}.
  \end{array}
  \labelEquation{dom-cons-null-copy-match-selection}
\end{equation}
%
where
%
\begin{equation}
  \mIncreasing(\mVar{x}_1, \ldots, \mVar{x}_k)
  \equiv
  \hspace{-3pt}
  \mBigAnd_{1 \leq i < k}
  \hspace{-3pt}
  \mVar{x}_{i} \leq \mVar{x}_{i + 1}
  \labelEquation{increasing-function}
\end{equation}
%
It is assumed there exists exactly one \gls{null-copy match} to cover each
\gls{copy node}.

Similarly to \glspl{null-copy match}, symmetries can also arise due to
\glspl{kill match}.
%
In the previous example, for example, if only one of the two \glspl{copy node}
are needed, then we are free to decide which.
%
Intuitively, we want to forbid \glspl{solution} where \gls{killed.d}
\glspl{datum} ``appear to the right'' of non-\gls{killed.d} \glspl{datum}, which
is modeled as
%
\begin{equation}
  \begin{array}{c}
    \mIncreasing(\mVar{sel}[m_1], \ldots, \mVar{sel}[m_k]) \\
    \forall c \in \mInterchDataSet[\mCopy],
    \forall 1 \leq i < k,
    \exists m_i \in \mMatchSet[c[i]] \cap \mKillMatchSet,
  \end{array}
  \labelEquation{dom-cons-kill-match-selection}
\end{equation}
%
where $\mKillMatchSet$ denotes the set of \glspl{kill match}.
%
It is assumed there exists exactly one \gls{kill match} to cover each \gls{copy
  node}.


\section{Branching Strategies}
\labelSection{st-branching-strategies}

As explained in \refChapter{constraint-programming}, the \gls{branching
  strategy} decides how to explore the \gls{search space}.
%
In optimization problems, it is generally a good approach to try to find the
optimal \gls{solution} first as that will allow pruning of the still unexplored
parts of the \gls{search space}.
%
To this end, we first branch on the $\mVar{ocost}$~\glspl{variable}, selecting
the \gls{variable}~$v$ with the maximum \gls!{regret} -- that is, the
\gls{variable} with the largest difference between the two smallest values in
its \gls{domain} -- and the smallest value in the \gls{domain} of~$v$.
%
The intuition here is that, because our \gls{objective function} strives to
minimize the total cost, we wish to minimize the cost incurred per
\gls{operation}.
%
By selecting the \gls{variable} with the maximum \gls{regret}, we try to cover
the \glspl{operation} for which to cost of bad decisions is largest, and for
these we obviously try to cover these at least cost.
%
Note that this \gls{branching strategy} is only possible due to the refined
\gls{objective function} described earlier in this chapter.

Remaining decisions are left to the \gls{constraint solver}.
%
To improve \gls{match} selection, however, we make sure to arrange them in
order of increasing latency.
%
If there is a tie between two \glspl{match}, and either of them is a \gls{kill
  match}, then the \gls{kill match} comes first.
%
Otherwise, the \gls{match} covering more \glspl{operation} comes first (hence
mimicking the scheme of \gls{maximum munch}).
%
This is because the \gls{constraint solver} will most likely attempt to select
\glspl{match} in the order given to the \glsshort{constraint model}.
%
In such a setting, it is generally a good approach to first try a \gls{kill
  match}, which incurs no cost and encourages value reuse, and then the
\gls{match} which incurs the least cost.ma


\section{Presolving}
\labelSection{st-presolving}

As explained in \refChapter{constraint-programming}, \gls{presolving} is the
process of applying problem-specific algorithms to reduce the number of
\glspl{variable} or to shrink the \gls{variable} \glspl{domain} before
solving.\!%
%
\footnote{%
  In this sense, the bound tightening technique described in
  \refSection{st-cost-bounds} is a form of \gls{presolving}.%
}
%
In this dissertation, \gls{presolving} is used to remove \glspl{match} which can
be safely removed without compromising code quality, which directly translates
to fewer $\mVar{alt}$, $\mVar{sel}$, and $\mVar{uplace}$~\glspl{variable},
smaller $\mVar{dplace}$ and $\mVar{oplace}$~\glspl{domain}, as well as fewer
\glspl{constraint} that need to be managed by the \gls{constraint solver}.


\subsection{Dominated Matches}
\labelSection{st-pre-dom-matches}

If two \glspl{match} are equal in all respects except latency, then the
\gls{match} with longer latency is \gls!{dominate.m}[d] and can safely be
removed from the \gls{match set}.
%
A \gls{match}~$m_1$ is \gls{dominate.m}[d] if there exists another
\gls{match}~$m_2$ such that
%
\begin{itemize}
  \item $m_1$ has greater than or equal cost to $m_2$,
  \item both cover the same \glspl{operation},
  \item both have the same \glspl{entry block} (if any),
  \item both \gls{span.b} the same \glspl{block} (if any),
  \item both have the same \glspl{definition edge} (if any),
  \item $m_1$ has at least as strong \gls{location} requirements on its
    \glspl{datum} as $m_2$ -- that is, $\forall p_1 \in \mUses(m_1) \cup
    \mDefines(m_1), \exists p_2 \in \mUses(m_2) \cup \mDefines(m_2) \text{ \st }
    \mDataSet[p_1] \subseteq \mDataSet[p_2] \mAnd \mStores(m_1, p_1) \subseteq
    \mStores(m_2, p_2)$ -- and
  \item both apply the same auxiliary \glspl{constraint} (if any).
\end{itemize}
%
As a precaution, we assume that \glspl{null match}, \glspl{phi-match}, and
\glspl{match} with \gls{fall-through} conditions can never be
\gls{dominate.m}[d].

The method above can be generalized to letting combinations of \glspl{match} to
be jointly \gls{dominate.m}[d] by another \gls{match}.
%
Intuitively, if the combination of \glspl{match} can be selected, then the
\gls{solution} can always be improved by replacing them with the single
\gls{match}.
%
The idea is to combine the \glspl{match} into a single \gls{match}~$m$, and then
check whether the above conditions for \glsshort{dominate.m}[ance] apply with
the additional check that none of intermediate values of $m$ are used by other
\glspl{match}.
%
\begin{figure}
  \centering%
  \input{figures/solving-techniques/dominated-matches-example}

  \caption[Example of dominated matches]%
          {%
            Example where matches~$m_1$ and $m_2$ are jointly dominated by
            match~$m_3$ and can therefore safely be removed (provided that no
            other match uses value~\irVar{v}[3]).
            %
            The table contains the location restrictions enforced by each match%
          }
  \labelFigure{dominated-matches-example}
\end{figure}
%
An example is shown in \refFigure{dominated-matches-example}.


\subsection{Illegal Matches}

Depending on the \gls{instruction set}, the \gls{match set} may contain
\glspl{match} that will, for one reason or another, never participate in any
\gls{solution}.
%
Such \glspl{match} are said to be \gls!{illegal.m}.

One set of \gls{illegal.m} \glspl{match} are those which would leave some
\gls{operation} uncoverable if selected.
%
\begin{figure}
  \centering%
  \input{figures/solving-techniques/uncovered-operations-example}

  \caption[Example of an illegal match]%
          {%
            Example of an illegal match, where selecting match~$m_1$ would leave
            operation~$o_2$ uncovered%
          }
  \labelFigure{uncovered-operations-example}
\end{figure}
%
In \refFigure{uncovered-operations-example}, for example, selecting
\gls{match}~$m_1$ would leave \gls{operation}~$o_2$ uncovered since it can only
be covered by match~$m_2$, but selection of this \gls{match} is inhibited if
$m_1$ is selected.
%
Hence this set of \gls{illegal.m} \glspl{match} is computed as
%
\begin{equation}
  \mSetBuilder{m}%
              {
                \forall m \in \mMatchSet,
                \forall o_1, o_2 \in \mOpSet
                \text{ \st }
                \mMatchSet[o_1] \subset \mMatchSet[o_2]
                \mAnd
                m \in \mMatchSet[o_2]
              }.
  \labelEquation{illegal-matches-uncovered-ops}
\end{equation}

Likewise, a \gls{match} is \gls{illegal.m} if selecting it would
leave some \gls{datum} un\gls{define.d}[d].
%
With similar reasoning, this set of \gls{illegal.m} \glspl{match} is computed as
%
\begin{equation}
  \mSetBuilder{m}%
              {
                \forall m \in \mMatchSet,
                \forall d_1, d_2 \in \mDataSet
                \text{ \st }
                \mMatchSet[d_1] \subset \mMatchSet[d_2]
                \mAnd
                m \in \mMatchSet[d_2]
              }.
  \labelEquation{illegal-matches-undefined-data}
\end{equation}

If a \gls{kill match}~$m$ \gls{define.d}[s] a \gls{datum}~$d$ and every other
\gls{match} using $d$ has no \glsshort{alternative value}[s] but~$d$, then $m$
is \gls{illegal.m} as $d$ must be \gls{define.d}[d] by a non-\gls{kill match}.
%
This set of \gls{illegal.m} \glspl{match} is computed as
%
\begin{equation}
  \mSetBuilder*{m_1}%
               {%
                 \begin{array}{@{}l@{}}
                   m_1 \in \mKillMatchSet,
                   p_1 \in \mDefines(m_1),
                   d \in \mDataSet[p_1], \\
                   m_2 \in \mKillMatchCompSet,
                   p_2 \in \mUses(m_2) \text{ \st }
                   d \in \mDataSet[p_2] \mImp \mDataSet[p_2] = \mSet{d}
                 \end{array}
               }\!.
  \labelEquation{illegal-matches-kills}
\end{equation}

If a \gls{match}~$m$ is not a \gls{kill match} and \gls{define.d}[s] a
\gls{datum}~$d$ in a \gls{location} that cannot be accessed by any of the
\glspl{match} making \gls{use.d} of $d$, then $m$ can never be selected and is
thus \gls{illegal.m}.
%
This set of \gls{illegal.m} \glspl{match} is computed as
%
\begin{equation}
  \mSetBuilder*{m}%
               {%
                 \begin{array}{@{}l@{}}
                   m \in \mKillMatchCompSet,
                   p \in \mDefines(m),
                   d \in \mDataSet[p] \hspace{-1pt} \text{ \st} \\
                   \mIsExt(m, p)
                   \mAnd
                   \mCupUseLocsOf(d) \neq \mEmptySet
                   \mAnd \mbox{} \\
                   \mStores(m, p) \cap \mCupUseLocsOf(d) = \mEmptySet
                 \end{array}
               }\!,
  \labelEquation{illegal-matches-def-locs}
\end{equation}
%
where
%
\begin{equation}
  \mCupUseLocsOf(d)
  \equiv
  \hspace{-2.5em}
  \bigcup_{\substack{%
                   m \, \in \, \mMatchSet[d] \setminus \mKillMatchSet, \\
                   p \, \in \, \mUses(m)
                   \text{ \st } d \, \in \, \mDataSet[p]
                  }}
    \hspace{-2.5em}
    \mStores(m, p).
  \labelEquation{cup-use-locs-of-function}
\end{equation}
%
Note that if \mbox{$\mCupUseLocsOf(d) = \mEmptySet$} holds then the
\glspl{match} \glsshort{use.d}[ing] \gls{datum}~$d$ have themselves conflicting
\gls{location} requirements, and in such cases we cannot infer whether a
\gls{match} \glsshort{define.d}[ing] $d$ is \gls{illegal.m}.

Similarly, if a \gls{match}~$m$ is not a \gls{kill match} and \gls{use.d}[s] a
\gls{datum}~$d$ from a \gls{location} that cannot be written to by any of the
\glspl{match} \glsshort{define.d}[ing]~$d$, then $m$ can never be selected and
is thus \gls{illegal.m}.
%
This set of \gls{illegal.m} \glspl{match} is computed as
%
\begin{equation}
  \mSetBuilder*{m}%
               {%
                 \begin{array}{@{}l@{}}
                   m \in \mKillMatchCompSet,
                   p \in \mUses(m) \setminus \mDefines(m),
                   d \in \mDataSet[p] \hspace{-1pt} \text{ \st} \\
                   \mCupDefLocsOf(d) \neq \mEmptySet
                   \mAnd
                   \mStores(m, p) \cap \mCupDefLocsOf(d) = \mEmptySet
                 \end{array}
               } \!,
  \labelEquation{illegal-matches-use-locs}
\end{equation}
%
where
%
\begin{equation}
  \mCupDefLocsOf(d)
  \equiv
  \hspace{-2.5em}
  \bigcup_{\substack{%
                   m \, \in \, \mMatchSet[d] \setminus \mKillMatchSet, \\
                   p \, \in \, \mDefines(m)
                   \text{ \st } d \, \in \, \mDataSet[p]
                  }}
    \hspace{-2.5em}
    \mStores(m, p).
  \labelEquation{cup-def-locs-of-function}
\end{equation}


\subsection{Redundant Matches}

In certain circumstances a \gls{match} is redundant, meaning it can be
safely removed without compromising code quality.

One such case is that for each \gls{copy node}~$c$, if there exists a
\gls{null-copy match} to cover $c$, then the \gls{kill match} covering $c$ is
redundant since it is always safe to select the \gls{null-copy match} over the
\gls{kill match}.
%
Consequently, all \glspl{kill match} covering \glspl{copy node} that take a
non-constant value as input -- which cannot be covered using a \gls{null-copy
  match} -- can be removed from the \gls{match set}.
%
This makes sense as the \glspl{kill match} were added to the \gls{match set} as
a consequence of \gls{alternative value}, which was introduced to handle cases
where loaded constants could be reused among \glspl{match}.
%
Hence this set of redundant \glspl{match} is computed as
%
\begin{equation}
  \mSetBuilder{m}%
              {%
                m \in \mKillMatchSet,
                o \in \mCovers(m)
                \text{ \st }
                \mMatchSet[o] \cap \mNullCopyMatchSet \neq \mEmptySet
              }.
  \labelEquation{redun-kills}
\end{equation}

Another case concerns \glspl{match} that cover a \gls{copy node} and are not
\glspl{null match}.
%
For each copy chain \mbox{$\mEdge{v_1}{\mEdge{c}{v_2}}$}, where $c$ is a
\gls{copy node} and $v_1$ and $v_2$ are \glspl{value node}, if every \gls{match}
\glsshort{define.d}[ing] $v_1$ writes the value to a \gls{location} that can be
used by all \glspl{match} \glsshort{use.d}[ing]~$v_2$, then all non-\glspl{null
  match} covering $c$ are redundant since a \gls{null-copy match} can always be
selected.
%
We exclude, however, \glspl{copy node} that take a constant value as input since
such \glspl{node} can never be covered by a \gls{null-copy match}.
%
We also exclude \glspl{copy node} whose \gls{define.d}[d] \gls{datum} is
\gls{use.d}[d] by some \gls{phi-match} since an actual copy may be needed to
satisfy \refEquation{phi-match-locations}.
%
Hence this set of redundant \glspl{match} is computed as
%
\begin{equation}
  \mSetBuilder*{m}%
               {%
                 \begin{array}{@{}l@{}}
                   m \in \mCopyMatchSet \setminus \mNullMatchSet,
                   d_1 \in \mUses(m),
                   d_2 \in \mDefines(m) \\
                   \text{\st }
                   \mDataSet[d_1] \cap \mPhiMatchSet = \mEmptySet
                   \mAnd
                   \mDataSet[d_2] \cap \mPhiMatchSet = \mEmptySet
                   \mAnd
                   d_1 \notin \mConstDataSet \\
                   \mbox{}\hspace{-2pt} \mAnd
                   \mCapUseLocsOf(d_1) \cap \mCapDefLocsOf(d_2) \neq \mEmptySet
                 \end{array}
               }\!,
  \labelEquation{redun-non-null-copy-matches}
\end{equation}
%
where \mbox{$\mCopyMatchSet \subseteq \mMatchSet$} denotes the set of
\glspl{copy match}, \mbox{$\mConstDataSet \subseteq \mDataSet$} denotes the set
of \glspl{datum} representing constant values, and \mbox{$\mCapUseLocsOf(d)
  \subseteq \mLocationSet$} and \mbox{$\mCapDefLocsOf(d) \subseteq
  \mLocationSet$} denote the intersection of all \glspl{location} for all
\gls{match} where a \gls{datum}~$d$ is \gls{use.d}[d] respectively
\gls{define.d}[d].
%
As can be deduced from their name, $\mCapUseLocsOf$ and $\mCapDefLocsOf$ are
defined similarly to $\mCupUseLocsOf$ and $\mCupDefLocsOf$ (see
\refEquationList{cup-use-locs-of-function, cup-def-locs-of-function}), with the
exception that \glspl{location} known to violate
\refEquation{phi-match-locations} are removed from these sets.


\subsection{Canonical Locations}
\labelSection{st-canonical-locations}

For most architectures, its \glspl{instruction} read from and write to the same
set of \glspl{register}.
%
This gives rise to many symmetric \glspl{solution} as the exact \gls{location}
assigned to a value often does not matter.
%
We can remove these symmetries by removing \glspl{location} which are considered
symmetric to one another.

The idea is to select a \gls{location} as a representative for each distinct
intersection made by the storage requirements.
%
\begin{figure}
  \centering%
  \input{figures/solving-techniques/canonical-locations-example}

  \caption[Example of canonical locations]%
          {%
            Example of canonical locations for a location set with ten
            registers%
          }
  \labelFigure{canonical-locations-example}
\end{figure}
%
See for example \refFigure{canonical-locations-example}.
%
For sake of discussion, each storage requirment has been labeled with a tag.
%
Given the \gls{location set} and storage requirements shown in the figure, they
give rise to five intersections with respect to the locations:
%
\def\mReg#1{\text{\instrFont r$_{\text{#1}}$}}%
%
\mbox{$\mSet{\mReg{1}, \ldots, \mReg{4}}$} due to tags~1 and~2,
%
\mbox{$\mSet{\mReg{5}}$} due to tag~1,
%
$\mSet{\mReg{6}}$ due to tag~1 and~3,
%
\mbox{$\mSet{\mReg{7}, \mReg{8}}$} due to tags~3 and~4, and
%
$\mSet{\mReg{9}}$ due to tag~4.
%
From each of these intersections we select a representative, and the union of
these representative \glspl{location} constitute the set of \gls!{canonical.l}[
  \glspl{location}].
%
An algorithm for computing this set based on the intuition above is shown in
\refAlgorithm{canonical-locs-algorithm}.
%
\begin{algorithm}[t]
  \DeclFunction{CanonicalizeLocs}{location set $L$, match set~$M$}%
  {%
    $T$ \Assign vector with $\mCard{L}$ elements initialized to $\mEmptySet$\;
    $t$ \Assign $1$\;
    \For(\tcp*[f]{assign tags}){$m \in M$}{%
      \For{$p \in \mUses(m) \cup \mDefines(m)$}{%
        \For{$l \in \mStores(m, p)$}{%
          $T[l]$ \Assign $T[l] \cup \mSet{t}$\;
        }
        $t$ \Assign $t$ $+$ $1$\;
      }
    }
    $G$ \Assign $\mSetBuilder{T[l]}{l \in L}$\tcp*{find all groups of tags}
    $L_{\mathsc{c}}$ \Assign $\mEmptySet$\;
    \For{$g \in G$}{%
      $l_{\mathsc{c}}$ \Assign
      $\mMin(\mSetBuilder{l}{l \in L \text{ \st } T[l] = g})$
      \tcp*{find representative for this group of tags}
      $L_{\mathsc{c}}$ \Assign
      $L_{\mathsc{c}} \cup \mSet{l_{\mathsc{c}}}$\;
    }
    \Return $L_{\mathsc{c}}$\;
  }

  \caption[Algorithm for computing the set of canonical locations]%
          {%
            Computes the canonical locations from a given location set.
            %
            If location restrictions for some data are already enforced by
            the function, then these are also tagged and processed accordingly%
          }
  \labelAlgorithm{canonical-locs-algorithm}
\end{algorithm}

Once computed, we substitute all locations appearing in the storage requirements
with their \gls{canonical.l} representative and then replace the original
\gls{location set} with the \gls{canonical.l} set, thus shrinking the
\glspl{domain} of the $\mVar{loc}$~\glspl{variable}.


\section{Experimental Evaluation}
\labelSection{st-experimental-evaluation}

Excluding the refinement described in \refSection{st-refining-define-before-use
  constraint}, whose naive equivalence cannot be implemented on this
experimental setup and therefore not be evaluated, we now evaluate the impact of
each solving technique introduced in this chapter.

When filtering, we remove all \glspl{function} that have less than less than
\num{50}~\gls{LLVM} \gls{IR} \glspl{instruction} -- anything smaller will most
likely not show the impact of the given solving technique -- and greater than
\num{150}~\glspl{instruction} -- anything larger will lead to unreasonably long
experiment runtimes.
%
This leaves a pool of \num{284}~\glspl{function}, on which we then perform
sampling.

When solving, we apply a time limit of \SI{600}{\s} to the \gls{constraint
  solver}.
%
This is to curb experiment runtimes.
%
For any given \gls{function}, the last \gls{solution} found is considered
optimal if and only if the \glsshort{constraint solver} has finished its
execution within the time limit.

When using an upper bound for a given \gls{function}, we take the cost for the
\gls{solution} computed by \mbox{\gls{LLVM} 3.8}.


\subsection{Objective Function Refinements and Cost Bounding}

\def\modelA{\textsc{i}}
\def\modelB{\textsc{ii}}

First, we evaluate the different methods for computing the cost matrix by
comparing the solving times exhibited by two versions of the \gls{constraint
  model}: one based on the \gls{multiply-then-divide method}, and another based
on \glspl{divide-then-multiply method}.
%
We refer to these \glsplshort{constraint model} as \modelA{} and \modelB,
respectively.
%
No hypothesis is attempted on which \glsshort{constraint model} is better.

\def\modelC{\textsc{iii}}
\def\modelD{\textsc{iv}}
\def\modelE{\textsc{v}}
\def\modelF{\textsc{vi}}
\def\modelG{\textsc{vii}}

Second, based on the results for the experiment above we then use the superior
method to evaluate the impact of the \gls{objective function} refinement and
cost bounding by comparing the number of \glspl{function} that can be solved
optimally by five versions of the \gls{constraint model}: two based on the naive
implementation of the \gls{objective function}
(\refEquation{naive-objective-function}), one with cost bounds and one without;
and three based on the refined implementation
(\refEquationList{omatch-oplace-ocost-connection, total-cost}), one without cost
bounds, one with only the upper bound, and one with both lower and upper bound.
%
We refer to these \glsplshort{constraint model} as \modelC, \modelD, \modelE,
\modelF, and \modelG, respectively.
%
Since the refined \gls{objective function} enables tighter bounds to be derived
for the \gls{cost variable}, we expect \glsplshort{constraint model}~\modelE,
\modelF, and~\modelG{} to find a greater number of optimal \glspl{solution}
compared with \glsplshort{constraint model}~\modelC{} and~\modelD.
%
Due to further tightening of the cost bounds, we expect \glsshort{constraint
  model}~\modelD{} to outperform \glsshort{constraint model}~\modelC{},
\glsplshort{constraint model}~\modelF{} and~\modelG{} to outperform
\glsshort{constraint model}~\modelE, and \glsshort{constraint model}~\modelG{}
to outperform \glsshort{constraint model}~\modelF.

\def\modelH{\textsc{viii}}
\def\modelI{\textsc{ix}}

Third, we evaluate the impact on code quality by comparing the cost of the best
\gls{solution} found within the time limit by two versions of the
\gls{constraint model}: one based on the naive implementation with upper bound,
and another based on the refined implementation with upper bound.
%
We refer to these \glsplshort{constraint model} as \modelH{} and \modelI,
respectively.
%
For the same reason as with the second experiment, we expect the
\glspl{solution} produced by \glsshort{constraint model}~\modelI{} to be of
significantly better quality compared with those produced by
\glsshort{constraint model}~\modelH.

\input{\expDir/new-op-cost-fun-vs-old-pre+solving-time-speedup.stats}

\begin{figure}
  \centering%
  \maxsizebox{\textwidth}{!}{%
    \trimBarchartPlot{%
      \input{\expDir/new-op-cost-fun-vs-old-pre+solving-time-speedup.plot}%
    }%
  }

  \caption[%
            Plot for evaluating the operation cost function's impact on
            solving time%
          ]%
          {%
            Normalized solving times (including the time for presolving) for two
            constraint models using different operation cost functions: one
            based on the multiply-then-divide method (baseline), and another
            based on the divide-then-multiply method (subject).
            %
            GMI:~\printGMI{%
              \NewOpCostFunVsOldPrePlusSolvingTimeSpeedupPrePlusSolvingTimeRegularSpeedupGmean%
            },
            CI~\printGMICI{%
              \NewOpCostFunVsOldPrePlusSolvingTimeSpeedupPrePlusSolvingTimeRegularSpeedupCiMin%
            }{%
              \NewOpCostFunVsOldPrePlusSolvingTimeSpeedupPrePlusSolvingTimeRegularSpeedupCiMax%
            }%
          }
  \labelFigure{new-op-cost-fun-vs-old-plot}
\end{figure}

\RefFigure{new-op-cost-fun-vs-old-plot} shows the normalized solving times
(including \gls{presolving} time) for the two \glspl{constraint model}
described above in the first experiment, with \glsshort{constraint
  model}~\modelA{} as \gls{baseline} and \glsshort{constraint model}~\modelB{}
as \gls{subject}.
%
The solving times range from
\printMinSolvingTime{
  \NewOpCostFunVsOldPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMin,
  \NewOpCostFunVsOldPrePlusSolvingTimeSpeedupBaselinePrePlusSolvingTimeAvgMin
} to
\printMaxSolvingTime{
  \NewOpCostFunVsOldPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMax,
  \NewOpCostFunVsOldPrePlusSolvingTimeSpeedupBaselinePrePlusSolvingTimeAvgMax
}%
%
\footnote{%
  The solving time given here greater than the time limit because it also
  includes the \gls{presolving} time while the time limit is only applied on the
  \gls{constraint solver}.%
}
%
with a maximum coefficient of variation of
\numMaxOf{
  \NewOpCostFunVsOldPrePlusSolvingTimeSpeedupPrePlusSolvingTimeCvMax,
  \NewOpCostFunVsOldPrePlusSolvingTimeSpeedupBaselinePrePlusSolvingTimeCvMax
}.
%
As the \gls{GMI} is \printGMI{%
  \NewOpCostFunVsOldPrePlusSolvingTimeSpeedupPrePlusSolvingTimeRegularSpeedupGmean%
} with \gls{CI}~\printGMICI{%
  \NewOpCostFunVsOldPrePlusSolvingTimeSpeedupPrePlusSolvingTimeRegularSpeedupCiMin%
}{%
  \NewOpCostFunVsOldPrePlusSolvingTimeSpeedupPrePlusSolvingTimeRegularSpeedupCiMax%
}, we see that \glsshort{constraint model}~\modelB{} results in significantly
shorter solving times than \glsshort{constraint model}~\modelA.
%
In one case (checksum{\codeFont build\_ycc\_rgb\_t}), for example, the solving
time is two orders of magnitude shorter, and in three cases ({\codeFont
  alloc\_save\_spac}, {\codeFont gpk\_open}, and {\codeFont trueRandAccum}) the
solving time is more than one order of magnitude shorter.
%
Although \glsshort{constraint model}~\modelA{} outperforms \glsshort{constraint
  model}~\modelB{} in three other cases ({\codeFont alloc\_name\_is\_s},
{\codeFont debug\_dump\_byte}, and {\codeFont write\_file\_trai}) -- this
observation also underscores the fact that seemingly trivial changes to a
\gls{constraint model} may have considerable impact on solving time -- the loss
in performance for these \glspl{function} is overshadowed by the gain in
performance for other \glspl{function}.
%
Hence we conclude that, when implementing the refined \gls{objective function},
the \gls{divide-then-multiply method} is a better design choice over
\gls{multiply-then-divide method}.

\begin{figure}
  \def\modelFont#1{{\small\figureFont#1}}
  \def\modelA{\modelFont{a}}
  \def\modelB{\modelFont{b}}
  \def\modelC{\modelFont{c}}
  \def\modelD{\modelFont{d}}
  \def\modelE{\modelFont{e}}

  \renewcommand{\plotPercentageFont}{\large}
  \renewcommand{\plotSecondsFont}{\large}
  \centering%
  \maxsizebox{.6\textwidth}{!}{%
    \trimLinechartPlot{%
      \input{\expDir/obj-fun-refined-vs-naive-opt-proofs-over-time.plot}%
    }%
  }

  \caption[%
            Plot for evaluating the different objective functions' impact on
            finding optimal solutions%
          ]%
          {%
            Percentage of optimal solutions found over time for five constraint
            models: one using the naive objective function without
            bounds~(\modelA), one using the naive function with upper
            bound~(\modelB), one using the refined objective function without
            bounds~(\modelC), one using the refined function with upper
            bound~(\modelD), and another using the refined function with both
            lower and upper bound~(\modelE)%
          }
  \labelFigure{obj-fun-refined-vs-naive-opt-proofs-over-time-plot}
\end{figure}

\input{\expDir/obj-fun-refined-vs-naive-opt-proofs-over-time.stats}

\RefFigure{obj-fun-refined-vs-naive-opt-proofs-over-time-plot} shows the
percentage of optimal \glspl{solution} found over time for the five
\glspl{constraint model} described above in the second experiment.
%
The solving times range from
\printMinSolvingTime{
  \ObjFunRefinedVsNaiveOptProofsOverTimeNaiveNoBoundsSolvingTimeAvgMin,
  \ObjFunRefinedVsNaiveOptProofsOverTimeNaiveWUbSolvingTimeAvgMin,
  \ObjFunRefinedVsNaiveOptProofsOverTimeRefinedNoBoundsSolvingTimeAvgMin,
  \ObjFunRefinedVsNaiveOptProofsOverTimeRefinedWUbSolvingTimeAvgMin,
  \ObjFunRefinedVsNaiveOptProofsOverTimeRefinedWLbUbSolvingTimeAvgMin
} to
\printMaxSolvingTime{
  \ObjFunRefinedVsNaiveOptProofsOverTimeNaiveNoBoundsSolvingTimeAvgMax,
  \ObjFunRefinedVsNaiveOptProofsOverTimeNaiveWUbSolvingTimeAvgMax,
  \ObjFunRefinedVsNaiveOptProofsOverTimeRefinedNoBoundsSolvingTimeAvgMax,
  \ObjFunRefinedVsNaiveOptProofsOverTimeRefinedWUbSolvingTimeAvgMax,
  \ObjFunRefinedVsNaiveOptProofsOverTimeRefinedWLbUbSolvingTimeAvgMax
} with a maximum coefficient of variation of
\numMaxOf{
  \ObjFunRefinedVsNaiveOptProofsOverTimeNaiveNoBoundsSolvingTimeCvMax,
  \ObjFunRefinedVsNaiveOptProofsOverTimeNaiveWUbSolvingTimeCvMax,
  \ObjFunRefinedVsNaiveOptProofsOverTimeRefinedNoBoundsSolvingTimeCvMax,
  \ObjFunRefinedVsNaiveOptProofsOverTimeRefinedWUbSolvingTimeCvMax,
  \ObjFunRefinedVsNaiveOptProofsOverTimeRefinedWLbUbSolvingTimeCvMax
}.
%
We see that \glsplshort{constraint model}~\modelE, \modelF, and~\modelG{}
clearly outperforms \glsplshort{constraint model}~\modelC{} and~\modelD.
%
We see also that applying an upper bound has a positive effect for both
\gls{objective function}, although the benefit is greater for the refined
\gls{objective function}.
%
This gain is due to the fact that, for some \glspl{function}, the \gls{solution}
computed by \gls{LLVM} is already optimal with respect to the
\glsshort{constraint model}.
%
Consequently, the \glsshort{constraint solver} need only prove that there exist
no better \gls{solution} instead of exploring the entire \gls{search space}.
%
Applying a lower bound, however, does not appear to be equally beneficial; in
fact, \glsshort{constraint model}~\modelG{} fails to find the optimal
\gls{solution} for one \gls{function} whereas \glsshort{constraint
  model}~\modelF{} manages to find the optimal \gls{solution} for all
\glspl{function}.
%
A possible explanation is that the lower bound computed by the relaxed
\gls{constraint model} is too weak to be lead to any \gls{propagation} and
instead only interferes with \gls{Chuffed}'s \gls{lazy clause learning} engine.
%
Hence we conclude that, when implementing the \gls{objective function}, in terms
of solving time the refined version coupled with an upper cost bound is a better
design choice over naive version, and that this decision is crucial for
scalability.

\input{\expDir/obj-fun-refined-vs-naive-cycles-speedup.stats}

\begin{figure}
  \centering%
  \maxsizebox{\textwidth}{!}{%
    \trimBarchartPlot{%
      \input{\expDir/obj-fun-refined-vs-naive-cycles-speedup.plot}%
    }%
  }

  \caption[%
            Plot for evaluating the different objective functions' impact on
            code quality%
          ]%
          {%
            Normalized optimal solution costs for two constraint models: one
            implementing the naive objective function (baseline), and another
            implementing the refined objective function (subject).
            %
            GMI:~\printGMI{%
              \ObjFunRefinedVsNaiveCyclesSpeedupRefinedVsNaiveWUbCyclesRegularSpeedupGmean%
            },
            CI~\printGMICI{%
              \ObjFunRefinedVsNaiveCyclesSpeedupRefinedVsNaiveWUbCyclesRegularSpeedupCiMin%
            }{%
              \ObjFunRefinedVsNaiveCyclesSpeedupRefinedVsNaiveWUbCyclesRegularSpeedupCiMax%
            }.
            %
            Both models uses an upper bound computed by LLVM.
            %
            \Glspl{function} marked with \barValueNoBaselineSolution{} are
            those for which the naive objective function fails to produce any
            solution, and \glspl{function} marked with \barValueNoSolution{} are
            those where the solution produced by \gls{LLVM} is already optimal
            \wrt the model.
            %
            \Glspl{function} whose bars are marked with two dots are those
            for which the \gls{subject} fails to find the optimal solution%
          }
  \labelFigure{obj-fun-refined-vs-naive-cycles-speedup-plot}
\end{figure}

\RefFigure{obj-fun-refined-vs-naive-cycles-speedup-plot} shows the normalized
\gls{solution} costs for the two \glspl{constraint model} described above in the
third experiment, with \glsshort{constraint model}~\modelH{} as \gls{baseline}
and \glsshort{constraint model}~\modelI{} as \gls{subject}.
%
The costs range from
\printMinCycles{
  \ObjFunRefinedVsNaiveCyclesSpeedupNaiveWUbCyclesAvgMin,
  \ObjFunRefinedVsNaiveCyclesSpeedupRefinedWUbCyclesAvgMin
} to
\printMaxCycles{
  \ObjFunRefinedVsNaiveCyclesSpeedupNaiveWUbCyclesAvgMax,
  \ObjFunRefinedVsNaiveCyclesSpeedupRefinedWUbCyclesAvgMax
} with a maximum coefficient of variation of
\numMaxOf{
  \ObjFunRefinedVsNaiveCyclesSpeedupNaiveWUbCyclesCvMax,
  \ObjFunRefinedVsNaiveCyclesSpeedupRefinedWUbCyclesCvMax
}.
%
As the \gls{GMI} is \printGMI{%
  \ObjFunRefinedVsNaiveCyclesSpeedupRefinedVsNaiveWUbCyclesRegularSpeedupGmean%
} with \gls{CI}~\printGMICI{%
  \ObjFunRefinedVsNaiveCyclesSpeedupRefinedVsNaiveWUbCyclesRegularSpeedupCiMin%
}{%
  \ObjFunRefinedVsNaiveCyclesSpeedupRefinedVsNaiveWUbCyclesRegularSpeedupCiMax%
}, we observe that \glsplshort{constraint model} based on the refined
\gls{objective function} yield better code quality than those based on the naive
\gls{objective function}.
%
This also shows that there is a significant difference between the optimal
\gls{solution} and a \gls{solution} found using inferior solving techniques.
%
Hence we conclude that, when implementing the \gls{objective function}, in terms
of code quality the refined version coupled with an upper cost bound is a better
design choice over naive version.


\subsection{Implied Constraints}
\labelSection{st-exp-evaluation-implied-constraints}

\NewDocumentCommand{\insertSpeedupPlot}{mm}{%
  \maxsizebox{#2\textwidth}{!}{%
    \trimBarchartPlot{\input{#1}}%
  }%
}

\NewDocumentCommand{\insertSolvTechSpeedupPlot}{O{disable}mm}{%
  \insertSpeedupPlot{%
    \expDir/solv-tech-#1-#2-pre+solving-time-speedup.plot%
  }{#3}
}

\newsavebox{\solvTechPlot}
\newlength{\solvTechPlotW}
\newlength{\solvTechSubfigW}
\NewDocumentCommand{\mkSolvTechSubfigure}{O{disable}moom}{%
  \IfStrEq{#1}{disable}{\def\argUC{Disable}}{}%
  \IfStrEq{#1}{enable}{\def\argUC{Enable}}{}%
  \IfStrEq{#1}{best}{\def\argUC{Best}}{}%
  \edef\statsPartName{%
    SolvTech%
    \argUC%
    #5%
    PrePlusSolvingTimeSpeedupPrePlusSolvingTimeRegularSpeedup%
  }%
  \edef\gmi{\csname\statsPartName Gmean\endcsname}%
  \edef\cimin{\csname\statsPartName CiMin\endcsname}%
  \edef\cimax{\csname\statsPartName CiMax\endcsname}%
  \savebox{\solvTechPlot}{\insertSolvTechSpeedupPlot[#1]{#2}{.49}}%
  \settowidth{\solvTechPlotW}{\usebox{\solvTechPlot}}%
  \IfValueTF{#4}
            {%
              \pgfmathsetlength{\solvTechSubfigW}%
                               {max(\solvTechPlotW, #4)}%
            }%
            {\setlength{\solvTechSubfigW}{\solvTechPlotW}}%
  \begin{minipage}[t]{\solvTechSubfigW}%
    \captionsetup[sub]{justification=centerlast}%
    \IfValueTF{#4}{\captionsetup[sub]{width=#4}}{}%
    \centering%
    \usebox{\solvTechPlot}
    \subcaption{%
                 \IfValueTF{#3}{#3}{\refEquation{#2}}.
                 %
                 GMI:~\printGMI{\gmi},
                 CI:~\printGMICI{\cimin}{\cimax}%
                 \labelFigure{solv-tech-#1-#2}%
                }%
    \vspace{0pt}% Force \belowcaptionspace to be applied
  \end{minipage}%
}

\NewDocumentCommand{\includeSolvTechStats}{O{disable}m}{%
  \input{\expDir/solv-tech-#1-#2-pre+solving-time-speedup.stats}%
}

\def\modelA{\textsc{i}}
\def\modelB{\textsc{ii}}

First, we evaluate the impact of each \gls{implied.c} \gls{constraint}
(\refEquationRange{%
  impl-cons-defs-dominate-defs%
}{%
  impl-cons-fix-fall-throughs%
}) by comparing the solving times exhibited by two versions of the
\gls{constraint model}: one without a particular \gls{implied.c}
\gls{constraint}, and another with all such \glspl{constraint}.
%
For both \glsplshort{constraint model} -- we will refer to these as \modelA{}
and \modelB, respectively -- we include all other solving techniques since,
without them, a very long time out would have to be applied to show the impact
of a single solving technique, making it intractable to run the experiments.
%
Because the \gls{pattern set} used in these experiments contains only
\glspl{pattern} that do not \gls{span.b} any \glspl{block}, we expect
\refEquationList{impl-cons-defs-in-spanned-blocks, impl-cons-spanned-input} to
have no impact on solving time.
%
For the rest, we expect some \glspl{constraint} to lead to an overall reduction
in solving time while others may degrade solving time.
%
This is because some \glspl{constraint} may be too expensive to execute compared
with the amount of \gls{propagation} they provide, as described in
\refChapter{constraint-programming}.

\def\modelC{\textsc{iii}}
\def\modelD{\textsc{iv}}
\def\modelE{\textsc{v}}
\def\modelF{\textsc{vi}}

Second, based on the results for the experiment above, we then evaluate synergy
effects among these \glspl{constraint} by comparing the solving times exhibited
by \glspl{constraint model} with and without a given combination of
\glspl{constraint}.
%
Because a full evaluation would require us to test every combination of
\glspl{constraint}, which would result in an intractable number of experiments,
we limit ourselves to only comparing the solving times exhibited by four
versions of the \glsshort{constraint model}: one with no \gls{implied.c}
\glspl{constraint}, one with only those who individually lead to an overall
reduction in solving time, one without those who individually lead to an overall
increase in solving time, and another with all such \glspl{constraint}.
%
We refer to these \glsplshort{constraint model} as \modelC, \modelD, \modelE,
and \modelF, respectively.
%
We expect \glsplshort{constraint model}~\modelD, \modelE, and~\modelF{}
to all perform better than \glsplshort{constraint model}~\modelC.
%
No hypothesis is attempted regarding the relative performance between
\glsplshort{constraint model}~\modelD, \modelE, and~\modelF.

\includeSolvTechStats{impl-cons-defs-dominate-defs}
\includeSolvTechStats{impl-cons-defs-in-spanned-blocks}
\includeSolvTechStats{impl-cons-identical-entry-blocks}
\includeSolvTechStats{impl-cons-defs-dominate-entry-blocks}
\includeSolvTechStats{impl-cons-place-phi-ops-same-as-def-edges}
\includeSolvTechStats{impl-cons-no-span-uses}
\includeSolvTechStats{impl-cons-no-span-defs}
\includeSolvTechStats{impl-cons-no-span-use-defs}
\includeSolvTechStats{impl-cons-spanned-input}
\includeSolvTechStats{impl-cons-used-data-must-be-available}
\includeSolvTechStats{impl-cons-exterior-data-must-be-available}
\includeSolvTechStats{impl-cons-locs-of-uses}
\includeSolvTechStats{impl-cons-locs-of-defs}
\includeSolvTechStats{impl-cons-fix-fall-throughs}

\NewDocumentCommand{\mkFigureCaption}{s}{%
  \def\capToCText{%
    Set of plots for evaluating each implied constraint's impact on solving
    time%
  }%
  \def\capText{%
    Normalized solving times (incl.\ presolving time) for two constraint models:
    one without a particular implied constraint (baseline) and another with all
    such constraints (subject)%
  }%
  \IfBooleanTF{#1}{%
    \caption*{\capText}%
  }{%
    \caption[\capToCText]{\capText}%
  }%
}

\begin{figure}
  \centering

  \mkSolvTechSubfigure{impl-cons-defs-dominate-defs}%
                      {ImplConsDefsDominateDefs}%
  \hfill%
  \mkSolvTechSubfigure{impl-cons-defs-in-spanned-blocks}%
                      {ImplConsDefsInSpannedBlocks}

  \vspace{\betweensubfigures}

  \mkSolvTechSubfigure{impl-cons-identical-entry-blocks}%
                      {ImplConsIdenticalEntryBlocks}%
  \hfill%
  \mkSolvTechSubfigure{impl-cons-defs-dominate-entry-blocks}%
                      {ImplConsDefsDominateEntryBlocks}

  \vspace{\betweensubfigures}

  \mkSolvTechSubfigure{impl-cons-place-phi-ops-same-as-def-edges}%
                      {ImplConsPlacePhiOpsSameAsDefEdges}%
  \hfill%
  \mkSolvTechSubfigure{impl-cons-no-span-uses}%
                      {ImplConsNoSpanUses}

  \vspace{\betweensubfigures}

  \mkSolvTechSubfigure{impl-cons-no-span-defs}%
                      {ImplConsNoSpanDefs}%
  \hfill%
  \mkSolvTechSubfigure{impl-cons-no-span-use-defs}%
                      {ImplConsNoSpanUseDefs}

  \vspace{\betweensubfigures}

  \mkSolvTechSubfigure{impl-cons-spanned-input}%
                      {ImplConsSpannedInput}%
  \hfill%
  \mkSolvTechSubfigure{impl-cons-used-data-must-be-available}%
                      {ImplConsUsedDataMustBeAvailable}

  \mkFigureCaption
  \labelFigure{single-impl-con-vs-no-con-solving-time-plots}
\end{figure}

\let\oldthefigure\thefigure
\renewcommand{\thefigure}{\oldthefigure{} (cont.)}

\begin{figure}
  \ContinuedFloat
  \centering

  \mkSolvTechSubfigure{impl-cons-exterior-data-must-be-available}%
                      {ImplConsExteriorDataMustBeAvailable}%
  \hfill%
  \mkSolvTechSubfigure{impl-cons-locs-of-uses}%
                      {ImplConsLocsOfUses}

  \vspace{\betweensubfigures}

  \mkSolvTechSubfigure{impl-cons-locs-of-defs}%
                      {ImplConsLocsOfDefs}%
  \hfill%
  \mkSolvTechSubfigure{impl-cons-fix-fall-throughs}%
                      {ImplConsFixFallThroughs}

  \mkFigureCaption*
\end{figure}

\renewcommand{\thefigure}{\oldthefigure}

\RefFigure{single-impl-con-vs-no-con-solving-time-plots} shows the normalized
solving times (including \gls{presolving} time) for the two \glspl{constraint
  model} described above in the first experiment, with \glsshort{constraint
  model}~\modelA{} as \gls{baseline} and \glsshort{constraint model}~\modelB{}
as \gls{subject}.
%
The solving times range from
\printMinSolvingTime{
  \SolvTechDisableImplConsDefsDominateDefsPrePlusSolvingTimeSpeedupAllPrePlusSolvingTimeAvgMin,
  \SolvTechDisableImplConsDefsDominateDefsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMin,
  \SolvTechDisableImplConsDefsInSpannedBlocksPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMin,
  \SolvTechDisableImplConsIdenticalEntryBlocksPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMin,
  \SolvTechDisableImplConsDefsDominateEntryBlocksPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMin,
  \SolvTechDisableImplConsPlacePhiOpsSameAsDefEdgesPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMin,
  \SolvTechDisableImplConsNoSpanUsesPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMin,
  \SolvTechDisableImplConsNoSpanDefsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMin,
  \SolvTechDisableImplConsNoSpanUseDefsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMin,
  \SolvTechDisableImplConsSpannedInputPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMin,
  \SolvTechDisableImplConsUsedDataMustBeAvailablePrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMin,
  \SolvTechDisableImplConsExteriorDataMustBeAvailablePrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMin,
  \SolvTechDisableImplConsLocsOfUsesPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMin,
  \SolvTechDisableImplConsLocsOfDefsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMin,
  \SolvTechDisableImplConsFixFallThroughsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMin
} to
\printMaxSolvingTime{
  \SolvTechDisableImplConsDefsDominateDefsPrePlusSolvingTimeSpeedupAllPrePlusSolvingTimeAvgMax,
  \SolvTechDisableImplConsDefsDominateDefsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMax,
  \SolvTechDisableImplConsDefsInSpannedBlocksPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMax,
  \SolvTechDisableImplConsIdenticalEntryBlocksPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMax,
  \SolvTechDisableImplConsDefsDominateEntryBlocksPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMax,
  \SolvTechDisableImplConsPlacePhiOpsSameAsDefEdgesPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMax,
  \SolvTechDisableImplConsNoSpanUsesPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMax,
  \SolvTechDisableImplConsNoSpanDefsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMax,
  \SolvTechDisableImplConsNoSpanUseDefsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMax,
  \SolvTechDisableImplConsSpannedInputPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMax,
  \SolvTechDisableImplConsUsedDataMustBeAvailablePrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMax,
  \SolvTechDisableImplConsExteriorDataMustBeAvailablePrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMax,
  \SolvTechDisableImplConsLocsOfUsesPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMax,
  \SolvTechDisableImplConsLocsOfDefsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMax,
  \SolvTechDisableImplConsFixFallThroughsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMax
} with a maximum coefficient of variation of
\numMaxOf{
  \SolvTechDisableImplConsDefsDominateDefsPrePlusSolvingTimeSpeedupAllPrePlusSolvingTimeCvMax,
  \SolvTechDisableImplConsDefsDominateDefsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeCvMax,
  \SolvTechDisableImplConsDefsInSpannedBlocksPrePlusSolvingTimeSpeedupPrePlusSolvingTimeCvMax,
  \SolvTechDisableImplConsIdenticalEntryBlocksPrePlusSolvingTimeSpeedupPrePlusSolvingTimeCvMax,
  \SolvTechDisableImplConsDefsDominateEntryBlocksPrePlusSolvingTimeSpeedupPrePlusSolvingTimeCvMax,
  \SolvTechDisableImplConsPlacePhiOpsSameAsDefEdgesPrePlusSolvingTimeSpeedupPrePlusSolvingTimeCvMax,
  \SolvTechDisableImplConsNoSpanUsesPrePlusSolvingTimeSpeedupPrePlusSolvingTimeCvMax,
  \SolvTechDisableImplConsNoSpanDefsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeCvMax,
  \SolvTechDisableImplConsNoSpanUseDefsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeCvMax,
  \SolvTechDisableImplConsSpannedInputPrePlusSolvingTimeSpeedupPrePlusSolvingTimeCvMax,
  \SolvTechDisableImplConsUsedDataMustBeAvailablePrePlusSolvingTimeSpeedupPrePlusSolvingTimeCvMax,
  \SolvTechDisableImplConsExteriorDataMustBeAvailablePrePlusSolvingTimeSpeedupPrePlusSolvingTimeCvMax,
  \SolvTechDisableImplConsLocsOfUsesPrePlusSolvingTimeSpeedupPrePlusSolvingTimeCvMax,
  \SolvTechDisableImplConsLocsOfDefsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeCvMax,
  \SolvTechDisableImplConsFixFallThroughsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeCvMax
}.
%
From the \glspl{GMI} and \glspl{CI}, also given in
\refFigure{single-impl-con-vs-no-con-solving-time-plots}, we see that
\refEquation{impl-cons-fix-fall-throughs} leads to an overall reduction in
solving time, that \refEquationList{impl-cons-defs-dominate-entry-blocks,
  impl-cons-no-span-use-defs} lead to an overall increase in solving time, and
that \refEquationList{impl-cons-defs-in-spanned-blocks, impl-cons-spanned-input,
  impl-cons-place-phi-ops-same-as-def-edges} have no impact on solving time.
%
For \refEquationList{impl-cons-defs-dominate-defs,
  impl-cons-identical-entry-blocks, impl-cons-no-span-uses,
  impl-cons-no-span-defs, impl-cons-used-data-must-be-available,
  impl-cons-exterior-data-must-be-available, impl-cons-locs-of-uses,
  impl-cons-locs-of-defs}, the results are inconclusive.

\includeSolvTechStats[enable]{only-good-implied-cons}
\includeSolvTechStats{bad-implied-cons}
\includeSolvTechStats{all-implied-cons}

\begin{figure}
  \centering%

  \mkSolvTechSubfigure[enable]%
                      {only-good-implied-cons}%
                      [Only \refEquation{impl-cons-fix-fall-throughs}]
                      {OnlyGoodImpliedCons}%
  \hfill%
  \mkSolvTechSubfigure{bad-implied-cons}%
                      [%
                        Without
                        \refEquationList{impl-cons-defs-dominate-entry-blocks,
                          impl-cons-no-span-use-defs}%
                      ]%
                      [4.3cm]%
                      {BadImpliedCons}%

  \vspace{\betweensubfigures}

  \mkSolvTechSubfigure{all-implied-cons}%
                      [All implied constraints]%
                      [\linewidth]%
                      {AllImpliedCons}%

  \caption[%
            Plot for evaluating the impact on solving time made by different
            combinations of implied constraints%
          ]%
          {%
            Normalized solving times (incl.\ presolving time) for four
            constraint models: one with no implied constraints (baseline), one
            with only such constraints that individually have a positive impact
            on solving time (subject), one without such constraints that
            individually have a negative impact on solving time (subject), and
            another with all such constraints (subject)%
          }
  \labelFigure{diff-impl-cons-comb-solving-time-plot}
\end{figure}

\RefFigure{diff-impl-cons-comb-solving-time-plot} shows the normalized solving
times (including \gls{presolving} time) for the four \glspl{constraint model}
described above in the second experiment, with \glsshort{constraint
  model}~\modelC{} as \gls{baseline} and \glsplshort{constraint model}~\modelD,
\modelE, and~\modelF{} as \glspl{subject}.
%
The solving times range from
\printMinSolvingTime{
  \SolvTechEnableOnlyGoodImpliedConsPrePlusSolvingTimeSpeedupNonePrePlusSolvingTimeAvgMin,
  \SolvTechEnableOnlyGoodImpliedConsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMin,
  \SolvTechDisableBadImpliedConsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMin,
  \SolvTechDisableAllImpliedConsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMin
} to
\printMaxSolvingTime{
  \SolvTechEnableOnlyGoodImpliedConsPrePlusSolvingTimeSpeedupNonePrePlusSolvingTimeAvgMax,
  \SolvTechEnableOnlyGoodImpliedConsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMax,
  \SolvTechDisableBadImpliedConsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMax,
  \SolvTechDisableAllImpliedConsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMax
} with a maximum coefficient of variation of
\numMaxOf{
  \SolvTechEnableOnlyGoodImpliedConsPrePlusSolvingTimeSpeedupNonePrePlusSolvingTimeCvMax,
  \SolvTechEnableOnlyGoodImpliedConsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeCvMax,
  \SolvTechDisableBadImpliedConsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeCvMax,
  \SolvTechDisableAllImpliedConsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeCvMax
}.

To begin with, we observe that the \gls{GMI} for \glsshort{constraint
  model}~\modelD{} over \glsshort{constraint model}~\modelC{}
(\refFigure{solv-tech-enable-only-good-implied-cons}) is \printGMI{%
  \SolvTechEnableOnlyGoodImpliedConsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeRegularSpeedupGmean%
} with \gls{CI}~\printGMICI{%
  \SolvTechEnableOnlyGoodImpliedConsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeRegularSpeedupCiMin%
}{%
  \SolvTechEnableOnlyGoodImpliedConsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeRegularSpeedupCiMax%
}.
%
Since we no longer achieve an overall reduction in solving time, it means that
adding an \gls{implied.c} \gls{constraint} to a \glsshort{constraint model} with
other \gls{implied.c} \glspl{constraint} is more beneficial than adding an
\gls{implied.c} \gls{constraint} to a \glsshort{constraint model} with no such
\glspl{constraint}.
%
In comparison, we observe that the \gls{GMI} for \glsshort{constraint
  model}~\modelE{} over \glsshort{constraint model}~\modelC{}
(\refFigure{solv-tech-disable-bad-implied-cons}) is \printGMI{%
  \SolvTechDisableBadImpliedConsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeRegularSpeedupGmean%
} with \gls{CI}~\printGMICI{%
  \SolvTechDisableBadImpliedConsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeRegularSpeedupCiMin%
}{%
  \SolvTechDisableBadImpliedConsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeRegularSpeedupCiMax%
}.
%
Although the result is statistically not conclusive, this suggests that also
adding \gls{implied.c} \glspl{constraint} whose individual impact on solving
time is inconclusive is more beneficial than only including \gls{implied.c}
\glspl{constraint} whose individual positive impact on solving time is
statistically significant.
%
Lastly, we observe that the \gls{GMI} for \glsshort{constraint model}~\modelF{}
over \glsshort{constraint model}~\modelC{}
(\refFigure{solv-tech-disable-all-implied-cons}) is \printGMI{%
  \SolvTechDisableAllImpliedConsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeRegularSpeedupGmean%
} with \gls{CI}~\printGMICI{%
  \SolvTechDisableAllImpliedConsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeRegularSpeedupCiMin%
}{%
  \SolvTechDisableAllImpliedConsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeRegularSpeedupCiMax%
}.
%
Since \glsshort{constraint model}~\modelF{} contains \gls{implied.c}
\glspl{constraint} whose individual impact on solving time is statistically
negative yet still outperforms \glsshort{constraint model}~\modelE, this means
that there are synergy effects between the \gls{implied.c} \glspl{constraint}
whose collective positive impact on solving time is greater than the sum of its
parts.
%
Hence we conclude that it is beneficial to include all \gls{implied.c}
\glspl{constraint} in the \glsshort{constraint model}.


\subsection{Symmetry and Dominance Breaking Constraints}
\labelSection{st-exp-evaluation-dom-constraints}

\def\modelA{\textsc{i}}
\def\modelB{\textsc{ii}}

First, like in \refSection{st-exp-evaluation-implied-constraints} we evaluate
the impact of each \glsshort{symmetry breaking.c} and \gls{dominance breaking.c}
\gls{constraint}
(\refEquationRange{%
  dom-cons-locs-of-states%
}{%
  dom-cons-kill-match-selection%
}) by comparing the solving times exhibited by two versions of the
\gls{constraint model}: one without a particular \glsshort{symmetry breaking.c}
or \gls{dominance breaking.c} \gls{constraint}, and another with all such
\glspl{constraint}.
%
We refer to these \glsplshort{constraint model} as \modelA{} and \modelB,
respectively.
%
For the same reason as with the \gls{implied.c} \glspl{constraint}, we expect
some \glspl{constraint} to lead to an overall reduction in solving time while
others may degrade solving time.

\def\modelC{\textsc{iii}}
\def\modelD{\textsc{iv}}
\def\modelE{\textsc{v}}
\def\modelF{\textsc{vi}}

Second, based on the results for the experiment above, we then evaluate synergy
effects among these \glspl{constraint} by comparing the solving times exhibited
by \glspl{constraint model} with and without a given combination of
\glspl{constraint}.
%
Again, for practicality we limit ourselves to only comparing the solving times
exhibited by four versions of the \glsshort{constraint model}: one with no
\glsshort{symmetry breaking.c} or \gls{dominance breaking.c} \glspl{constraint},
one with only those who individually lead to an overall reduction in solving
time, one without those who individually lead to an overall increase in solving
time, and another one with all such \glspl{constraint}.
%
We refer to these \glsplshort{constraint model} as \modelC, \modelD, \modelE,
and \modelF, respectively.
%
We expect \glsplshort{constraint model}~\modelD, \modelE, and~\modelF{} to all
perform better than \glsplshort{constraint model}~\modelC.
%
No hypothesis is attempted regarding the relative performance between
\glsplshort{constraint model}~\modelD, \modelE, and~\modelF.

\includeSolvTechStats{dom-cons-locs-of-states}
\includeSolvTechStats{dom-cons-operands-of-non-selected-matches}
\includeSolvTechStats{impl-cons-input-operands-not-taking-min-value}
\includeSolvTechStats{dom-cons-interch-data-chains}
\includeSolvTechStats{dom-cons-null-copy-match-selection}
\includeSolvTechStats{dom-cons-kill-match-selection}

\begin{figure}
  \centering

  \mkSolvTechSubfigure{dom-cons-locs-of-states}%
                      {DomConsLocsOfStates}%
  \hfill%
  \mkSolvTechSubfigure{dom-cons-operands-of-non-selected-matches}%
                      {DomConsOperandsOfNonSelectedMatches}

  \vspace{\betweensubfigures}

  \mkSolvTechSubfigure{impl-cons-input-operands-not-taking-min-value}%
                      {ImplConsInputOperandsNotTakingMinValue}%
  \hfill%
  \mkSolvTechSubfigure{dom-cons-interch-data-chains}%
                      {DomConsInterchDataChains}

  \vspace{\betweensubfigures}

  \mkSolvTechSubfigure{dom-cons-null-copy-match-selection}%
                      {DomConsNullCopyMatchSelection}%
  \hfill%
  \mkSolvTechSubfigure{dom-cons-kill-match-selection}%
                      {DomConsKillMatchSelection}

  \caption[%
            Set of plots for evaluating each symmetry or dominance breaking
            constraint's impact on solving time%
          ]%
          {%
            Normalized solving times (incl.\ presolving time) for two
            constraint models: one without a particular symmetry or dominance
            breaking constraint (baseline) and another with all such
            constraints (subject)%
          }
  \labelFigure{single-dom-con-vs-no-con-solving-time-plots}
\end{figure}

\RefFigure{single-dom-con-vs-no-con-solving-time-plots} shows the normalized
solving times (including \gls{presolving} time) for two \glspl{constraint model}
described above in the first experiment, with \glsshort{constraint
  model}~\modelA{} as \gls{baseline} and \glsshort{constraint model}~\modelB{}
as \gls{subject}.
%
The solving times range from
\printMinSolvingTime{
  \SolvTechDisableDomConsLocsOfStatesPrePlusSolvingTimeSpeedupAllPrePlusSolvingTimeAvgMin,
  \SolvTechDisableDomConsLocsOfStatesPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMin,
  \SolvTechDisableDomConsOperandsOfNonSelectedMatchesPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMin,
  \SolvTechDisableImplConsInputOperandsNotTakingMinValuePrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMin,
  \SolvTechDisableDomConsInterchDataChainsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMin,
  \SolvTechDisableDomConsNullCopyMatchSelectionPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMin,
  \SolvTechDisableDomConsKillMatchSelectionPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMin
} to
\printMaxSolvingTime{
  \SolvTechDisableDomConsLocsOfStatesPrePlusSolvingTimeSpeedupAllPrePlusSolvingTimeAvgMax,
  \SolvTechDisableDomConsLocsOfStatesPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMax,
  \SolvTechDisableDomConsOperandsOfNonSelectedMatchesPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMax,
  \SolvTechDisableImplConsInputOperandsNotTakingMinValuePrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMax,
  \SolvTechDisableDomConsInterchDataChainsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMax,
  \SolvTechDisableDomConsNullCopyMatchSelectionPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMax,
  \SolvTechDisableDomConsKillMatchSelectionPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMax
} with a maximum coefficient of variation of
\numMaxOf{
  \SolvTechDisableDomConsLocsOfStatesPrePlusSolvingTimeSpeedupAllPrePlusSolvingTimeCvMax,
  \SolvTechDisableDomConsLocsOfStatesPrePlusSolvingTimeSpeedupPrePlusSolvingTimeCvMax,
  \SolvTechDisableDomConsOperandsOfNonSelectedMatchesPrePlusSolvingTimeSpeedupPrePlusSolvingTimeCvMax,
  \SolvTechDisableImplConsInputOperandsNotTakingMinValuePrePlusSolvingTimeSpeedupPrePlusSolvingTimeCvMax,
  \SolvTechDisableDomConsInterchDataChainsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeCvMax,
  \SolvTechDisableDomConsNullCopyMatchSelectionPrePlusSolvingTimeSpeedupPrePlusSolvingTimeCvMax,
  \SolvTechDisableDomConsKillMatchSelectionPrePlusSolvingTimeSpeedupPrePlusSolvingTimeCvMax
}.
%
From the \glspl{GMI} and \glspl{CI}, also given in
\refFigure{single-dom-con-vs-no-con-solving-time-plots}, we observe that
\refEquationList{dom-cons-null-copy-match-selection,
  dom-cons-kill-match-selection} lead to an overall reduction in solving time
and that \refEquation{impl-cons-input-operands-not-taking-min-value} has no
impact on solving time.
%
For \refEquationList{dom-cons-locs-of-states,
  dom-cons-operands-of-non-selected-matches, dom-cons-interch-data-chains}, the
results are inconclusive.

\includeSolvTechStats[enable]{only-good-dom-cons}
\includeSolvTechStats{all-dom-cons}

\begin{figure}
  \centering%

  \mkSolvTechSubfigure[enable]%
                      {only-good-dom-cons}%
                      [%
                        Only
                        \refEquationList{dom-cons-null-copy-match-selection,
                          dom-cons-kill-match-selection}%
                      ]%
                      [4cm]%
                      {OnlyGoodDomCons}%
  \hfill%
  \mkSolvTechSubfigure{all-dom-cons}%
                      [All symmetry and dominance breaking constraints]%
                      {AllDomCons}%

  \caption[%
            Plot for evaluating the impact on solving time made by different
            combinations of symmetry and dominance breaking constraints%
          ]%
          {%
            Normalized solving times (incl.\ presolving time) for three
            constraint models: one with no symmetry or dominance breaking
            constraints (baseline), one with only such constraints that
            individually have a positive impact on solving time (subject), and
            another with all such constraints (subject)%
          }
  \labelFigure{diff-dom-cons-comb-solving-time-plot}
\end{figure}

\RefFigure{diff-dom-cons-comb-solving-time-plot} shows the normalized solving
times (including \gls{presolving} time) for three of the \glspl{constraint
  model} described above in the second experiment, with \glsshort{constraint
  model}~\modelC{} as \gls{baseline} and \glsplshort{constraint model}~\modelD{}
and~\modelF{} as \glspl{subject} (\glsshort{constraint model}~\modelE{} is not
evaluated since no \glsshort{symmetry breaking.c} or \gls{dominance breaking.c}
\glspl{constraint} appear to individually have a negative impact on solving
time).
%
The solving times range from
\printMinSolvingTime{
  \SolvTechEnableOnlyGoodDomConsPrePlusSolvingTimeSpeedupNonePrePlusSolvingTimeAvgMin,
  \SolvTechEnableOnlyGoodDomConsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMin,
  \SolvTechDisableAllDomConsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMin
} to
\printMaxSolvingTime{
  \SolvTechEnableOnlyGoodDomConsPrePlusSolvingTimeSpeedupNonePrePlusSolvingTimeAvgMax,
  \SolvTechEnableOnlyGoodDomConsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMax,
  \SolvTechDisableAllDomConsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMax
} with a maximum coefficient of variation of
\numMaxOf{
  \SolvTechEnableOnlyGoodDomConsPrePlusSolvingTimeSpeedupNonePrePlusSolvingTimeCvMax,
  \SolvTechEnableOnlyGoodDomConsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeCvMax,
  \SolvTechDisableAllDomConsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeCvMax
}.

We observe that the \gls{GMI} for \glsshort{constraint model}~\modelD{}
over \glsshort{constraint model}~\modelC{}
(\refFigure{solv-tech-enable-only-good-dom-cons}) is \printGMI{%
  \SolvTechEnableOnlyGoodDomConsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeRegularSpeedupGmean%
} with \gls{CI}~\printGMICI{%
  \SolvTechEnableOnlyGoodDomConsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeRegularSpeedupCiMin%
}{%
  \SolvTechEnableOnlyGoodDomConsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeRegularSpeedupCiMax%
}, thus the solving time improvement is statically significant.
%
This means that, unlike with \gls{implied.c} \glspl{constraint}, adding a
\glsshort{symmetry breaking.c} or \gls{dominance breaking.c} \gls{constraint} to
a \glsshort{constraint model} with no such \glspl{constraint} may be equally or
more beneficial as adding a \gls{constraint} to a \glsshort{constraint model}
with other such \glspl{constraint}.
%
Note also that these two \glspl{constraint} yield considerable reduction in
solving time for two \gls{function} that were only marginally impacted by each
\gls{constraint} individually.
%
Hence there are strong synergy effects between
\refEquationList{dom-cons-null-copy-match-selection,
  dom-cons-kill-match-selection}, which most likely also interacts with other
\glspl{constraint}.

In comparison, we see that \glsshort{constraint model}~\modelF{} yields a
greater reduction in solving time for a larger number of \glspl{function} than
\glsshort{constraint model}~\modelD{} but also a considerable increase in
solving time for several \glspl{function}.
%
Despite a \gls{GMI} of \printGMI{%
  \SolvTechDisableAllDomConsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeRegularSpeedupGmean%
}, the \gls{CI} of \printGMICI{%
  \SolvTechDisableAllDomConsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeRegularSpeedupCiMin%
}{%
  \SolvTechDisableAllDomConsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeRegularSpeedupCiMax%
} means it is not worthwhile to include all \glsshort{symmetry breaking.c} and
\gls{dominance breaking.c} \glspl{constraint}.
%
Hence we conclude that it is most beneficial to only include
\refEquationList{dom-cons-null-copy-match-selection,
  dom-cons-kill-match-selection} in the \glsshort{constraint model}.


\subsection{Presolving}

\def\modelA{\textsc{i}}
\def\modelB{\textsc{ii}}

Like in \refSectionList{st-exp-evaluation-implied-constraints,
  st-exp-evaluation-dom-constraints}, we first evaluate the impact of each
\gls{presolving} technique (\refSection{st-pre-dom-matches},
\refEquationRange{illegal-matches-uncovered-ops}{redun-non-null-copy-matches},
and \refSection{st-canonical-locations}) by comparing the solving times
exhibited by two versions of the \gls{constraint model}: one without a
particular \gls{presolving} technique, and another with all such techniques.
%
We refer to these \glsplshort{constraint model} as \modelA{} and \modelB,
respectively.
%
For the same reason as with the \gls{implied.c}, \gls{symmetry breaking.c},
\gls{dominance breaking.c} \glspl{constraint}, we expect some techniques to lead
to an overall reduction in solving time while others may degrade solving time.

\def\modelC{\textsc{iii}}
\def\modelD{\textsc{iv}}
\def\modelE{\textsc{v}}
\def\modelF{\textsc{vi}}

Second, based on the results for the experiment above, we then evaluate synergy
effects among these techniques by comparing the solving times exhibited by
\glspl{constraint model} with and without a given combination of techniques.
%
Again, for practicality we limit ourselves to only comparing the solving times
exhibited by four versions of the \glsshort{constraint model}: one with no
\gls{presolving} techniques, one with only those who individually lead to an
overall reduction in solving time, one without those who individually lead to an
overall increase in solving time, and another one with all such techniques.
%
We refer to these \glsplshort{constraint model} as \modelC, \modelD, \modelE,
and \modelF, respectively.
%
We expect \glsplshort{constraint model}~\modelD, \modelE, and~\modelF{} to all
perform better than \glsplshort{constraint model}~\modelC.
%
No hypothesis is attempted regarding the relative performance between
\glsplshort{constraint model}~\modelD, \modelE, and~\modelF.

\includeSolvTechStats{dom-matches}
\includeSolvTechStats{illegal-matches-uncovered-ops}
\includeSolvTechStats{illegal-matches-undefined-data}
\includeSolvTechStats{illegal-matches-kills}
\includeSolvTechStats{illegal-matches-def-locs}
\includeSolvTechStats{illegal-matches-use-locs}
\includeSolvTechStats{redun-kills}
\includeSolvTechStats{redun-non-null-copy-matches}
\includeSolvTechStats{canonical-locs}

\begin{figure}
  \centering

  \mkSolvTechSubfigure{dom-matches}%
                      [Dominated matches]%
                      [3.65cm]%
                      {DomMatches}%
  \hfill%
  \mkSolvTechSubfigure{illegal-matches-uncovered-ops}%
                      {IllegalMatchesUncoveredOps}

  \vspace{\betweensubfigures}

  \mkSolvTechSubfigure{illegal-matches-undefined-data}%
                      {IllegalMatchesUndefinedData}%
  \hfill%
  \mkSolvTechSubfigure{illegal-matches-kills}%
                      {IllegalMatchesKills}%

  \vspace{\betweensubfigures}

  \mkSolvTechSubfigure{illegal-matches-def-locs}%
                      {IllegalMatchesDefLocs}%
  \hfill%
  \mkSolvTechSubfigure{illegal-matches-use-locs}%
                      {IllegalMatchesUseLocs}

  \vspace{\betweensubfigures}

  \mkSolvTechSubfigure{redun-kills}%
                      {RedunKills}%
  \hfill%
  \mkSolvTechSubfigure{redun-non-null-copy-matches}%
                      {RedunNonNullCopyMatches}

  \vspace{\betweensubfigures}

  \mkSolvTechSubfigure{canonical-locs}%
                      [Canonical locations]%
                      [\linewidth]%
                      {CanonicalLocs}

  \caption[%
            Set of plots for evaluating each presolving technique's impact on
            solving time%
          ]%
          {%
            Normalized solving times (incl.\ presolving time) for two
            constraint models: one without a particular presolving technique
            (baseline) and another with all such techniques (subject)%
          }
  \labelFigure{single-presolving-vs-no-presolving-solving-time-plots}
\end{figure}

\RefFigure{single-presolving-vs-no-presolving-solving-time-plots} shows the
normalized solving times (including \gls{presolving} time) for the two
\glspl{constraint model} described above in the first experiment, with
\glsshort{constraint model}~\modelA{} as \gls{baseline} and \glsshort{constraint
  model}~\modelB{} as \gls{subject}.
%
The solving times range from
\printMinSolvingTime{
  \SolvTechDisableDomMatchesPrePlusSolvingTimeSpeedupAllPrePlusSolvingTimeAvgMin,
  \SolvTechDisableDomMatchesPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMin,
  \SolvTechDisableIllegalMatchesUncoveredOpsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMin,
  \SolvTechDisableIllegalMatchesUndefinedDataPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMin,
  \SolvTechDisableIllegalMatchesKillsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMin,
  \SolvTechDisableIllegalMatchesDefLocsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMin,
  \SolvTechDisableIllegalMatchesUseLocsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMin,
  \SolvTechDisableRedunKillsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMin,
  \SolvTechDisableRedunNonNullCopyMatchesPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMin,
  \SolvTechDisableCanonicalLocsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMin
} to
\printMaxSolvingTime{
  \SolvTechDisableDomMatchesPrePlusSolvingTimeSpeedupAllPrePlusSolvingTimeAvgMax,
  \SolvTechDisableDomMatchesPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMax,
  \SolvTechDisableIllegalMatchesUncoveredOpsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMax,
  \SolvTechDisableIllegalMatchesUndefinedDataPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMax,
  \SolvTechDisableIllegalMatchesKillsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMax,
  \SolvTechDisableIllegalMatchesDefLocsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMax,
  \SolvTechDisableIllegalMatchesUseLocsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMax,
  \SolvTechDisableRedunKillsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMax,
  \SolvTechDisableRedunNonNullCopyMatchesPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMax,
  \SolvTechDisableCanonicalLocsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMax
} with a maximum coefficient of variation of
\numMaxOf{
  \SolvTechDisableDomMatchesPrePlusSolvingTimeSpeedupAllPrePlusSolvingTimeCvMax,
  \SolvTechDisableDomMatchesPrePlusSolvingTimeSpeedupPrePlusSolvingTimeCvMax,
  \SolvTechDisableIllegalMatchesUncoveredOpsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeCvMax,
  \SolvTechDisableIllegalMatchesUndefinedDataPrePlusSolvingTimeSpeedupPrePlusSolvingTimeCvMax,
  \SolvTechDisableIllegalMatchesKillsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeCvMax,
  \SolvTechDisableIllegalMatchesDefLocsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeCvMax,
  \SolvTechDisableIllegalMatchesUseLocsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeCvMax,
  \SolvTechDisableRedunKillsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeCvMax,
  \SolvTechDisableRedunNonNullCopyMatchesPrePlusSolvingTimeSpeedupPrePlusSolvingTimeCvMax,
  \SolvTechDisableCanonicalLocsPrePlusSolvingTimeSpeedupPrePlusSolvingTimeCvMax
}.
%
From the \glspl{GMI} and \glspl{CI}, also given in
\refFigure{single-presolving-vs-no-presolving-solving-time-plots}, we observe
that \refEquationList{redun-kills, redun-non-null-copy-matches} lead to an
overall reduction in solving time, that
\refEquationList{illegal-matches-def-locs, illegal-matches-use-locs} lead to an
overall increase in solving time, and that
\refEquation{illegal-matches-uncovered-ops} has no impact on solving time.
%
For \gls{dominate.m}[d] \glspl{match},
\refEquationList{illegal-matches-uncovered-ops, illegal-matches-undefined-data},
and \gls{canonical.l} \glspl{location}, the results are inconclusive.

\includeSolvTechStats[enable]{only-good-presolving}
\includeSolvTechStats{bad-presolving}
\includeSolvTechStats{all-presolving}

\begin{figure}
  \centering%

  \mkSolvTechSubfigure[enable]%
                      {only-good-presolving}%
                      [%
                        Only \refEquationList{redun-kills,
                          redun-non-null-copy-matches}%
                      ]%
                      [3.6cm]%
                      {OnlyGoodPresolving}%
  \hfill%
  \mkSolvTechSubfigure{bad-presolving}%
                      [%
                        Without \refEquationList{illegal-matches-def-locs,
                          illegal-matches-use-locs}%
                      ]%
                      [4.3cm]%
                      {BadPresolving}%

  \vspace{\betweensubfigures}

  \mkSolvTechSubfigure{all-presolving}%
                      [All presolving techniques]%
                      [\linewidth]%
                      {AllPresolving}%

  \caption[%
            Plot for evaluating the impact on solving time made by different
            combinations of presolving techniques%
          ]%
          {%
            Normalized solving times (incl.\ presolving time) for four
            constraint models: one with no presolving techniques (baseline), one
            with only such techniques that individually have a positive impact
            on solving time (subject), one without such techniques that
            individually have a negative impact on solving time (subject), and
            another with all such techniques (subject)%
          }
  \labelFigure{diff-presolving-comb-solving-time-plot}
\end{figure}

\RefFigure{diff-presolving-comb-solving-time-plot} shows the normalized solving
times (including \gls{presolving} time) for the four \glspl{constraint model}
described above in the second experiment, with \glsshort{constraint
  model}~\modelC{} as \gls{baseline} and \glsplshort{constraint model}~\modelD,
\modelE, and~\modelF{} as \glspl{subject}.
%
The solving times range from
\printMinSolvingTime{
  \SolvTechEnableOnlyGoodPresolvingPrePlusSolvingTimeSpeedupNonePrePlusSolvingTimeAvgMin,
  \SolvTechEnableOnlyGoodPresolvingPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMin,
  \SolvTechDisableBadPresolvingPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMin,
  \SolvTechDisableAllPresolvingPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMin
} to
\printMaxSolvingTime{
  \SolvTechEnableOnlyGoodPresolvingPrePlusSolvingTimeSpeedupNonePrePlusSolvingTimeAvgMax,
  \SolvTechEnableOnlyGoodPresolvingPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMax,
  \SolvTechDisableBadPresolvingPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMax,
  \SolvTechDisableAllPresolvingPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMax
} with a maximum coefficient of variation of
\numMaxOf{
  \SolvTechEnableOnlyGoodPresolvingPrePlusSolvingTimeSpeedupNonePrePlusSolvingTimeCvMax,
  \SolvTechEnableOnlyGoodPresolvingPrePlusSolvingTimeSpeedupPrePlusSolvingTimeCvMax,
  \SolvTechDisableBadPresolvingPrePlusSolvingTimeSpeedupPrePlusSolvingTimeCvMax,
  \SolvTechDisableAllPresolvingPrePlusSolvingTimeSpeedupPrePlusSolvingTimeCvMax
}.

To begin with, we observe that the \gls{GMI} for \glsshort{constraint
  model}~\modelD{} over \glsshort{constraint model}~\modelC{}
(\refFigure{solv-tech-enable-only-good-presolving}) is \printGMI{%
  \SolvTechEnableOnlyGoodPresolvingPrePlusSolvingTimeSpeedupPrePlusSolvingTimeRegularSpeedupGmean%
} with \gls{CI}~\printGMICI{%
  \SolvTechEnableOnlyGoodPresolvingPrePlusSolvingTimeSpeedupPrePlusSolvingTimeRegularSpeedupCiMin%
}{%
  \SolvTechEnableOnlyGoodPresolvingPrePlusSolvingTimeSpeedupPrePlusSolvingTimeRegularSpeedupCiMax%
}, thus the solving time improvement is statically significant.
%
This means, like with \glsshort{symmetry breaking.c} and \gls{dominance
  breaking.c} \glspl{constraint}, adding a \gls{presolving} technique to a
\glsshort{constraint model} with no such techniques may be equally or more
beneficial as adding a technique to a \glsshort{constraint model} with other
such techniques.
%
Note also that these two \glspl{constraint} yield considerable reduction in
solving time for two \gls{function} that were only marginally impacted by each
\gls{constraint} individually.
%
Hence there are strong synergy effects between \refEquationList{redun-kills,
  redun-non-null-copy-matches}, which most likely also interacts with other
parts of the \glsshort{constraint model}.

Next, we observe that the \gls{GMI} for \glsshort{constraint model}~\modelE{}
over \glsshort{constraint model}~\modelC{}
(\refFigure{solv-tech-disable-bad-presolving}) is \printGMI{%
  \SolvTechDisableBadPresolvingPrePlusSolvingTimeSpeedupPrePlusSolvingTimeRegularSpeedupGmean%
} with \gls{CI}~\printGMICI{%
  \SolvTechDisableBadPresolvingPrePlusSolvingTimeSpeedupPrePlusSolvingTimeRegularSpeedupCiMin%
}{%
  \SolvTechDisableBadPresolvingPrePlusSolvingTimeSpeedupPrePlusSolvingTimeRegularSpeedupCiMax%
}, which is a greater reduction in solving time compared to \glsshort{constraint
  model}~\modelD.
%
Note also that for no \glspl{function} does this \glsshort{constraint model}
degrade solving time, which means that the \gls{presolving} techniques that
individually had a negative impact on solving time have strong synergy effects.
%
Many of these \gls{presolving} techniques also require computations that are
expensive to execute but whose result can be shared among the techniques,
allowing this cost to be amortized when the techniques are executed in unison.

Lastly, we observe that the \gls{GMI} for \glsshort{constraint model}~\modelF{}
over \glsshort{constraint model}~\modelC{}
(\refFigure{solv-tech-disable-all-presolving}) is \printGMI{%
  \SolvTechDisableAllPresolvingPrePlusSolvingTimeSpeedupPrePlusSolvingTimeRegularSpeedupGmean%
} with \gls{CI}~\printGMICI{%
  \SolvTechDisableAllPresolvingPrePlusSolvingTimeSpeedupPrePlusSolvingTimeRegularSpeedupCiMin%
}{%
  \SolvTechDisableAllPresolvingPrePlusSolvingTimeSpeedupPrePlusSolvingTimeRegularSpeedupCiMax%
}, which is still better than \glsshort{constraint model}~\modelD but worse than
\glsshort{constraint model}~\modelE.
%
Looking at the \gls{CI}, we see that the spread of improvement for each
\gls{function} is wider.
%
In other words, while \glsshort{constraint model}~\modelF{} improves solving
time for some \glspl{function} over \glsshort{constraint model}~\modelE, it
degrades solving time for a larger set of \glspl{function}, which is reflected
in the \gls{GMI}.
%
Hence we conclude that it is most beneficial to exclude
\refEquationList{illegal-matches-def-locs, illegal-matches-use-locs} from the
\glsshort{constraint model}.


\subsection{Impact of All Solving Techniques}

\def\modelA{\textsc{i}}
\def\modelB{\textsc{ii}}
\def\modelC{\textsc{iii}}
\def\modelD{\textsc{iv}}
\def\modelE{\textsc{v}}

We now evaluate the impact on solving time made by different collections of
solving techniques by comparing the solving times exhibited by five versions of
the \gls{constraint model}: one with no solving techniques, one with only those
who individually lead to an overall reduction in solving time, one without those
who individually lead to an overall increase in solving time, one with all such
techniques, and another with with the \gls{implied.c}, \glsshort{symmetry
  breaking.c} and \gls{dominance breaking.c} \glspl{constraint} and
\gls{presolving} techniques that were performed best in previous experiments.
%
We refer to these \glsplshort{constraint model} as \modelA, \modelB, \modelC,
\modelD, and \modelE, respectively.
%
We expect \glsplshort{constraint model}~\modelB, \modelC, \modelD, and~\modelE{}
to all perform better than \glsplshort{constraint model}~\modelA.
%
We also expect \glsshort{constraint model}~\modelE{} to outperform
\glsplshort{constraint model}~\modelB, \modelC, and~\modelD{} as it contains the
solving techniques that performed best in the previous experiments.

\includeSolvTechStats[enable]{only-all-good}
\includeSolvTechStats{all-bad}
\includeSolvTechStats{all}
\includeSolvTechStats[best]{combos}

\begin{figure}
  \centering%

  \mkSolvTechSubfigure[enable]%
                      {only-all-good}%
                      [%
                        Only \refEquationList{impl-cons-fix-fall-throughs,
                          dom-cons-null-copy-match-selection,
                          dom-cons-kill-match-selection, redun-kills,
                          redun-non-null-copy-matches}%
                      ]%
                      [6cm]%
                      {OnlyAllGood}%
  \hfill%
  \mkSolvTechSubfigure{all-bad}%
                      [%
                        Without
                        \refEquationList{impl-cons-defs-dominate-entry-blocks,
                          impl-cons-no-span-use-defs, illegal-matches-def-locs,
                          illegal-matches-use-locs}%
                      ]%
                      [6cm]%
                      {AllBad}%

  \vspace{\betweensubfigures}

  \mkSolvTechSubfigure{all}%
                      [All solving techniques]%
                      [3.8cm]%
                      {All}%
  \hfill%
  \mkSolvTechSubfigure[best]{combos}%
                      [%
                        Eqs.\thinspace\refEquationRange*{%
                          impl-cons-defs-dominate-defs%
                        }{%
                          impl-cons-fix-fall-throughs%
                        },
                        \refEquationList*{dom-cons-null-copy-match-selection,
                          dom-cons-kill-match-selection,
                          illegal-matches-uncovered-ops,
                          illegal-matches-undefined-data, illegal-matches-kills,
                          redun-kills, redun-non-null-copy-matches},
                        dominated matches, and canonical locations%
                      ]%
                      {Combos}

  \caption[%
            Plot for evaluating the impact on solving time made by different
            combinations of solving techniques%
          ]%
          {%
            Normalized solving times (incl.\ presolving time) for five
            constraint models: one with no solving techniques (baseline), one
            with only such techniques that individually have a positive impact
            on solving time (subject), one without such techniques that
            individually have a negative impact on solving time (subject), one
            with all such techniques (subject), and another with the
            implied, symmetry and dominance breaking constraints
            and presolving techniques that were performed best in previous
            experiments (subject)%
          }
  \labelFigure{diff-solv-tech-comb-solving-time-plot}
\end{figure}

\RefFigure{diff-solv-tech-comb-solving-time-plot} shows the normalized solving
times (including \gls{presolving} time) for the five \glspl{constraint model}
described above, with \glsshort{constraint model}~\modelA{} as \gls{baseline}
and \glsplshort{constraint model}~\modelB, \modelC, \modelD, and~\modelE{} as
\glspl{subject}.
%
The solving times range from
\printMinSolvingTime{
  \SolvTechEnableOnlyAllGoodPrePlusSolvingTimeSpeedupNonePrePlusSolvingTimeAvgMin,
  \SolvTechEnableOnlyAllGoodPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMin,
  \SolvTechDisableAllBadPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMin,
  \SolvTechDisableAllPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMin,
  \SolvTechBestCombosPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMin
} to
\printMaxSolvingTime{
  \SolvTechEnableOnlyAllGoodPrePlusSolvingTimeSpeedupNonePrePlusSolvingTimeAvgMax,
  \SolvTechEnableOnlyAllGoodPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMax,
  \SolvTechDisableAllBadPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMax,
  \SolvTechDisableAllPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMax,
  \SolvTechBestCombosPrePlusSolvingTimeSpeedupPrePlusSolvingTimeAvgMax
} with a maximum coefficient of variation of
\numMaxOf{
  \SolvTechEnableOnlyAllGoodPrePlusSolvingTimeSpeedupNonePrePlusSolvingTimeCvMax,
  \SolvTechEnableOnlyAllGoodPrePlusSolvingTimeSpeedupPrePlusSolvingTimeCvMax,
  \SolvTechDisableAllBadPrePlusSolvingTimeSpeedupPrePlusSolvingTimeCvMax,
  \SolvTechDisableAllPrePlusSolvingTimeSpeedupPrePlusSolvingTimeCvMax,
  \SolvTechBestCombosPrePlusSolvingTimeSpeedupPrePlusSolvingTimeCvMax
}.
%
From the \glspl{GMI} and \glspl{CI}, also given in
\refFigure{diff-solv-tech-comb-solving-time-plot}, we observe that all
\glsplshort{constraint model} significantly improve solving time over
\glsshort{constraint model}~\modelA.
%
Hence we conclude that the solving techniques are crucial for being able to
solve anything larger than the smallest of \glspl{function}.

We also observe that, counter to our expectation, \glsshort{constraint
  model}~\modelC{} (\refFigure{solv-tech-disable-all-bad}) yields the largest
improvement over \glsshort{constraint model}~\modelA, closely followed by
\glsplshort{constraint model}~\modelD{} (\refFigure{solv-tech-disable-all}) and
\modelE{} (\refFigure{solv-tech-best-combos}), which in turn both considerably
outperform \glsshort{constraint model}~\modelB{}
(\refFigure{solv-tech-enable-only-all-good}).
%
Hence we conclude that only picking solving techniques that have a statistically
positive effect on solving time when evaluated individually is too conservative.

Moreover, because \glsshort{constraint model}~\modelE{} performs worse than both
\glsplshort{constraint model}~\modelC and~\modelD, it means that solving
techniques from one category (such as \gls{implied.c} \glspl{constraint}) have
strong synergy effects with solving techniques from another category (such as
\glsshort{symmetry breaking.c} and \gls{dominance breaking.c}
\glspl{constraint}).
%
This highlights the difficulty of accurately evaluating the full effect that a
solving technique has on the performance of a \gls{constraint model} as it may
interact with other parts of the \glsshort{constraint model} in complex and
counterintuitive ways, which is further magnified when \gls{lazy clause
  learning} is involved.

Regarding \glsshort{constraint model}~\modelC{} and \glsshort{constraint
  model}~\modelD, we observe when comparing
\refFigureList{solv-tech-disable-all-bad, solv-tech-disable-all} that both
\glsplshort{constraint model} have very similar performance characteristics (the
\glspl{GMI} are \printGMI{%
  \SolvTechDisableAllBadPrePlusSolvingTimeSpeedupPrePlusSolvingTimeRegularSpeedupGmean%
} with \gls{CI}~\printGMICI{%
  \SolvTechDisableAllBadPrePlusSolvingTimeSpeedupPrePlusSolvingTimeRegularSpeedupCiMin%
}{%
  \SolvTechDisableAllBadPrePlusSolvingTimeSpeedupPrePlusSolvingTimeRegularSpeedupCiMax%
} and \printGMI{%
  \SolvTechDisableAllPrePlusSolvingTimeSpeedupPrePlusSolvingTimeRegularSpeedupGmean%
} with \gls{CI}~\printGMICI{%
  \SolvTechDisableAllPrePlusSolvingTimeSpeedupPrePlusSolvingTimeRegularSpeedupCiMin%
}{%
  \SolvTechDisableAllPrePlusSolvingTimeSpeedupPrePlusSolvingTimeRegularSpeedupCiMax%
}, respectively).
%
While \glsshort{constraint model}~\modelD{} yields considerable solving time
improvement for one case (second from the left), it is slightly worse than
\glsshort{constraint model}~\modelC{} on several other cases, which is also
reflected in the \gls{GMI}.
%
Hence we conclude that rejecting solving techniques that have a statistically
negative effect on solving time when evaluated individually yields a
\gls{constraint model} with best performance.


\section{Summary}
\labelSection{st-summary}

In this chapter, we have introduced a wide range of techniques for solving the
\gls{constraint model} presented in the previous chapter.
%
Through experimental evaluation, the techniques were demonstrated to be
crucial for scalability and robustness.
%
Some techniques were also rejected after having been shown to have a negative
impact on solving time.
