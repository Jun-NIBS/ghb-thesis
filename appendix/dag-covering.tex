% Copyright (c) 2017, Gabriel Hjort Blindell <ghb@kth.se>
%
% This work is licensed under a Creative Commons 4.0 International License (see
% LICENSE file or visit <http://creativecommons.org/licenses/by/4.0/> for a copy
% of the license).

\chapter{DAG Covering}
\labelAppendix{dag-covering}

\todo{write introduction}

As we saw in \refAppendix{tree-covering}, the \gls{principle} of \gls{tree
  covering} has two significant disadvantages.
%
The first is that common subexpressions cannot be properly expressed in
\glspl{expression tree}, and the second is that many \glspl{machine instruction
  characteristics}---such as \glspl{multi-output instruction}---cannot be
modeled as \glspl{pattern tree}.
%
As these shortcomings are primarily due to the restricted use of
\glspl{tree}, we can achieve a more powerful approach to \gls{instruction
  selection} by extending \gls{tree covering} to \gls{DAG covering}.

This appendix is based on material presented in
\cite[Chap.\thinspace4]{HjortBlindell:2016:Survey} that has been adapted for
this dissertation.


\section{The Principle}

\todo{fix inline figure}

%\def\inparfigwidth{2.5cm}
%\begin{inParFigure}{\inparfigwidth}[r]%
%  \centering%
%  \input{\figurePath/dag-covering/dag-covering-example}%
%
%  % LAYOUT FIX:
%  % Add some space below
%  \vspace*{\baselineskip}
%\end{inParFigure}%
%\noindent
By lifting the restriction that every \gls{node} in the tree have exactly one
\gls{parent}, we get a new shape called a \gls{DAG}.
%
Because \glspl{DAG} permit
\glspl{node} to have multiple \glspl{parent}, the intermediate values in an
expression can be shared and reused within the same \gls{program DAG}.
%
This also
enables \glspl{pattern DAG} to contain multiple \gls{root} \glspl{node}, which
signify the production of multiple output values and thus extend the
\gls{instruction} support to include \glspl{multi-output instruction}.

% LAYOUT FIX:
% The in-par figure above extends into this paragraph
\begin{inParFigure}{\inparfigwidth}[r]%
  \vspace*{2\baselineskip}
\end{inParFigure}%
Since \glspl{DAG} are less restrictive compared to \glspl{tree}, transitioning
from \gls{tree covering} to \gls{DAG covering} requires new methods for solving
the problems of \gls{pattern matching} and \gls{pattern selection}.
%
\Gls{pattern
  matching} is typically addressed using one of the following methods:
\begin{itemize}
  \item First split the \glspl{pattern DAG} into \glspl{tree}, then match these
    individually, and then recombine the matched \glspl{pattern tree} into their
    original \gls{DAG} form.
%
In general, matching \glspl{tree} on \glspl{DAG} is
    NP-complete~\cite{Garey1979}, but designs applying this technique
    typically sacrifice completeness to retain linear time complexity.
  \item Match the \glspl{pattern DAG} directly using a generic \gls{subgraph
    isomorphism} algorithm.
%
Although such algorithms exhibit exponential
    worst-case time complexity, in the average case they often finish in
    polynomial time and are therefore used by several \gls{DAG covering}-based
    designs discussed in this chapter.
\end{itemize}

\Gls{optimal.ps} \gls{pattern selection} on \glspl{program DAG}, however,
does not offer the same range of choices in terms of complexity.


\section{Optimal Pattern Selection on DAGs Is NP-Complete}

The cost of the gain in generality and modeling capabilities that \glspl{DAG}
give us is a substantial increase in complexity.
%
As we saw in
\refAppendix{tree-covering}, selecting an \gls{optimal.ps} set of
\glspl{pattern} to cover a \gls{expression tree} can be done in linear time, but
doing the same for \glspl{program DAG} is an NP-complete problem.
%
Proofs
were given in 1976 by \textcite{Bruno1976} and \textcite{Aho1976b}, but
these were most concerned with the optimality of \gls{instruction scheduling}
and \gls{register allocation}.
%
In 1995, \textcite{Proebsting1995b} gave a very
concise proof for \gls{optimal instr-sel} \gls{instruction selection}, and
a longer, more detailed proof was given by \textcite{Koes2008} in~2008.
%
In this
dissertation, we will paraphrase the longer proof.


\subsection{The Proof}

The idea behind the proof is to transform the \gls{SAT problem} to an
\gls{optimal.ps}---that is, least-cost---\gls{DAG covering} problem.
%
The
\gls{SAT problem} is the task of deciding whether a Boolean formula, written in
\gls{CNF}, can be satisfied.
%
A \gls{CNF}~formula is an expression consisting of
Boolean variables and the Boolean operations $\mOr$ (\tOr) and $\mAnd$ (\tAnd)
with the following structure:
\begin{displaymath}
  (x_{1,1} \mOr x_{1,2} \mOr \ldots) \mAnd (x_{\,2,1} \mOr x_{\,2,2} \mOr \ldots) \mAnd
  \ldots
\end{displaymath}
A variable~$x$ can also be negated, written as \mbox{$\mNot x$}.

Since the \gls{SAT problem} is NP-complete, all polynomial-time
transformations from \glsshort{SAT problem} to any other problem $\mClass{P}$
must also render $\mClass{P}$ NP-complete.


\subsubsection{Modeling SAT as a Covering Problem}

First, we transform an instance~$S$ of the \gls{SAT problem} into a \gls{program
  DAG}.
%
The goal is then to find an exact cover for the \gls{DAG} in order to
deduce the truth assignment for the Boolean variables from the set of selected
\glspl{pattern}.
%
For this purpose we will use $\mOr$, $\mAnd$, $\mNot$, $v$,
$\mBox$, and $\mStop$ as \gls{node} types, and define $\mType{n}$ as the type of
a \gls{node}~$n$.
%
\Glspl{node} of type~$\mBox$ and $\mStop$ will be referred to
as \glspl{box node} and \glspl{stop node}, respectively.
%
Now, for each Boolean
variable~\mbox{$x \in S$} we create two \glspl{node}~$n_1$ and $n_2$ such that
\mbox{$\mType{n_1} = v$} and \mbox{$\mType{n_2} = \mBox$}, and add these to the
\gls{program DAG}.
%
At the same time we also add an
\gls{edge}~$\mDEdge{n_1}{n_2}$.
%
The same is done for each binary Boolean
operator~\mbox{$\mathit{op} \in S$} by creating two \glspl{node}~$n'_1$ and
$n'_2$ such that \mbox{$\mType{n'_1} = op$} and \mbox{$\mType{n'_2} = \mBox$},
along with an \gls{edge}~$\mDEdge{n'_1}{n'_2}$.
%
To model the connection between
the $\mathit{op}$ operation and its two input operands~$x$ and $y$, we add two
\glspl{edge}~$\mDEdge{n_x}{n'_1}$ and $\mDEdge{n_y}{n'_1}$ such that
\mbox{$\mType{n_x} = \mType{n_y} = \mBox$}.
%
For the unary operation~$\neg$ we
obviously only need one such \gls{edge}, and since $\mOr$ and $\mAnd$ are
commutative it does not matter in what order the \glspl{edge} are arranged with
respect to the operator \gls{node}.
%
Hence, in the resulting \gls{program DAG},
only \glspl{box node} will have more than one \gls{outgoing ed} \gls{edge}.
%
An
example of such a \gls{DAG} is shown in \refSubfigure{sat-example}, which can
be constructed in linear time simply by traversing the Boolean formula.


\subsubsection{Boolean Operations as Patterns}

%\begin{figure}[b]
%
%  \begin{subfigure}[The SAT patterns.
%%
%For brevity, the patterns for the
%                    $\mAnd$~operation are omitted (but these can be easily
%                    inferred from the $\mOr$~patterns) All patterns are
%                    assumed to have the same unit cost]%
%                   [sat-patterns]
%    \begin{minipage}{8cm}
%      % LAYOUT FIX:
%      % Make this subfigure align properly with the next subfigure
%      \vspace*{20pt}
%
%      \centering%
%      \begin{minipage}[t]{1cm}
%        \centering%
%        \input{\figurePath/dag-covering/sat-pattern-v-F}
%
%        $x: F$
%      \end{minipage}%
%      \hfill%
%      \begin{minipage}[t]{1cm}
%        \centering%
%        \input{\figurePath/dag-covering/sat-pattern-v-T}
%
%        $x: T$
%      \end{minipage}%
%      \hfill%
%      \begin{minipage}[t]{1.5cm}
%        \centering%
%        \input{\figurePath/dag-covering/sat-pattern-satisfied}
%
%        \textit{satisfied}
%      \end{minipage}%
%      \hfill%
%      \begin{minipage}[t]{1cm}
%        \centering%
%        \input{\figurePath/dag-covering/sat-pattern-not-F}
%
%        $F: \mNot T$
%      \end{minipage}%
%      \hfill%
%      \begin{minipage}[t]{1cm}
%        \centering%
%        \input{\figurePath/dag-covering/sat-pattern-not-T}
%
%        $T: \mNot F$
%      \end{minipage}
%
%      \vspace{14pt}
%
%      \begin{minipage}[t]{2cm}
%        \centering%
%        \input{\figurePath/dag-covering/sat-pattern-or-T1}
%
%        $T: T \mOr T$
%      \end{minipage}%
%      \hfill%
%      \begin{minipage}[t]{2cm}
%        \centering%
%        \input{\figurePath/dag-covering/sat-pattern-or-T2}
%
%        $T: T \mOr F$
%      \end{minipage}%
%      \hfill%
%      \begin{minipage}[t]{2cm}
%        \centering%
%        \input{\figurePath/dag-covering/sat-pattern-or-T3}
%
%        $T: F \mOr T$
%      \end{minipage}%
%      \hfill%
%      \begin{minipage}[t]{2cm}
%        \centering%
%        \input{\figurePath/dag-covering/sat-pattern-or-F}
%
%        $F: F \mOr F$
%      \end{minipage}%
%
%    \end{minipage}%
%  \end{subfigure}
%  \hfill%
%  \begin{subfigure}[Example of a SAT problem represented as a DAG
%                    covering problem][sat-example]
%    \begin{minipage}{3cm}
%      \centering%
%      \input{\figurePath/dag-covering/sat-example}
%
%      $(x_1 \mOr x_2) \mAnd (\mNot x_2)$
%    \end{minipage}
%  \end{subfigure}
%
%  \figCaption{Transforming SAT to DAG covering}[Koes2008]
%  \labelFigure{sat-to-instruction-selection-reduction}
%\end{figure}

\def\mPSat{P_\text{\smaller SAT}}%

To cover the \gls{program DAG}, we will use the \glspl{pattern tree} given in
\refSubfigure{sat-patterns}, and we will refer to this \gls{pattern set}
as~$\mPSat$.
%
 Every \gls{pattern} in $\mPSat$ adheres to the following
invariant:
\begin{enumerate}
  \item If a variable~$x$ is set to~$T$~(\tTrue), then the selected
    \gls{pattern} covering the $x$~\gls{node} will also cover the corresponding
    \gls{box node} of~$x$.
  \item If the result of an operation $\mathit{op}$ evaluates to~$F$~(\tFalse),
    then that \gls{pattern} will not cover the corresponding \gls{box node}
    of~$\mathit{op}$.
\end{enumerate}
Another way of looking at it is that an operator in a \gls{pattern}
\emph{consumes} a \gls{box node} if its corresponding value must be set to~$T$,
and \emph{produces} a \gls{box node} if the result must evaluate to~$F$.
%
Using
this scheme, we can easily deduce the truth assignments to the variables by
inspecting whether the \glspl{pattern} selected to cover the \gls{DAG} consume
the \glspl{box node} of the variables.
%
Since the only \gls{pattern} to contain a
\gls{stop node} also consumes a \gls{box node}, the entire expression will be
forced to evaluate to~$T$.

In addition to the \gls{node} types that can appear in the \gls{program DAG},
the \glspl{pattern} can also contain \glspl{node} of an additional type,
$\mAnchor$, which we will refer to as \glspl{anchor node}.
%
Let
$\mNumChildren{n}$ denote the number of \glspl{child} of~$n$, and
$\mChild{i}{n}$ the $i$th child of~$n$.
%
We now say that a \gls{pattern}~$p$,
with \gls{root} \gls{node}~$p_r$, \emph{matches} the part of a \gls{program
  DAG}~\mbox{$\mTupleTwo{N}{E}$} which is rooted at a \gls{node}~\mbox{$n \in
  N$} if and only if:
\begin{enumerate}
  \item $\mType{n} = \mType{p_r}$,
  \item $\mNumChildren{n} = \mNumChildren{p_r}$, and
  \item $\mType{\mChild{i}{n}} = \mAnchor \mOr
    \text{$\mChild{i}{n}$ matches $\mChild{i}{p_r}$}, \quantsep
    \forall 1 \leq i \leq \mNumChildren{n}$.
\end{enumerate}
In other words, the structure of the \gls{pattern tree}---which includes the
\gls{node} types and \glspl{edge}---must correspond to the structure of the
matched \gls{subgraph}, with the exception of \glspl{anchor node}, which match
any \gls{node} in the \gls{program DAG}.

We also introduce two new definitions, $\mMatchset{n}$ and $\mMatched{p, n_p}$:
for a \gls{node}~\mbox{$n \in N$} in the \gls{program DAG}~\mbox{$G =
  \mTupleTwo{N}{E}$}, $\mMatchset{n}$ is the set of \glspl{pattern} in $\mPSat$
that match at~$n$; and for a \gls{node}~\mbox{$n_p \in N_p$} in the selected
\gls{match} of pattern~\mbox{$\mTupleTwo{N_p}{E_p}$}, \mbox{$\mMatched{n, n_p}$}
is the \gls{node}~\mbox{$n \in N$} that is matched by~$n_p$.
%
Lastly, we say that
$G$ is \emph{covered} by a function~\mbox{$\mFunctionDecl{f}{N}{2^{\mPSat}}$},
which maps \glspl{node} in the \gls{program DAG} to a set of \glspl{pattern}, if
and only if, for each \mbox{$n \in N$},
\begin{enumerate}
  \item $\text{$p$ matches $n$}, \quantsep \forall p \in \mFunctionUse{f}{n}$,
  \item $\mType{n} = \mStop \implies \mFunctionUse{f}{n} \neq \emptyset$, and
  \item $\mType{n_p} = \mAnchor \implies
    \mFunctionUse{f}{\mMatched{n, n_p}} \neq \emptyset, \quantsep
    \forall p = \mTupleTwo{N_p}{E_p} \in \mFunctionUse{f}{v}, n_p \in N_p$.
\end{enumerate}
The first constraint enforces that only valid \glspl{match} are selected.
%
The
second constraint enforces that some \gls{match} has been selected to cover the
\gls{stop node}, and the third constraint enforces that \glspl{match} have been
selected to cover the rest of the \gls{DAG}.
%
An \gls{optimal.ps} cover is
thus a mapping~$\mFunctionName{f}$ which covers the \gls{program
  DAG}~\mbox{$\mTupleTwo{N}{E}$} and also minimize
\begin{displaymath}
  \sum_{n \, \in \, N} \sum_{p \, \in \, \mFunctionUse{f}{n}} \mCost{p},
\end{displaymath}
where $\mCost{p}$ is the cost of \gls{pattern}~$p$.


\subsubsection{Optimal Solution to DAG Covering $\Rightarrow$ Solution to SAT}

We now postulate that if the \gls{optimal.ps} cover has a total cost equal
to the number of non-\glspl{box node} in the~\gls{program DAG}, then the
corresponding \gls{SAT problem} is satisfiable.
%
Since all \glspl{pattern} in
$\mPSat$ cover exactly one non-\gls{box node} and have equal unit cost, the
condition above means that every \gls{node} in the \gls{DAG} is covered by
exactly one \gls{pattern}.
%
This in turn means that exactly one value will be
assumed for every Boolean variable and operator result, which is easy to deduce
through inspection of the selected \glspl{match}.

We have thereby shown that an instance of the \gls{SAT problem} can be solved by
transforming it, in polynomial time, to an instance of the \gls{optimal.ps}
\gls{DAG covering} problem.
%
Hence \gls{optimal.ps} \gls{DAG covering}---and
therefore also \gls{optimal instr-sel} \gls{instruction selection} based on
\gls{DAG covering}---is NP-complete.
%
\hfill\qedsymbol


\section{Straightforward, Greedy Techniques}

Since \gls{instruction selection} on \glspl{DAG} with \gls{optimal.ps}
\gls{pattern selection} is computationally difficult, most \glspl{instruction
  selector} based on this \gls{principle} are suboptimal.
%
One of the first
\glspl{code generator} to operate on \glspl{DAG} was developed by
\textcite{Aho1976b}.
%
In a paper from~1976, \citeauthor{Aho1976b} introduce some
simple greedy heuristics for producing \gls{assembly code} for a commutative
one-\gls{register} \gls{target machine}, but these methods assume a one-to-one
mapping between the \glspl{node} in a \gls{program DAG} and the
\glspl{instruction} and thus effectively ignore the \gls{instruction selection}
problem.


\subsection{LLVM}
\labelSection{llvm}

A more flexible, but still greedy, heuristic is applied in the well-known
\gls{LLVM} \gls{compiler} infrastructure~\cite{Lattner2004}.
%
According to a blog
entry by \textcite{Bendersky2013}\unskip% Removes unwanted space
%
---which at the time of writing provides the only documentation, except for
the source code itself---the \gls{instruction selector} is basically a greedy
\mbox{\gls{DAG}-to-\gls{DAG}} rewriter\footnote{\gls{LLVM} is also equipped with
  a ``fast'' \gls{instruction selector}, but it is implemented as a typical
  \gls{macro expander} and is only intended to be used when compiling without
  extensive \gls{program} optimization.}.

The \glspl{pattern}---which are limited to \glspl{tree}---are expressed in a
\gls{machine description} that allows common features to be factored out into
abstract \glspl{instruction}.
%
A tool called \gls{TABLEGEN} expands the abstract
\glspl{instruction} into \glspl{pattern tree}, which are then processed by a
matcher generator.
%
To ensure a partial order among all \glspl{pattern}, the
matcher generator first performs a lexicographical sort on the \gls{pattern
  set}: first by decreasing complexity, which is the sum of the \gls{pattern}'s
size and a constant that can be tweaked to give higher priority for particular
\glspl{instruction}; then by increasing cost; and lastly by increasing the size
of the \gls{subgraph} that replaces the covered part in the \gls{program DAG} if
the corresponding \gls{pattern} is selected.
%
Once sorted, the \glspl{pattern}
are converted into small recursive \glspl{program} which essentially check
whether the corresponding \gls{pattern} matches at a given \gls{node} in the
\gls{program DAG}.
%
These \glspl{program} are then compiled into a form of byte
code and assembled into a matcher table, arranged such that the lexicographical
sort is preserved.
%
The \gls{instruction selector} applies this table by simply
executing the byte code, starting with the first element.
%
When a \gls{match} is
found, the \gls{pattern} is greedily selected and the matched \gls{subgraph} is
replaced with the output (usually a single \gls{node}) of the selected
\gls{pattern}.
%
This process repeats until there are no \glspl{node} remaining in
the original \gls{program DAG}.

Although in extensive use (as of version 3.4), \gls{LLVM}'s \gls{instruction
  selector} has several drawbacks.
%
The main disadvantage is that any
\gls{pattern} that is not supported by \gls{TABLEGEN} has to be handled manually
through custom \gls{C}~functions.
%
Unlike \gls{GCC}---which applies \gls{macro
  expansion} combined with \gls{peephole optimization} (see
\refSection{macro-expansion-with-peephole-optimization})---this includes all
\glspl{multi-output instruction}, since \gls{LLVM} is restricted to
\glspl{pattern tree} only.
%
In addition, the greedy scheme compromises code
quality.


\section{Techniques Based on Exhaustive Search}

Although \gls{optimal.ps} \gls{pattern selection} can be achieved through
exhaustive search, in practice this is typically infeasible due to the
exponential number of possible combinations.
%
Nonetheless, there do exist a few
techniques that do exactly this, but they apply various techniques to prune the
search space.


\subsection{Extending Means-End Analysis to DAGs}

Twenty years after \citeauthor{Newcomer1975} and Cattell~\etal (see
\refSection{tree-covering-first-approaches}),
\citeauthor{Yu1994a}~\cite{Yu1994a, Yu1994b} rediscovered \gls{means-end
  analysis} as a method for \gls{instruction selection} and also made two major
improvements.
%
First, \citeauthor{Yu1994a}'s design supports \glsshort{program
  DAG} and \glspl{pattern DAG} whereas those by~\citeauthor{Newcomer1975} and
Cattell~\etal are both limited to \glspl{tree}.
%
Second, it combines
\gls{means-end analysis} with \gls{hierarchical planning}~\cite{Sacerdoti1973},
which is a search strategy that relies on the fact that many problems can be
arranged in a hierarchical manner for handling larger and more complex problem
instances.
%
Using hierarchical planning enables exhaustive exploration of the
search space while at the same time avoiding the situations of dead ends and
infinite looping that may occur in straightforward implementations of
\gls{means-end analysis} (\citeauthor{Newcomer1975} and Cattell~\etal both
circumvented this problem by enforcing a cut-off when a certain depth in the
search space had been reached).

Although this technique exhibits a worst time execution that is exponential in
the depth of the search, \citeauthor{Yu1994a} assert that a depth of~3 is
sufficient to yield results of equal quality to that of handwritten
\gls{assembly code}.
%
This claim notwithstanding, it is unclear whether it can be
extended to support complex \glspl{instruction} such as \glsshort{inter-block
  instruction} and \glspl{interdependent instruction}.


\subsection{Relying on Semantic-Preserving Transformations}

In 1996, \textcite{Hoover1996} developed a system called \gls{TOAST} with the
goal of automating the generation of entire \gls{compiler}
frameworks---including \gls{instruction scheduling} and \gls{register
  allocation}---from a declarative \gls{machine description}.
%
In \gls{TOAST} the
\gls{instruction selection} is done by applying semantic-preserving
transformations during \gls{pattern selection} to make better use of the
\gls{instruction set}.
%
For example, although \mbox{$x * 2$} is semantically
equivalent to \mbox{$x \ll 1$}---meaning that $x$ is arithmetically shifted one
bit to the right, which is faster than executing a multiplication---most
\glspl{instruction selector} will fail to select \glspl{instruction}
implementing the latter when the former appears in the \gls{program DAG}, as the
\glspl{pattern} are syntactically different from one another.

Their design works as follows.
%
First, the \gls{frontend} emits \glspl{program
  DAG} consisting of \glspl{semantic primitive}, a kind of \gls{IR} code also used
for describing the \glspl{instruction}.
%
The \gls{program DAG} is then
semantically matched using single-output \glspl{pattern} derived from the
\glspl{instruction}.
%
Semantic \glspl{match}---which \citeauthor{Hoover1996}
call \glspl{toe print}---and are found by a \gls{semantic comparator}.
%
The
\gls{semantic comparator} first performs syntactic matching---that is, checking
that the \glspl{node} are of the same type, which is done using a
straightforward \mbox{$\mBigO(nm)$}~algorithm---and then resorts to
semantic-preserving transformations for when syntactic matching fails.
%
To bound
the exhaustive search for all possible \glspl{toe print}, a transformation is
only applied if it will lead to a syntactic \gls{match} later on.
%
 Once all
\glspl{toe print} have been found, they are combined into \glspl{foot print},
which correspond to the full effects of an \gls{instruction}.
%
A \gls{foot print}
can consist of just a single \gls{toe print} (as with \glspl{single-output
  instruction}) or several (as with \glspl{multi-output instruction}), but the
paper lacks details on how this is done exactly.
%
Lastly, all combinations of
\glspl{foot print} are considered in pursuit of the one leading to the most
effective implementation of the \gls{program DAG}.
%
To further prune the search
space, this process only considers combinations where each selected \gls{foot
  print} syntactically matches at least one \gls{semantic primitive} in the
\gls{program DAG}, and only ``trivial amounts'' of the \gls{program DAG} (for
example constants) may be included in more than one \gls{foot print}.

Using a prototype implementation, \citeauthor{Hoover1996} reported that almost
$10^{70}$~``implied \gls{instruction} \glspl{match}'' were found for one of the
test cases, but it is unclear how many of them were actually useful.
%
Moreover,
in its current form the design appears to be unpractical for generating
\gls{assembly code} for all but very small \glspl{program}.


\section{Extending Tree Covering Techniques to DAGs}

Another common approach to \gls{DAG covering} is to reuse already-known,
linear-time methods from \gls{tree covering}.
%
This can be achieved either by
transforming the \glspl{program DAG} into \glspl{tree}, or by generalizing the
\gls{tree}-based algorithms for \gls{pattern matching} and \gls{pattern
  selection}.
%
We begin by discussing designs that apply the first technique.


\subsection{Undagging Program DAGs}

The simplest approach for reusing \gls{tree covering} techniques is to transform
the \gls{program DAG} into several \glspl{expression tree}.
%
We will refer to this
idea as \gls{undagging}.

As illustrated in \refFigure{undagging-example}, a \gls{program DAG} can be
\glsshort{undagging}[ged] into \glspl{expression tree} in two ways.
%
The first
approach is to split the edges involving \glspl{shared node}---these are
\glspl{node} where reuse occurs due to the presence of common
subexpressions---which results in a set of disconnected \glspl{expression tree}
that can then be covered individually.
%
Not surprisingly, this approach is called
\gls{edge splitting}.
%
An implicit connection between the \glspl{expression tree} is
maintained by forcing the values computed at the \glspl{shared node} to be
stored and read from a specific location, typically in memory.
%
An example of
such an implementation is \gls{DAGON}, a technology binder developed by
\textcite{Keutzer1987}, which maps technology-independent descriptions onto
circuits.
%
The second approach is to duplicate the \glspl{node} involved in
computing the shared value, which is known as \gls{node duplication}.
%
This
results in a single but larger \gls{expression tree} compared to those produced
with \gls{edge splitting}.

%\begin{figure}[t]
%  \centering%
%  \begin{subfigure}[DAG][undagging-example-dag]
%    \begin{minipage}{2.5cm}
%      \centering%
%      \input{\figurePath/dag-covering/undagging-1}
%    \end{minipage}%
%  \end{subfigure}
%  \hfill%
%  \begin{subfigure}[Result of edge splitting][undagging-example-splitting]
%    \begin{minipage}{4.3cm}
%      % LAYOUT FIX:
%      % Make this subfigure align properly with the previous subfigure
%      \vspace*{18pt}
%
%      \centering%
%      \input{\figurePath/dag-covering/undagging-2}
%    \end{minipage}%
%  \end{subfigure}
%  \hfill%
%  \begin{subfigure}[Result of node duplication][undagging-example-duplication]
%    \begin{minipage}{4.5cm}
%      % LAYOUT FIX:
%      % Make this subfigure align properly with the previous subfigure
%      \vspace*{4pt}
%
%      \centering%
%      \input{\figurePath/dag-covering/undagging-3}
%    \end{minipage}
%  \end{subfigure}
%
%  \figCaption{Undagging a \glsentrytext{program DAG} with a common
%    subexpression}
%  \labelFigure{undagging-example}
%\end{figure}

Common for both schemes is that they compromise code quality: too aggressive
\gls{edge splitting} produces many small \glspl{tree} that cannot be covered
using larger \glspl{pattern}, inhibiting use of more efficient
\glspl{instruction}; and too aggressive \gls{node duplication} incurs a larger
computational workload where many operations are needlessly re-executed in the
final \gls{assembly code}.
%
Moreover, the intermediate results of an edge-split
\gls{program DAG} must be forcibly stored in specific locations, which can be
troublesome for heterogeneous memory-\gls{register} architectures (this
particular problem was studied by \textcite{Araujo1996}).


\subsubsection{Balancing Splitting and Duplication}

In 1994, \citeauthor{Fauth1994}~\cite{Fauth1994, Muller1994} developed a
technique that tries to mitigate the deficiencies of \gls{undagging} by
balancing the use of \gls{node duplication} and \gls{edge
  splitting}.
%
Implemented in the \gls{CBC}, the \gls{instruction selector}
applies a heuristic algorithm that first favors \gls{node duplication}, and
resorts to \gls{edge splitting} when the former is deemed too costly.
%
The
decision about whether to duplicate or to split is taken by comparing the cost
of the two solutions and selecting the cheapest one.
%
The cost is calculated as a
weighted sum \mbox{$w_1 n_{\text{dup}} + w_2 n_{\text{split}}$}, where
$n_{\text{dup}}$ is the number of \glspl{node} in the \gls{program DAG} (a rough
estimate of code size), and $n_{\text{split}}$ is the expected number of
\glspl{node} executed along each execution path (a rough estimate of execution
time).
%
Once this is done, each resulting \gls{expression tree} is covered by an
improved version of \gls{IBURG} (see \refSection{iburg}) with extended match
condition support.
%
However, the experimental data is too limited for us to judge
how efficient this technique is compared to a design where the \glspl{program
  DAG} have been transformed into \glspl{expression tree} using just one method.


\subsubsection{Register-Sensitive Instruction Selection}

In 2001, \textcite{Sarkar2001} developed an \gls{instruction selection}
technique that attempts to reduce the \gls{register pressure}---that is, the
number of \glspl{register} needed by the \gls{program}---in order to facilitate
\gls{register allocation}\footnote{Another register-aware \gls{instruction
    selection} technique was developed in 2014 by \textcite{Xie2014}, with the
  aim of reducing the number of writes to a nonvolatile \gls{register}
  file.
%
However, the \glspl{instruction} are selected using a proprietary and
  greedy heuristic hat does not warrant in-depth discussion.}.

The design works as follows.
%
The \gls{program DAG} is first augmented with
additional \glspl{edge} to signify scheduling dependencies between memory
operations, and then it is split into a several \glspl{expression tree} using a
heuristic to decide which \glspl{edge} to break.
%
The \glspl{expression tree} are
then covered individually using conventional methods based on \gls{tree
  covering}, but instead of being the usual number of execution cycles, the cost
of each \gls{instruction} is set so as to reflect the amount of \gls{register
  pressure} incurred by that instruction (unfortunately, the paper lacks details
on how these costs are computed exactly).
%
Once \glspl{pattern} have been
selected, the \glspl{node} which are covered by the same \gls{pattern} are
merged into \glspl{super node}.
%
The resulting \gls{graph} is then checked for
whether it contains any \glspl{cycle}, which may appear due to the extra data
dependencies that were added at the earlier stage.
%
If it does, it means that
there exist cyclic scheduling dependencies between two or more memory
operations, making it an illegal cover.
%
The splits are then reverted and the
process repeats until a legal cover is found.

\citeauthor{Sarkar2001} implemented their \gls{register}-sensitive design in
\gls{Jalapeno}, a \gls{register}-based \gls{Java} virtual machine developed by
\gls{IBM}.
%
For a small set of problems the performance increased by about 10\%,
which \citeauthor{Sarkar2001} claim to be due to fewer \glspl{instruction}
needed for \gls{register spilling} compared to the default \gls{instruction
  selector}.
%
Although innovative, it is doubtful that the technique can be
extended much further.


\subsection{Extending the Dynamic Programming Approach to DAGs}

To avoid the application of ad hoc heuristics, several \gls{DAG}-based
\glspl{instruction selector} perform \gls{pattern selection} by applying an
extension of the \gls{tree}-based \tDPalgorithm originally developed by
\textcite{Aho1976a} (see \refSection{prehistory}).
%
According to the literature,
\combinedTextcite{Liem1994, Paulin1994, Paulin1995} appear to have been the
first to have done so.

In a seminal paper from~1994, \citeauthor{Liem1994} introduce a design which is
part of \gls{CodeSyn}, a well-known code synthesis system, which in turn is part
of a development environment for embedded systems called \gls{FlexWare}.
%
For
\gls{pattern matching}, \citeauthor{Liem1994} applied the same technique as
\textcite{Weingart1973} (see
\refSection{first-techniques-to-use-tree-pattern-matching}) by combining all
available \glspl{pattern tree} into a single \gls{tree} of \glspl{pattern}.
%
This
\gls{pattern tree} is traversed in tandem with the \gls{program DAG}, and for
each \gls{node} an \mbox{$\mBigO(nm)$} \gls{pattern matcher} is used to find all
\glspl{match set}.
%
\Gls{pattern selection} is then performed using an extended
version of the \tDPalgorithm, but the paper does not explain how this is done
exactly.
%
Moreover, the algorithm is only applied on the data flow of the
\gls{program DAG}---control flow is covered separately using a simple
heuristic---and no guarantees are made that the \gls{pattern selection} is
\gls{optimal.ps}, as that is an NP-complete problem.


\subsubsection{Potentially Optimal Pattern Selection}

In a paper from~1999, \textcite{Ertl1999} introduces a design which guarantees
\gls{optimal.ps} \gls{pattern selection} on \glspl{program DAG} for certain
\glspl{machine grammar}.
%
The idea is to first make a bottom-up pass over
the \gls{program DAG} and compute the costs using the conventional \tDPalgorithm
as discussed in \refAppendix{tree-covering}.
%
Each \gls{node} is thus labeled with
the same costs, as if the \gls{program DAG} had first been transformed into a
\gls{tree} through \gls{node} duplication; but \citeauthor{Ertl1999} recognized
that if several \glspl{pattern} reduce the same \gls{node} to the same
\gls{nonterminal}, then the reduction to that \gls{nonterminal} can be shared
between several \glspl{rule} whose \glspl{pattern} contain the
\gls{nonterminal}.
%
 Hence the \glspl{instruction} for implementing shared
\glspl{nonterminal} only need to be emitted once, decreasing code size and also
improving performance, since the amount of redundant computation is
reduced.
%
With appropriate data structures, a linear-time implementation can be
achieved.

%\begin{figure}[b]
%  \centering
%
%  \begin{minipage}{3.5cm}
%    \input{\figurePath/dag-covering/ertl-example}
%  \end{minipage}
%  \hfill%
%  \begin{minipage}{7.5cm}
%    \figCaption[Sharing nonterminals of \glsentrytext{tree covering} on a
%        \glsentrytext{program DAG}]%
%      {A \glsentrytext{tree covering} of a \glsentrytext{program DAG} where the
%        patterns have been selected optimally.
%%
%The two shades indicate the
%        relation between rules, and the text along the edges indicates the
%        nonterminals to which each pattern is reduced.
%%
%Note that the
%        \gT{Reg}~\gls{node} is covered by two patterns (as indicated by the
%        double dash pattern) which both reduce to the same nonterminal and can
%        thus be shared}[Ertl1999]
%    \labelFigure{ertl-example}
%  \end{minipage}
%\end{figure}

An example illustrating such a situation is given in \refFigure{ertl-example},
where we see an addition that will have to be implemented twice, as its
\gls{node} is covered by two separate \glspl{pattern} each of which reduces the
\gls{subtree} to a different \gls{nonterminal}.
%
The \gT{Reg}~\gls{node}, on the
other hand, is reduced twice to the same \gls{nonterminal} (\gNT{reg}), and can
thus be shared between the \glspl{rule} that use this \gls{nonterminal} in the
\glspl{pattern}.

As said earlier, however, this technique yields \gls{optimal.ps} \gls{pattern
  selection} only for certain \glspl{machine grammar}.
%
\citeauthor{Ertl1999} therefore devised a checker, called
\gls{DBURG}, that detects when the \gls{grammar} does not belong into this
category and thus cannot guarantee optimality.
%
The basic idea is to check
whether every locally \gls{optimal.ps} decision is also globally
\gls{optimal.ps} by performing inductive proofs over the set of all
possible \glspl{program DAG}.
%
To do this efficiently, \citeauthor{Ertl1999}
implemented \gls{DBURG} using the ideas behind \gls{BURG} (hence the name).


\subsubsection{Combining DP and Edge Splitting}

\def\overlapCost{overlap-cost\xspace}
\def\cseCost{cse-cost\xspace}

\textcite{Koes2008} extended \citeauthor{Ertl1999}'s ideas by providing a
heuristic that splits the \gls{program DAG} at points where \gls{node
  duplication} is estimated to have a detrimental effect on code quality.
%
Like
\citeauthor{Ertl1999}'s algorithm, \citeauthor{Koes2008}'s first selects
\glspl{pattern} optimally by performing a \gls{tree}-like, bottom-up \tDPpass
which ignores the fact that the input is a~\gls{DAG}.
%
Then, at points where
multiple \glspl{pattern} overlap, two costs are calculated: an
\emph{\overlapCost} and a \emph{\cseCost}.
%
The \overlapCost is an estimate of
the cost of letting the \glspl{pattern} overlap and thus incur duplication of
operations in the final \gls{assembly code}.
%
The \cseCost is an estimate of the
cost of splitting the \glspl{edge} at such points.
%
If \cseCost is lower than
\overlapCost, then the \gls{node} where overlapping occurs is marked as
\gls{fixed nod}.
%
Once all such \glspl{node} have been processed, a second
bottom-up \tDPpass is performed on the \gls{program DAG}, but this time no
\glspl{pattern} are allowed to span across \gls{fixed nod} \glspl{node}, which
can only be matched at the \gls{root} of a \gls{pattern}.
%
Lastly, a top-down
pass emits the \gls{assembly code}.

For evaluation purposes \citeauthor{Koes2008} compared their own implementation,
called \gls{NOLTIS}, against an implementation based on \glsdesc{IP}---we will
discuss such techniques later in this chapter---and found that \gls{NOLTIS}
achieved \gls{optimal.ps} \gls{pattern selection} in 99.7\% of the test
cases.
%
More details are given in \citeauthor{Koes2009}'s doctoral
dissertation~\cite{Koes2009}.
%
But like \citeauthor{Ertl1999}'s design,
\citeauthor{Koes2008}'s is limited to \glspl{pattern tree} and thus cannot
support more complex \glspl{instruction} such as \glspl{multi-output
  instruction}.


\subsubsection{Supporting Multi-output Instructions}

In most \gls{instruction selection} techniques based on \gls{DAG covering}, it
is assumed that the outputs of a \gls{pattern DAG} always occur at the
\gls{root}~\glspl{node}.
%
But in a design by \combinedTextcite{Arnold1999b,
  Arnold2001} (originally introduced in a technical report by
\textcite{Arnold1999a}), the \glspl{node} representing output can be marked
explicitly.
%
The advantage of this is that it allows the \glspl{pattern DAG} to
be fully decomposed into \glspl{tree} such that each output value receives its
own \gls{pattern tree}, which \citeauthor{Arnold1999b} call
\tpartialPatterns.
%
An example is given in \refFigure{arnold-example}.

%\begin{figure}[t]
%  \centering%
%  \begin{subfigure}[Original \glsentrytext{pattern DAG}][arnold-example-dag]
%    \begin{minipage}{2.5cm}%
%      \centering%
%      \input{\figurePath/dag-covering/arnold-example-dag}%
%    \end{minipage}%
%  \end{subfigure}
%  \hspace{40pt}%
%  \begin{subfigure}[Corresponding partial \glsentrytext{pattern tree}s]%
%    [arnold-example-trees]
%    \begin{minipage}{4.5cm}%
%      % LAYOUT FIX:
%      % Make this subfigure align properly with the previous subfigure
%      \vspace*{6pt}
%
%      \centering%
%      \begin{minipage}{1cm}%
%        \centering%
%        \input{\figurePath/dag-covering/arnold-example-tree1}%
%      \end{minipage}%
%      \hfill%
%      \begin{minipage}{2cm}%
%        \centering%
%        \input{\figurePath/dag-covering/arnold-example-tree2}%
%      \end{minipage}%
%    \end{minipage}
%  \end{subfigure}
%
%  \figCaption[Converting a \glsentrytext{pattern DAG} into partial (tree)
%    patterns]%
%    {Converting a \glsentrytext{pattern DAG}, which represents an
%      \dataTerm{add} instruction
%      that also sets a status flag if the result is equal to~0, into partial
%      pattern trees.
%%
%The darkly shaded nodes indicate the output nodes}
%  \labelFigure{arnold-example}
%\end{figure}

The \tpartialPatterns are then matched over the \gls{program DAG} using an
\mbox{$\mBigO(nm)$} algorithm.
%
After matching, another algorithm attempts to
merge appropriate combinations of partial \glspl{match} into \glspl{match} of
the original \gls{pattern DAG}.
%
This is done in a straightforward manner by
maintaining, for each \gls{match}, an array that maps the \glspl{node} in the
\gls{pattern DAG} to the covered \glspl{node} in the \gls{program DAG}, and then
checking whether two \tpartialPatterns belong to the same original \gls{pattern
  DAG} and have compatible mappings.
%
This means that no pair of \gls{pattern}
\glspl{node} belonging to different \tpartialPatterns but corresponding to the
same \gls{node} in the original \gls{pattern DAG} may cover different
\glspl{node} in the \gls{program DAG}.
%
For \gls{pattern selection}
\citeauthor{Arnold1999b} applied a variant of the \tDPscheme but combined it
with a greedy heuristic in order to enforce that each \gls{node} is covered
exactly once.
%
Hence code quality is compromised.


\section{Transforming Pattern Selection to an M(W)IS Problem}
\labelSection{maximum-independent-set}

In the techniques discussed so far, the \gls{instruction selector} operates
directly on the \gls{program DAG} when performing \gls{pattern selection}.
%
The
same applies for most designs based on \gls{tree covering}.
%
But another approach
is to indirectly solve the \gls{pattern selection} problem by first transforming
it into an instance of some other problem for which there already exist
efficient solving methods.
%
When that problem has been solved, the answer can be
translated back into a solution for the original \gls{pattern selection}
problem.

%\begin{figure}[t]
%  \centering%
%  \begin{subfigure}[Matched DAG][mis-example-dag]
%    \begin{minipage}{3.3cm}
%      \centering%
%      \input{\figurePath/dag-covering/mis-example-dag}%
%    \end{minipage}%
%  \end{subfigure}
%  \hspace{40pt}%
%  \begin{subfigure}[Conflict graph][mis-example-conflict]
%    \begin{minipage}{3cm}
%      \centering%
%      \input{\figurePath/dag-covering/mis-example-conflict}%
%    \end{minipage}
%  \end{subfigure}
%
%  \figCaption[A conflict graph example]%
%    {Example of a pattern-matched \gls{program DAG} and its corresponding
%      conflict graph}
%  \labelFigure{mis-example}
%\end{figure}

One such problem is the \gls{MIS problem}, where the task is to select the
largest set of \glspl{node} from a \gls{graph} such that no pairs of selected
\glspl{node} have an \gls{edge} between them.
%
In the general case, finding such
a solution is NP-complete~\cite{Garey1979}, and the \gls{pattern
  selection} problem is transformed into an \gls{MIS problem} as follows.
%
From
the \glspl{match set} found by \gls{pattern matching}, a corresponding
\gls{conflict graph}---or \gls{interference graph}, as it is sometimes
called---is formed.
%
Each \gls{node} in the \gls{conflict graph} represents a
\gls{match}, and there exists an \gls{edge} between two \glspl{node} if and only
if the corresponding \glspl{match} overlap.
%
An example of this is given in
\refFigure{mis-example}.
%
By solving the \gls{MIS problem} for the \gls{conflict
  graph}, we obtain a selection of \glspl{match} such that every \gls{node} in
the \gls{program DAG} is covered by exactly one \gls{match}.

But a solution to the \gls{MIS problem} does not necessarily yield an
\gls{optimal.ps} solution to the \gls{pattern selection} problem, as the
former does not incorporate costs.
%
We address this limitation by transforming
the \gls{MIS problem} into a \gls{MWIS problem}, where the task is to find a
solution to the \gls{MIS problem} that maximizes \mbox{$\sum_{p} \mWeight{p}$},
and assign as weights the costs of the \glspl{pattern}.
%
We can get the solution
with minimal total cost simply by negating the weights.
%
Note that although the
\glsshort{MWIS problem}-based techniques discussed in this dissertation have all been
limited to \glspl{program DAG}, the approach can just as well be applied in
\gls{graph covering}, which will be introduced in \refAppendix{graph-covering}.


\subsection{Applications}

In 2007, \textcite{Scharwaechter2007} introduced what appears to be the first
\gls{instruction selection} technique to use the \glsshort{MWIS problem}
approach for selecting \glspl{pattern}.
%
But despite this novelty, the most cited contribution of their design is its
extensions to \glspl{machine grammar} to support \glspl{multi-output
  instruction}.


\subsubsection{Machine Grammars with Multiple Left-Hand Side Nonterminals}

\labelPage{rules-multiple-productions}

To begin with, \citeauthor{Scharwaechter2007} distinguishes between \glspl{rule}
having only one left-hand side \gls{nonterminal} in their \glspl{production}
from \glspl{rule} containing multiple left-hand side \glspl{nonterminal} by
referring to them as \glspl{rule} and \tcomplexRules, respectively.
%
In addition, we will refer to the right-hand side \glspl{symbol}
appearing in \glspl{rule} as \tsimplePatterns, and to the right-hand side
\glspl{symbol} appearing in \tcomplexRules as \tsplitPatterns\footnote{In their
  paper, \citeauthor{Scharwaechter2007} call these \tsimpleRules and
  \tsplitRules, respectively, but to conform with the terminology established
  on \refPage{rule-terms}, I chose to refer to them as \glspl{pattern}.}.
%
A
combination of \tsplitPatterns is thus known as a \tcomplexPattern.
%
The
aforementioned terms are illustrated more clearly below:
%
\begingroup%
\def\tPadded#1{%
  \vbox to 1.5\baselineskip {%
    \vfil%
    \hbox{#1}%
    \vfil%
  }%
}%
\def\tPadded#1{%
  \vbox to 1.5\baselineskip {%
    \vfil%
    \hbox{#1}%
    \vfil%
  }%
}%
\NewDocumentCommand{\pgT}{mo}{%
  \tPadded{\text{%
    \IfValueTF{#2}{%
      \gT{#1}[#2]%
    }{%
      \gT{#1}%
    }%
  }}%
}
\NewDocumentCommand{\pgNT}{mo}{%
  \tPadded{\text{%
    \IfValueTF{#2}{%
      \gNT{#1}[#2]%
    }{%
      \gNT{#1}%
    }%
  }}%
}
\NewDocumentCommand{\pgA}{mo}{%
  \tPadded{\text{%
    \IfValueTF{#2}{%
      \gA{#1}[#2]%
    }{%
      \gA{#1}%
    }%
  }}%
}
\def\tText#1{\text{\figureFontStyle#1}}%
\begin{displaymath}
  \boolfalse{TinRunningText}%
  \small%
  \underbrace{
    \pgNT{nt} \; \tPadded{$\rightarrow$} \;
    \overbrace{%
      \pgT{Op} \; \pgT{I}[1] \; \tPadded{\ldots}%
    }^{\tText{simple pattern}}
    \qquad
    \tPadded{\tText{cost}}
    \qquad
    \tPadded{\tText{action}}
  }_{\tText{rule}}
\end{displaymath}
\begin{displaymath}
  \boolfalse{TinRunningText}%
  \small%
  \underbrace{
    \pgNT{nt}[1] \; \pgNT{nt}[2] \; \tPadded{\ldots} \;
    \tPadded{$\rightarrow$} \;
    \overbrace{%
      \overbrace{%
        \pgT{Op}[1] \; \pgT{I}[1,1] \; \tPadded{\ldots}%
      }^{\tText{split pattern}}
      \tPadded{\raisebox{0pt}[1.5ex]{\tText{,}}} \;
      \overbrace{%
        \pgT{Op}[2] \; \pgT{I}[2,1] \; \tPadded{\ldots}%
      }^{\tText{split pattern}}
      \tPadded{\raisebox{0pt}[1.5ex]{\tText{,}}} \; \tPadded{\ldots}
    }^{\tText{complex pattern}}
    \qquad
    \tPadded{\tText{cost}}
    \qquad
    \tPadded{\tText{action}}
  }_{\text{complex rule}}
\end{displaymath}%
\endgroup

\Gls{pattern matching} is a two-step process.
%
First, the \glspl{match set} are
found for the \glsshort{simple pattern} and \tsplitPatterns, using conventional
\gls{tree}-based \gls{pattern matching} techniques.
%
Second, the \glspl{match set}
for the \tcomplexPatterns are found by combining the \glspl{match} of
\tsplitPatterns into \glspl{match} of \tcomplexPatterns where appropriate.
%
The
\gls{pattern selector} then checks whether it is worth applying a
\tcomplexPattern for covering a certain set of \glspl{node}, or if they should
be covered using the \tsimplePatterns instead.
%
Since the intermediate results of
\glspl{node} within \tcomplexPatterns cannot be reused for other
\glspl{pattern}, selecting a \tcomplexPattern can incur an additional overhead
cost as \glspl{node} in the \gls{program DAG} may need to be covered using
multiple \glspl{pattern}.
%
Consequently, a \tcomplexPattern will only be selected
if the cost reduced by replacing a set of \tsimplePatterns with this
\gls{pattern} is greater than the cost incurred by code duplication.
%
After these
decisions have been taken, the next step is to perform \gls{pattern
  selection}.
%
For this, \citeauthor{Scharwaechter2007} solve the corresponding
\gls{MWIS problem} in order to limit solutions to those of exact covering
only.
%
The weights are calculated as the negated sum of the \tsplitPattern costs,
but the paper is ambiguous on how these costs are calculated.
%
Since the
\gls{MWIS problem} is known to be NP-complete,
\citeauthor{Scharwaechter2007} employed a greedy heuristic called \gls{Gwmin2}
by \textcite{Sakai2003}.
%
Lastly, \tsplitPatterns which have not been merged into
\tcomplexPatterns are replaced by corresponding \tsimplePatterns before
\gls{assembly code} emission.

\citeauthor{Scharwaechter2007} implemented a prototype called \gls{CBURG} as an
extension of \gls{OLIVE} (see \refSection{olive}), and then ran some experiments
by targeting a \gls{MIPS}-like architecture.
%
In these experiments \gls{CBURG}
generated \gls{assembly code} which improved performance by almost 25\%, and
reduced code size by nearly 22\%, compared to \gls{assembly code} which was only
allowed to make use of \glspl{single-output instruction}.
%
Measurements of
\gls{CBURG} also indicate that this technique exhibits near-linear time
complexity.
%
\textcite{Ahn2009} later broadened this work by including scheduling
dependency conflicts between \tcomplexPatterns, and incorporating a feedback
loop with the \gls{register allocator} to facilitate \gls{register allocation}.

A shortcoming of both designs by \citeauthor{Scharwaechter2007} and
\citeauthor{Ahn2009} is that \tcomplexRules can only consist of disconnected
\glspl{pattern tree} (hence there can be no sharing of \glspl{node} between the
\tsplitPatterns).
%
\textcite{Youn2011} address this problem in a
2011~paper---which is a revised and extended version of the original paper by
\citeauthor{Scharwaechter2007}\unskip% Removes unwanted space
%
---by introducing index subscripts for the operand specification of the
\tcomplexRules; but the subscripts are restricted to the input \glspl{node} of
the \gls{pattern}, still hindering support for completely arbitrary
\glspl{pattern DAG}.


\subsubsection{Targeting Machines with Echo Instructions}

In 2004, \textcite{Brisk2004} introduced a technique to perform \gls{instruction
  selection} for \glspl{target machine} with special \glspl{echo instruction},
which are small markers that refer back to an earlier portion in the
\gls{assembly code} for re-execution.
%
This allows the \gls{assembly code} to be
compressed by basically using the same idea that is applied in the LZ77
algorithm~\cite{Ziv1977}\footnote{The algorithm performs string compression by
  replacing recurring substrings that appear earlier in the string with
  pointers, allowing the original string to be reconstructed by
  ``copy-pasting.''}.
%
Since \glspl{echo instruction} do not incur a branch or a
procedure call, the \gls{assembly code} can be reduced in size without
sacrificing performance.
%
Consequently, unlike for traditional \glspl{target
  machine}, the \gls{pattern set} is not fixed in this case but must be
determined as a precursor to \gls{pattern matching} (this is closely related to
the \gls{ISE problem}, which we will discuss in \refAppendix{conclusions}).

The intuition behind this design is to use \glspl{echo instruction} where code
duplication is most prominent.
%
To find these cases in a given \gls{program},
\citeauthor{Brisk2004} first enumerate all \glspl{subgraph} from the
\glspl{program DAG}, and then match each \gls{subgraph} over the \glspl{program
  DAG}.
%
\Gls{pattern matching} is done using \gls{VF2}, which is a generic
\gls{subgraph isomorphism} algorithm that we will describe in
\refAppendix{graph-covering}.
%
Summing the sizes of the resulting \glspl{match set}
gives a measure of code duplication for each \gls{subgraph}, but this value will
be an overestimation as the \glspl{match set} may contain overlapping
\glspl{match}.
%
\citeauthor{Brisk2004} addressed this by first solving the
\gls{MIS problem} on the \gls{conflict graph} for each \gls{match set}, and then
adding up the sizes of \emph{these} sets.
%
After selecting the most beneficial
\gls{subgraph}, the covered \glspl{node} in the \glspl{program DAG} are
collapsed into single \glspl{node} to reflect the use of \glspl{echo
  instruction}.
%
This process of matching and collapsing is then repeated until
no new \gls{subgraph} better than some user-defined value criterion can be
found.
%
\citeauthor{Brisk2004} performed experiments on a prototype using a
selected set of benchmark applications, which showed code size reductions of
25\% to 36\% on average.


\section{Transforming Pattern Selection to a Unate/Binate Covering Problem}

Another approach to solving \gls{pattern selection} is to translate it to a
corresponding \typesetNewTerm{\glsshort{unate covering}} or \gls{binate
  covering} problem.
%
The concepts behind the two are identical with the
exception of one detail, and both \glsshort{unate covering} and \gls{binate
  covering} can be used directly for covering \glspl{graph} even though the
designs discussed in this dissertation have only been applied on \glspl{program DAG}.

Although \gls{binate covering}-based techniques actually appeared first, we will
begin with explaining \gls{unate covering}, as \gls{binate covering} is an
extension of \gls{unate covering}.


\subsubsection{Unate Covering}

The idea of \gls{unate covering} is to create a Boolean matrix~$\mMatrix{M}$,
where each row represents a \gls{node} in the \gls{program DAG} and each column
represents a \gls{match} covering one or more \glspl{node} in the \gls{program
  DAG}.
%
If we denote $m_{ij}$ as row~$i$ and column~$j$ in~$\mMatrix{M}$, then
\mbox{$m_{ij} = 1$} indicates that \gls{node}~$i$ is covered by
\gls{pattern}~$j$.
%
Hence the \gls{pattern selection} problem is equivalent to
finding a valid configuration of $\mMatrix{M}$ such that the sum of every row is
at least~1.
%
An example is given in
\refFigure{unate-covering-example}.
%
\Gls{unate covering} is an NP-complete
problem, but as with the \glsshort{MIS problem} and \glspl{MWIS problem} there
exist several efficient techniques for solving it heuristically (see for
example~\cite{Cordone2000, Goldberg2006} for an overview).

%\begin{figure}[b]
%  \centering%
%
%  \begin{subfigure}[Graph to cover][unate-covering-example-graph]
%    \begin{minipage}{6cm}%
%      \centering%
%      \input{\figurePath/graph-covering/unate-covering-example-graph}%
%    \end{minipage}%
%  \end{subfigure}
%  \hfill%
%  \begin{subfigure}[Boolean matrix][unate-covering-example-matrix]
%    \begin{minipage}{4cm}%
%      \centering%
%      \begin{framedBoxWI}{\textwidth}
%        \centering%
%        \begin{tabular}{c@{~~}|@{~~}ccccccc}
%                & \tT{p}[1] & \tT{p}[2] & \tT{p}[3] & \tT{p}[4] & \tT{p}[5]
%                & \tT{p}[6] & \tT{p}[7]\\
%          \hline
%          % LAYOUT FIX: prevent first row from touching the line above
%          \rule{0pt}{2ex}%
%          \tT{n}[1] & 1* & 1* &  0 &  0 &  0 &  0 &  0\\
%          \tT{n}[2] &  0 & 1* &  1 &  0 &  0 &  0 &  0\\
%          \tT{n}[3] &  0 &  0 &  1 & 1* &  0 &  0 &  0\\
%          \tT{n}[4] &  0 &  0 &  0 & 1* &  1 &  1 &  0\\
%          \tT{n}[5] &  0 &  0 &  0 &  0 &  0 &  0 & 1*\\
%        \end{tabular}
%      \end{framedBoxWI}
%    \end{minipage}
%  \end{subfigure}
%
%  \figCaption[A unate covering example]%
%    {Example of unate covering.
%%
%Unmarked 1s in the matrix represent potential
%      but not selected covers, while the 1s marked with a star (1*) indicate a
%      selection that is optimal (assuming all patterns have the same cost)}
%  \labelFigure{unate-covering-example}
%\end{figure}

\Gls{unate covering} alone, however, does not incorporate all necessary
constraints of \gls{pattern selection} since some \glspl{pattern} require---and
prevent---the selection of other \glspl{pattern} in order to yield correct
\gls{assembly code}.
%
For example, assume that \gls{pattern}~\tT{p}[3] in
\refSubfigure{unate-covering-example-graph} requires that
\gls{pattern}~\tT{p}[6] be selected to cover \tT{n}[4] instead of
\gls{pattern}~\tT{p}[5].
%
Using \glspl{machine grammar} this can be enforced with the appropriate use of
\glspl{nonterminal}, but for \gls{unate covering} we have no means of expressing
this constraint.
%
We therefore turn to
\gls{binate covering}, where this is possible.


\subsubsection{Binate Covering}

We first rewrite the Boolean matrix from the \gls{unate covering} problem into
Boolean formulas consisting of conjunctions of non-negated disjunctions.
%
The
Boolean matrix in \refSubfigure{unate-covering-example-matrix} can thus be
rewritten as
\begin{displaymath}
  f =
  (\mT{p}[1] \mOr \mT{p}[2]) \mAnd (\mT{p}[2] \mOr \mT{p}[3]) \mAnd
  (\mT{p}[3] \mOr \mT{p}[4]) \mAnd (\mT{p}[4] \mOr \mT{p}[5] \mOr \mT{p}[6])
  \mAnd \mT{p}[7].
\end{displaymath}
Now, the difference between \gls{unate covering} and \gls{binate covering} is
that all variables must be non-negated in the former, but may be negated in the
latter.
%
Hence, the aforementioned constraint regarding the compulsory selection
of \tT{p}[6] if \tT{p}[4] is selected can now be expressed as
\begin{displaymath}
  \mCompT{p}[4] \mOr \mT{p}[6],
\end{displaymath}
which is called an \gls{implication clause} as it is logically equivalent to
\mbox{$\mT{p}[4] \implies \mT{p}[6]$}.
%
This is then simply appended to the
Boolean formula~$f$ using the $\mAnd$ operator.


\subsubsection{Applications}

According to \combinedTextcite{Liao1995, Liao1998} and \textcite{Cong2004}, the
pioneering use of \gls{binate covering} to solve \gls{DAG covering} was done by
\textcite{Rudell1989} in 1989 as a part of a \gls{VLSI} synthesis
design.
%
\citeauthor{Liao1995}~\cite{Liao1995, Liao1998} later adapted it to
\gls{instruction selection} in a method that optimizes code size for
one-\gls{register} \glspl{target machine}.
%
To prune the search space,
\citeauthor{Liao1995} perform \gls{pattern selection} in two iterations.
%
In the
first iteration, \glspl{pattern} are selected such that the \gls{program DAG} is
covered but the costs of necessary data transfers are ignored.
%
After this step
the \glspl{node} covered by the same \gls{pattern} are collapsed into single
\glspl{node}, and a second \gls{binate covering} problem is constructed to
minimize the costs of data transfers.
%
Although these two problems can be solved
simultaneously, \citeauthor{Liao1995} chose not to do so as the number of
necessary \glspl{implication clause} would become very large.
%
Recently,
\textcite{Cong2004} also applied \gls{binate covering} as part of generating
application-specific \glspl{instruction} for configurable processor
architectures.

\Gls{unate covering} was applied by \textcite{Clark2006} in generating
\gls{assembly code} for acyclic computation accelerators, which can be partially
customized in order to increase performance of the currently executed
\gls{program}.
%
Described in a paper from~2006, the \glspl{target machine} are
presumably homogeneous enough that \glspl{implication clause} are
unnecessary.
%
The work by \citeauthor{Clark2006} was later expanded by
\textcite{Hormati2007} to reduce the number of interconnects as well as
data-centered latencies in accelerator
designs.
%
\citeauthor{Martin2009}~\cite{Martin2009, Martin2012} also applied
\gls{unate covering} to solve a similar problem concerning reconfigurable
processor extensions, but combined the \gls{instruction selection} problem with
\gls{instruction scheduling} and solved both in tandem using a method called
\glsdesc{CP}---we will discuss this approach later in this chapter---which
they also applied for solving the \gls{pattern matching} problem.
%
Unlike in the
cases of \citeauthor{Clark2006} and \citeauthor{Hormati2007}, who solved their
\gls{unate covering} problems using heuristics, the \gls{assembly code}
generated by \citeauthor{Martin2009} is potentially \gls{optimal instr-sel}.


\section{Modeling Instruction Selection with IP}

As explained in \refAppendix{introduction}, performing \gls{instruction
  selection}, \gls{instruction scheduling}, or \gls{register allocation} in
isolation will typically always yield suboptimal \gls{assembly code}.
%
But since
each subproblem is already NP-complete on its own, attaining
\gls{integrated code-gen}[ \gls{code generation}]---where all these problems
are solved simultaneously---is an even more difficult problem.

These challenges notwithstanding, \textcite{Wilson1994} introduced in 1994 what
appears to be the first design that could be said to yield truly \gls{optimal
  instr-sel} \gls{assembly code}.
%
\citeauthor{Wilson1994} accomplished this by
using \gls{IP}, which is a method for solving combinatorial optimization
problems (sometimes \gls{IP} is also called \gls{ILP}).
%
In \gls{IP} a problem is
expressed using sets of \glspl{integer variable} and linear equations (see for
example~\cite{Wolsey1998} for an overview), and a solution to an \tIPmodel is an
assignment to all variables such that all equations are fulfilled.
%
In general,
solving an \tIPmodel is NP-complete, but extensive research in this field
has made many problem instances tractable.

In their seminal paper, \citeauthor{Wilson1994} describe that the \gls{pattern
  selection} problem can be expressed as the following linear inequality:
%
\begin{displaymath}
  \sum_{p \in P_n} x_p \leq 1, \quantsep \forall n \in N.
\end{displaymath}
%
This reads: for every \gls{node}~$n$ in the \gls{program
  DAG}~\mbox{$\mTupleTwo{N}{E}$}, at most one \gls{pattern}~$p$ from the
\gls{match set} involving~$n$ (represented by $P_n$) may be selected.
%
The
decision is represented by $x_p$, which is a Boolean \mbox{$0/1$}
variable\footnote{The more common constraint is that \emph{exactly one}
  \gls{pattern} must be selected, but in the design by \citeauthor{Wilson1994},
  \glspl{node} are allowed to become \gls{inactive nod} and thus need not be
  covered.}.
%
Similar linear equations can be formulated for modeling
\gls{instruction scheduling} and \gls{register allocation}---which
\citeauthor{Wilson1994} also included in their model---but these are out of
scope for this dissertation.
%
In fact, any constraint that can be formulated in this way
can be added to an existing \tIPmodel, making this approach a suitable \gls{code
  generation} method for targeting irregular architectures.
%
Furthermore, this is
the first design we have seen that could potentially support
\glspl{interdependent instruction} (although this was not the main focus of
\citeauthor{Wilson1994}).

Solving this monolithic \tIPmodel, however, typically requires considerably more
time compared to the previously discussed techniques of \gls{instruction
  selection}.
%
But the trade-off for longer compilation time is higher
code quality; \citeauthor{Wilson1994} reported that experiments showed that the
generated \gls{assembly code} was of comparable code quality to that of
manually optimized \gls{assembly code}.
%
In theory, \gls{optimal ass-code}
\gls{assembly code} can be generated, although this is in practice only feasible
for small enough \glspl{program}.
%
Another much-valued feature is the ability to
extend the model with additional constraints in order to support complicated
\glspl{target machine}, which cannot be properly handled by the conventional
designs as that typically violates assumptions made by the underlying
heuristics.


\subsection{Approaching Linear Solving Time with Horn Clauses}

\newcommand{\mCmd}[1]{\text{\smaller\sffamily\textbf{#1}}}

Although \tIPmodels are NP-complete to solve in general, it was discovered
that for a certain class of problem instances---namely those based on
\glspl{Horn clause}---an optimal solution can be found in linear
time~\cite{Hooker1988}.
%
A \gls{Horn clause} is a disjunctive Boolean formula
which contains at most one non-negated term.
%
This can also be phrased as a
logical statement that has at most one conclusion.
%
For example, the statement
\begin{displaymath}
  \mCmd{if}~\mT{p}[1]~\mCmd{and}~\mT{p}[2]~\mCmd{then}~\mT{p}[3]
\end{displaymath}
\noindent%
can be expressed as \mbox{$\mCompT{p}[1] \mOr \mCompT{p}[2] \mOr \mT{p}[3]$},
which is a \gls{Horn clause}, as only \mT{p}[3] is not negated.
%
This can then
easily be rewritten into the linear inequality
\begin{displaymath}
(1 - x_1) + (1 - x_2) + x_3 \ge 1,
\end{displaymath}
where $x_i$ is a Boolean variable corresponding to literal
\mT{p}[\larger$i$].
%
Moreover, statements that do not yield \glspl{Horn
  clause} in their current form can often be rewritten so that they do.
%
For
example,
\begin{displaymath}
  \mCmd{if}~\mT{a}~\mCmd{then}~\mT{b}~\mCmd{and}~\mT{c}
\end{displaymath}
\noindent%
can be expressed as \mbox{$\mCompT{a} \mOr \mT{b} \mOr \mT{c}$} and is thus not
a \gls{Horn clause} because it has more than one non-negated term.
%
But by
rewriting it into
\begin{displaymath}
  \begin{array}{c}
    \mCmd{if}~\mT{a}~\mCmd{then}~\mT{b}\\
    \mCmd{if}~\mT{a}~\mCmd{then}~\mT{c}
  \end{array}
\end{displaymath}
\noindent%
the statement can now be expressed as \mbox{$\mCompT{a} \mOr \mT{b}$} and
\mbox{$\mCompT{a} \mOr \mT{c}$}, which are two valid \glspl{Horn clause}.

\textcite{Gebotys1997a} exploited this property in 1997 by developing an
\tIPmodel for \gls{TMS320C2x}---a typical \gls{DSP} at the time---where many
of the target architecture, \gls{instruction selection}, and \gls{register
  allocation} constraints, and a part of the \gls{instruction scheduling}
constraints, are expressed as \glspl{Horn clause}.
%
Using only \glspl{Horn
  clause} may require a larger number of constraints than are otherwise needed,
but \citeauthor{Gebotys1997a} claims that the number is still manageable.
%
When
compared against a then-contemporary industrial \gls{DSP} \gls{compiler},
\citeauthor{Gebotys1997a} demonstrated that an implementation based on \gls{IP}
yielded a performance improvement mean of 44\% for a select set of functions,
while attaining reasonable compilation times.
%
However, the solving time
increased by orders of magnitude when \citeauthor{Gebotys1997a} augmented the
\tIPmodel with the complete set of constraints for \gls{instruction scheduling},
which cannot be expressed entirely as \glspl{Horn clause}.


\subsection{IP-Based Designs with Multi-output Instruction Support}

\citeauthor{Leupers1996}~\cite{Leupers1995, Leupers1996} expanded the work of
\citeauthor{Wilson1994}\unskip% Removes unwanted space
%
---whose design is restricted to \glspl{pattern tree}---by developing an
\gls{IP}-based \gls{instruction selector} which also supports
\glspl{multi-output instruction}.
%
In a paper from~1996, \citeauthor{Leupers1996}
describe a scheme where the \glspl{pattern DAG} of \glspl{multi-output
  instruction}---\citeauthor{Leupers1996} refer to these as
\tcomplexPatterns\unskip% Removes unwanted space
%
---are first decomposed into multiple \glspl{pattern tree} according to their
\glspl{RT}.
%
\glspl{RT}~are akin to \citeauthor{Fraser1979}'s
\glspl{RTL}~\cite{Fraser1979} (see \refSection{register-transfer-lists}), and
essentially mean that each observable effect gets its own \gls{pattern
  tree}.
%
Each individual \gls{RT} may in turn correspond to one or more
\glspl{instruction}, but unlike in \citeauthor{Fraser1979}'s design this is not
strictly required.

Assuming the \gls{program DAG} has already been \glsshort{undagging}[ged], each
\gls{expression tree} is first optimally covered using \gls{IBURG}.
%
The \glspl{RT}
are expressed as \glspl{rule} in an \gls{machine grammar} that has been
automatically generated from a \gls{machine description} written in \gls{MIMOLA}
(we will come back to this in
\refSection{modeling-entire-target-machines}).
%
Once \glspl{RT} have been
selected, the \gls{expression tree} is reduced to a \gls{tree} of \glspl{super
  node}, where each \gls{super node} represents a set of \glspl{node} covered by
some \gls{RT} that have been collapsed into a single \gls{node}.
%
Since
\glsshort{multi-output instruction} and \glspl{disjoint-output instruction}
implement more than one~\gls{RT}, the goal is now to cover the \gls{super node}
\gls{graph} using the \glspl{pattern} which are formed when the
\glspl{instruction} are modeled as~\glspl{RT}.
%
\citeauthor{Leupers1996}
addressed this problem by applying a modified version of the \tIPmodel by
\citeauthor{Wilson1994}.

But because the step of selecting \glspl{RT} to cover the \gls{expression tree} is
separate from the step which implements them with \glspl{instruction}, the
generated \gls{assembly code} is not necessarily \gls{optimal ass-code} for the
whole \gls{expression tree}.
%
To achieve this property, the covering of \glspl{RT}
and selection of \glspl{instruction} must be done in tandem.


\subsection{IP-Based Designs with Disjoint-output Instruction Support}

\textcite{Leupers2000b} later made a more direct extension of the \tIPmodel by
\citeauthor{Wilson1994} in order to support \glspl{SIMD instruction}, which
belong to the class of \glspl{disjoint-output instruction}.
%
Described in a paper
from~2000, \citeauthor{Leupers2000b}'s design assumes every \gls{SIMD
  instruction} performs two operations, each of which takes a disjoint set of
input operands.
%
This is collectively called a \gls{SIMD pair}, and
\citeauthor{Leupers2000b} then extended the \tIPmodel with linear equations for
combining \glspl{SIMD pair} into \glspl{SIMD instruction} and defined the
objective function so as to maximize the use of \glspl{SIMD instruction}.

In the paper, \citeauthor{Leupers2000b} reports experiments where the use of
\glspl{SIMD instruction} reduced code size by up to 75\% for the selected test
cases and \glspl{target machine}.
%
But since this technique assumes that each
individual operation of the \glspl{SIMD instruction} is expressed as a single
\gls{node} in the \gls{program DAG}, it is unclear whether the method can be
extended to more complex \glspl{SIMD instruction}, and whether it scales to
larger \glspl{program}.
%
\textcite{Tanaka2003} later expanded
\citeauthor{Leupers2000b}'s work for selecting \glspl{SIMD instruction} while
also taking the cost of data transfers into account by introducing auxiliary
transfer \glspl{node} and transfer \glspl{pattern} into the \gls{program DAG}.


\subsection{Modeling the Pattern Matching Problem with IP}

In 2006, \textcite{Bednarski2006} developed an \gls{integrated code-gen}
\gls{code generation} design where both \gls{pattern matching} and \gls{pattern
  selection} are solved using \glsdesc{IP}.
%
The scheme---which later was
applied by \textcite{Eriksson2008}, and is also described in an article by
\textcite{Eriksson2012}\unskip% Removes unwanted space
%
---is an extension of their earlier work where \gls{instruction selection} had
previously more or less been ignored (see~\cite{Kessler2001, Kessler2002}).

In broad outline, the \tIPmodel assumes that a sufficient number of
\glspl{match} has been generated for a given \gls{program DAG}~$G$.
%
This is done
using a \gls{pattern matching} heuristic that computes an upper bound.
%
For each
match~$m$, the \tIPmodel contains \glspl{integer variable} that:
\begin{itemize}
  \item map a \gls{pattern} \gls{node} in $m$ to a \gls{node} in $G$;
  \item map a \gls{pattern} \gls{edge} in $m$ to an \gls{edge} in $G$; and
  \item decide whether $m$ is used in the solution.
%
Remember that we may have an
    excess of \glspl{match}, so they cannot all be selected.
\end{itemize}
Hence, in addition to the typical linear equations we have seen previously for
enforcing coverage, this \tIPmodel also includes equations to ensure that the
selected \glspl{match} are valid \glspl{match}.

Implemented in a framework called \gls{OPTIMIST}, \citeauthor{Bednarski2006}
compared their \tIPmodel against another \gls{integrated code-gen} \gls{code
  generation} design based on \glsdesc{DP}, which was developed by the same
authors (see \cite{Kessler2001}) and has nothing to do with the conventional
\tDPalgorithm by \textcite{Aho1989}).
%
\citeauthor{Bednarski2006} found that
\gls{OPTIMIST} substantially reduced compilation time while retaining
code quality, but for several test cases---the largest \gls{program DAG}
containing only 33~\glspl{node}---\gls{OPTIMIST} failed to generate any
\gls{assembly code} whatsoever within the set time limit.
%
One reasonable cause
could be that the \tIPmodel also attempts to solve \gls{pattern matching}---a
problem which we have seen can be solved externally---and thus further
exacerbates an already computationally difficult problem.


\section{Modeling Instruction Selection with CP}

Although \glsdesc{IP} allows auxiliary constraints to be included into the
\tIPmodel, they may be cumbersome to express as linear equations.
%
This issue can
be alleviated by using \gls{CP}, which is another method for solving
combinatorial optimization problems (see for example~\cite{Rossi2006} for an
overview) but has more flexible modeling capabilities compared to \gls{IP}.
%
In
brief terms, a \tCPmodel consists of a set of \glspl{domain variable}, each of
which has an initial set of values that it can assume, and a set of constraints
that essentially specify the valid combinations of values for a subset of the
\glspl{domain variable}.
%
A solution to the \tCPmodel is thus an assignment for
all \glspl{domain variable}---meaning that each \gls{domain variable} takes
exactly one value---such that all constraints are fulfilled.

In 1990, \textcite{Bashford1999} pioneered the use of \glsdesc{CP} in \gls{code
  generation} by developing a \tCPmodel for \gls{integrated code-gen} \gls{code
  generation} that targets \glspl{DSP} with highly irregular architectures (the
work is also discussed in~\cite{Leupers2000b, Leupers2000c}).
%
Like
\citeauthor{Leupers1996}'s \gls{IP}-based design, \citeauthor{Bashford1999}'s
first breaks down the \gls{instruction set} of the \gls{target machine} into a
set of \glspl{RT} which are used to cover individual \glspl{node} in the
\gls{program DAG}.
%
As each \gls{RT} concerns specific \glspl{register} on the
\gls{target machine}, the covering problem essentially also incorporates
\gls{register allocation}.
%
The goal is then to minimize the cost of covering by
combining multiple \glspl{RT} that can be executed in parallel as part of some
\gls{instruction}.

For each \gls{node} in the \gls{program DAG} a \gls{FRT} is introduced, which
basically embodies all \glspl{RT} that match a particular \gls{node} and is
formally defined as the following tuple:
\begin{displaymath}
  \langle \mathit{Op}, D, [U_1, \ldots, U_n], F, C, T, \mathit{CS} \rangle.
\end{displaymath}
$\mathit{Op}$ is the operation of the \gls{node}.
%
$\mDVar{D}$ and
\mbox{$\mDVar{U}[1], \ldots, \mDVar{U}[n]$} are \glspl{domain variable}
representing the \glspl{storage location} of the result and the respective
inputs to the operation.
%
These are typically the \glspl{register} that can be
used for the operation, but also include \gls{virtual st-loc}[ \glspl{storage
    location}] which convey that the value is produced as an intermediate result
in a chain of operations (for example, the multiplication term in a
multiply-accumulate instruction is such a result).
%
Then, for every pair of
operations that are \gls{adjacent} in the \gls{program DAG}, a set of
constraints is added to ensure that there exists a valid data transfer between
the \glspl{storage location} of $\mDVar{D}$ and $\mDVar{U}[i]$ if these are
assigned to different \glspl{register}, or that both are identical if one is a
\gls{virtual st-loc} \gls{storage location}.
%
$\mDVar{F}$,~$\mDVar{C}$, and
$\mDVar{T}$ are all \glspl{domain variable} which collectively represent the
\gls{ERI} that specifies at which functional unit the operation will be
executed~($\mDVar{F}$); at what cost~($\mDVar{C}$), which is the number of
execution cycles; and by which \gls{instruction} type~($\mDVar{T}$).
%
A
combination of a functional unit and an \gls{instruction} type is later mapped
to a particular \gls{instruction}.
%
Multiple \glspl{RT} can be combined into the
same \gls{instruction} when the destination of the result is a \gls{virtual
  st-loc} \gls{storage location} by setting $\mDVar{C} = 0$ and letting the last
\gls{node} in the operation chain account for the required number of execution
cycles.
%
The last entity, $\mathit{CS}$, is the set of constraints for defining
the range of values for the \glspl{domain variable} and the dependencies between
$\mDVar{D}$ and~$\mDVar{U}[i]$, as well as other auxiliary constraints that may
be required for the \gls{target machine}.
%
For example, if the set of \glspl{RT}
matching a \gls{node} consists of \mbox{$\mSet{ \mbox{\mT{r}[c] = \mT{r}[a] +
      \mT{r}[b]}, \mbox{\mT{r}[a] = \mT{r}[c] + \mT{r}[b]} }$}, then the
corresponding \gls{FRT} becomes
\begin{displaymath}
  \left\langle
  +, \mDVar{D}, [\mDVar{U}[1], \mDVar{U}[2]], \mDVar{F}, \mDVar{C}, \mDVar{T},
  \mSet{ \mDVar{D} \in \mSet{ \mT{r}[c], \mT{r}[a] },
         \mDVar{U}[1] \in \mSet{ \mT{r}[a], \mT{r}[c] },
         \mDVar{U}[2] = \mT{r}[b],
         \mDVar{D} = \mT{r}[c] \implies \mDVar{U}[1] = \mT{r}[a]
  }
  \right\rangle.
\end{displaymath}
For brevity I have omitted several details such as the constraints concerning
the~\gls{ERI}.

This \tCPmodel is then solved to optimality using a \tCPsolver.
%
But since
\gls{optimal.ps} covering using \glspl{FRT} is NP-complete,
\citeauthor{Bashford1999} applied heuristics to curb the complexity by splitting
the \gls{program DAG} into smaller pieces along \glspl{edge} where intermediate
results are shared, and then performing \gls{instruction selection} on each
\gls{expression tree} in isolation.

Although the constraints in \citeauthor{Bashford1999}'s \tCPmodel appear to be
limited to involving only a single~\gls{FRT} at a time---thus hindering support
for \glspl{interdependent instruction}---\glsdesc{CP} in general seems like a
promising tool for performing \gls{instruction selection}.
%
As with \glsdesc{IP},
\glsdesc{CP} facilitates \gls{integrated code-gen} and potentially \gls{optimal
  ass-code} \gls{code generation}.
%
In addition, it allows additional
restrictions of the \gls{target machine} to be included in the \tCPmodel, but
without the need of expressing these constraints as linear equations.
%
At the
time of writing, however, the existing techniques for solving \tIPmodels are
more mature than those for solving \tCPmodels, which potentially makes
\glsdesc{IP} a more powerful method than \glsdesc{CP} for solving
\gls{instruction selection}.
%
Having said that, it is still unclear which
technique of combinatorial optimization---which also includes \glsshort{SAT
  problem} and other methods---is best suited for \gls{instruction selection}
(and code generation in general).


\subsection{Taking Advantage of Global Constraints}

So far we have discussed several techniques that apply \glsdesc{CP} for solving
the problems of \gls{pattern matching} and \gls{pattern selection}---namely
those by \citeauthor{Bashford1999} and \citeauthor{Martin2009}.
%
Recently,
\textcite{Beg2013} introduced another \tCPmodel for \gls{instruction selection}
as well as new methods for improving solving.
%
For example, in order to reduce
the search space, \citeauthor{Beg2013} applied conventional \gls{DP}-based
techniques to compute an upper bound on the cost.
%
However, the \tCPmodel mainly
deals with the problem of \gls{pattern matching} rather than \gls{pattern
  selection}.
%
Moreover, \citeauthor{Beg2013} noticed only a negligible
improvement (less than 1\%) in code quality compared to \gls{LLVM}, mainly
because the \glspl{target machine} (\gls{MIPS} and \gls{ARM}) were simple enough
that greedy heuristics generate near-\gls{optimal ass-code} \gls{assembly code}.
In addition, the \glspl{program DAG} of the benchmark \glspl{program} were
fairly \gls{tree}-shaped~\cite{VanBeek2014}, for which \gls{optimal.ps}
code can be generated in linear time.
%
In any case, none of these designs take
advantage of a key feature of \glsdesc{CP}, which is the use of \glspl{global
  constraint}.
%
A \gls{global constraint} is a restriction that is enforced
simultaneously over multiple \glspl{domain variable} and results in more search
space pruning than if it had been expressed using multiple constraints over only
a subset of the variables at a time (see for example~\cite{Beldiceanu2014} for
an overview).

Hence, when \textcite{Floch2010} in 2010 adapted the \tCPmodel by
\citeauthor{Martin2009} to support processors with reconfigurable cell fabric,
they replaced the method of \gls{pattern selection} with constraints that are
radically different from those incurred by \gls{unate covering}.
%
In addition,
unlike in the case of \citeauthor{Bashford1999}, the design by
\citeauthor{Floch2010} applies the more direct form of \gls{pattern matching}
instead of first breaking down the \glspl{pattern} into \glspl{RT} and then
selecting \glspl{instruction} that combine as many \glspl{RT} as possible.

As described in their 2010~paper, \citeauthor{Floch2010} use the \gls{global
  cardinality constraint}\footnote{It is also possible to enforce \gls{pattern
    selection} through a global set covering developed by
  \textcite{Mouthuy2007}, but I have not seen any implementation do so.} to
enforce the requirement that every \gls{node} in the \gls{program DAG} must be
covered by exactly one \gls{pattern}.
%
This constraint, which we will refer to as
\mbox{$\mConstraintUse{Count}{\mDVar{i}, \mDVar{var}, \mDVar{val}}$}, enforces
that exactly $\mDVar{val}$ number of \glspl{domain variable} from the
set~$\mDVar{var}$ assume the value~$\mDVar{i}$, where $\mDVar{val}$ can either
be a fixed value or represent another \gls{domain variable}.
%
Let us assume that
every \gls{node} in the \gls{program DAG} has an associated \gls{match set}
containing all the \glspl{pattern} that may cover that \gls{node}, and that each
\gls{match}~$m$ appearing in that \gls{match set} has been assigned a unique
integer value~$i_m$ (if $m$ appears in multiple \glspl{match set}, it is still
given the same value).
%
We introduce a \gls{domain variable}~$\mathit{match}_n$
for each \gls{node}~$n$ in the \gls{program DAG} to represent the \gls{match}
selected to cover~$n$.
%
For each \gls{match}~$m$, we also introduce a \gls{domain
  variable} \mbox{$\mDVar{nodecount}[m] \in \mSet{0, \mSize{m}}$}, where
\mbox{$\mSize{m}$} is the number of \gls{pattern} \glspl{node} in~$m$.
%
We also
define \mbox{$\mDVar{mset}[m] = \bigcup_{n \in \mFunctionName{nodes}(m)}
  \mathit{match}_n$} as the set of $\mathit{match}_n$ variables in which
\gls{match}~$m$ may appear, where $\mFunctionUse{nodes}{m}$ is the set of
\glspl{node} matched by~$m$.
%
With this we can express the constraint that every
\gls{node} in the \gls{program DAG} must be covered exactly once as
%
\begin{displaymath}
  \mConstraintUse{Count}{\mDVar{i}[m], \mDVar{mset}[m], \mDVar{nodecount}[m]},
  \quantsep
  \forall m \in M,
\end{displaymath}
%
where $M$ is the total set of \glspl{match}.
%
This may appear convoluted at first
glance, but it is actually rather simple.
%
$\mDVar{mset}[m]$ essentially provides
the \glspl{node} in the \gls{program DAG} that may be covered by
\gls{match}~$m$, $\mDVar{nodecount}[m]$ provides the number of \glspl{node}
covered by~$m$, and $\mConstraintName{Count}$ ensures that the relation between
the two holds.
%
But since the domain of $\mDVar{nodecount}[m]$ is initially
restricted to only two values---zero and the size of the \gls{pattern}---it
must be so that either \emph{no} \glspl{node} are covered by~$m$, or \emph{all}
\glspl{node} are covered by~$m$.
%
To identify which \glspl{match} have been
selected, we simply check whether \mbox{$\mDVar{nodecount}[m] \neq 0$} for every
\gls{pattern}~$m$.
%
Since a \gls{domain variable} cannot be assigned more than
one value, each \gls{node} can only be covered by exactly one \gls{match}
(remember that the $\mathit{match}_n$ variable for \gls{node}~$n$ can appear in
multiple $\mDVar{mset}[m]$ sets).
%
Hence this constraint is more restrictive than
that of \gls{unate covering}, which results in more propagation and thus reduces
the search space.

This \tCPmodel was also further extended by \combinedTextcite{Arslan2013,
  Arslan2014} to accommodate \gls{VLIW} architectures and \glspl{disjoint-output
  instruction}.
%
First, every \glspl{disjoint-output instruction} is split into
multiple \glspl{subinstruction}, each modeled by a disjoint \gls{pattern}.
%
A
generic \gls{subgraph isomorphism} algorithm is used to find all
\glspl{match set}, and \gls{pattern selection} is then modeled as an instance of
the \tCPmodel with the additional constraints to schedule the
\glspl{subinstruction} such that they can be replaced by the original
\gls{disjoint-output instruction}.
%
\citeauthor{Arslan2013}'s design therefore
differs from the previous techniques that we have seen before, where
\glspl{match} of \tpartialPatterns are recombined into \glspl{match} of
\tcomplexPatterns prior to \gls{pattern selection} (see for example
\textcite{Scharwaechter2007}, \textcite{Ahn2009},
\citeauthor{Arnold1999b}~\cite{Arnold1999a, Arnold1999b, Arnold2001}), as it
allows these two problems to be solved in tandem.
%
The design is also capable of
accepting multiple, disconnected \glspl{program DAG} as a single input.

However, a limitation inherent to the \tCPmodels applied by
\citeauthor{Martin2009}, \citeauthor{Floch2010}, and \citeauthor{Arslan2013} is
that they do not model the necessary data transfers between different
\glspl{register class}.
%
This in turn means that the cost model is only accurate
for \glspl{target machine} equipped with a homogeneous \gls{register}
architecture, which could compromise code quality for more complicated
\glspl{target machine}.


\section{Other DAG-Based Approaches}

\subsection{More Genetic Algorithms}

Seemingly independently from the earlier work by \textcite{Shu1996} (discussed
in \refAppendix{tree-covering}), \combinedTextcite{Lorenz2001, Lorenz2004}
introduced in 2001 another technique where \glsdesc{GA}s are applied to
\gls{code generation}.
%
But unlike the design by \citeauthor{Shu1996}, the one by
\citeauthor{Lorenz2001} takes \glspl{program DAG} instead of \glspl{tree} as
input and also incorporates \gls{instruction scheduling} and \gls{register
  allocation}.
%
\citeauthor{Lorenz2001} recognized that contemporary
\glspl{compiler} struggled with generating efficient \gls{assembly code} for
\glspl{DSP} equipped with very few \glspl{register} and typically always spill
the results of common subexpressions to memory and reload them when needed.
Compared to \gls{optimal ass-code} \gls{assembly code}, this may incur more
memory accesses than needed.

The design by \citeauthor{Lorenz2001} is basically an iterative process.
%
First,
the operations within a \gls{block} are scheduled using \gls{list scheduling},
which is a traditional method of scheduling (see for
example~\cite{Rau1993}).
%
For every scheduled operation, a \gls{gene} is
formulated to encode all the possible decisions to take in order to solve the
problems of \gls{instruction selection} and \gls{register allocation}.
%
These
decisions are then taken over multiple steps using standard \gls{GA}~operations,
where the values are selected probabilistically.
%
In each step the \gls{gene} is
mutated and crossed over in order to produce new, hopefully better \glspl{gene},
and a \gls{fitness function} is applied to evaluate each \gls{gene} in terms of
expected execution time.
%
After a certain number of generations, the process
stops and the best \gls{gene} is selected.
%
Certain steps are also followed by a
routine based on \glsdesc{CP} that prunes the search space for the subsequent
decisions by removing values which will never appear in any valid
\gls{gene}.
%
Although every \gls{gene} represents a single \gls{node} in the
\gls{program DAG}, \tcomplexPatterns can still be supported through an
additional variable for selecting the \gls{instruction} type for the
\gls{node}.
%
If \glspl{node} with the same \gls{instruction} type have been
scheduled to be executed on the same cycle, then they can be implemented using
the same \gls{instruction} during \gls{assembly code} emission.

\citeauthor{Lorenz2001} originally developed this technique in order to reduce
power usage of \gls{assembly code} generated for constrained \glspl{DSP}, and
later extended the design to also incorporate \gls{code compaction} and
\gls{address generation}.
%
Experiments indicate that the technique for a selected
set of test cases resulted in energy savings of 18 to 36\% compared to a
traditional \gls{tree covering}-based \gls{compiler}, and reduced execution time
by up to 51\%.
%
According to \citeauthor{Lorenz2001}, the major contribution to
this reduction is due to improved usage of \glspl{register} for common
subexpression values, which in turn leads to less use of power-hungry and
long-executing memory operations.
%
But due to the probabilistic nature of
\gls{GA}, optimality cannot be guaranteed, making it unclear how this technique
would fare against other \gls{DAG covering}-based designs which allow a more
exhaustive exploration of the search space.


\subsection{Extending Trellis Diagrams to DAGs}
\labelSection{trellis-diagrams-dags}

In 1998, \citeauthor{Hanono1998}~\cite{Hanono1998, Hanono1999} proposed a
technique that is similar to \citeauthor{Wess1992}'s use of \glspl{trellis
  diagram}, which we discussed in \refAppendix{tree-covering}.
%
Implemented in a
system called \gls{AVIV}, \citeauthor{Hanono1998}'s \gls{instruction selector}
takes a \gls{program DAG} as input and duplicates each operation \gls{node}
according to the number of functional units in the \gls{target machine} on which
that operation can run.
%
Special \typesetNewTerm{\glsshort{split node}} and
\glspl{transfer node} are inserted before and after each duplicated operation
\gls{node} to allows the data flow to diverge and then reconverge before passing
to the next operation \gls{node} in the \gls{program DAG}.
%
The use of
\glspl{transfer node} also allow the cost of transferring data from one
functional unit to another to be taken into account.
%
Similarly to the
\gls{trellis diagram}, \gls{instruction selection} is thus transformed to
finding a path from the \gls{leaf} \glspl{node} in the \gls{program DAG} to its
\gls{root} \gls{node}.
%
But differently from the \gls{optimal.ps},
\mbox{\gls{DP}-oriented} design of \citeauthor{Wess1992},
\citeauthor{Hanono1998} applied a greedy heuristic that starts from the
\gls{root} \gls{node} and makes it way towards the \glspl{leaf}.

Unfortunately, as in \citeauthor{Wess1992}'s design, this technique assumes a
\mbox{1-to-1} mapping between the \glspl{node} in the \gls{program DAG} and the
\glspl{instruction} in order to generate efficient \gls{assembly code}.
%
In fact,
the main purpose behind \gls{AVIV} was to generate efficient \gls{assembly code}
for \gls{VLIW} architectures, where the focus is on executing as many
\glspl{instruction} as possible in parallel.


\subsection{Hardware Modeling Techniques}
\labelSection{modeling-entire-target-machines}

In 1984, \textcite{Marwedel1984} developed a retargetable system called
\gls{MSS} for \gls{microcode generation}\footnote{\Gls{microcode} is essentially
  the hardware language that processors use internally for executing
  \glspl{instruction}.
%
For example, \gls{microcode} controls how the
  \glspl{register} and program counter should be updated for a given
  \gls{instruction}.}, where a \gls{machine description} written in
\gls{MIMOLA}~\cite{Zimmermann1979} is used for modeling the entire data path of
the processor, instead of just the \gls{instruction set} as we have commonly
seen.
%
This is commonly used for \glspl{DSP} where the processor is small but
highly irregular.
%
Although \gls{MSS} consists of several tools, we will
concentrate on the \gls{MSSQ} \gls{compiler}, as its purpose is most aligned
with \gls{instruction selection}.
%
\gls{MSSQ} was developed by
\textcite{Leupers1998} as a faster version of \gls{MSSC}~\cite{Nowak1989}, which
in turn is an extension of the \gls{tree}-based \gls{MSSV}~\cite{Marwedel1993}.

%\begin{figure}[b]
%  \centering%
%  \input{\figurePath/dag-covering/co-graph-example}
%  \figCaption[The \glsentrytext{CO graph} of a simple processor]%
%    {The \glsentrytext{CO graph} of a simple processor containing an arithmetic
%     logic unit,
%     two data registers, a program counter, and a control store}[Nowak1989]
%  \labelFigure{co-graph-example}
%\end{figure}

The \gls{MIMOLA} specification contains the processor \glspl{register} as well
as all the operations that can be performed on these \glspl{register} within a
single cycle.
%
From this specification, a hardware~\gls{DAG} called the \gls{CO
  graph} is automatically derived.
%
An example is given in
\refFigure{co-graph-example}.
%
A \gls{pattern matcher} then attempts to find
\glspl{subgraph} within the \gls{CO graph} to cover the \glspl{expression tree}.
%
Because the \gls{CO graph} contains explicit \glspl{node} for every
\gls{register}, a \gls{match} found on this \gls{graph}---called a
\gls{version}---is also an assignment of \gls{program} variables (and
\glspl{temporary}) to \glspl{register}.
%
If a \gls{match} cannot be found (due to
a lack of \glspl{register}), the \gls{expression tree} will be rewritten by
splitting assignments and inserting additional \glspl{temporary}.
%
The process
then backtracks and repeats in a recursive fashion until the entire \gls{expression tree} is covered.
%
A subsequent process then selects a specific \gls{version}
from each \gls{match set} and tries to schedule them so that they can be combined
into \glspl{bundle} for parallel execution.

Although \gls{microcode generation} is at a lower hardware level than
\gls{assembly code} generation---which is usually what we refer to with
\gls{instruction selection}---we see several similarities between the problems
that must be solved in each, and that is why it is included in this dissertation
(further examples include~\cite{Balakrishnan1986, Mahmood1990,
  Langevin1993}).
%
In \refAppendix{graph-covering} we will see another design that
also models the entire processor but applies a more powerful technique.


\section{Summary}

In this chapter we have investigated several methods that rely on the
\gls{principle} of \gls{DAG covering}, which is a more general form of \gls{tree
  covering}.
%
Operating on \glspl{DAG} instead of \glspl{tree} has several
advantages.
%
Most importantly, common subexpressions can be directly modeled, and
a larger set of \glspl{instruction}---including \glsshort{multi-output
  instruction} and \glspl{disjoint-output instruction}---can be supported and
exploited during \gls{instruction selection}, leading to improved performance
and reduced code size.
%
Consequently, techniques based on \gls{DAG covering} are
today one of the most widely applied methods for \gls{instruction selection} in
modern \glspl{compiler}.

The ultimate cost of transitioning from \glspl{tree} to \glspl{DAG}, however, is
that \gls{optimal.ps} \gls{pattern selection} can no longer be achieved in
linear time as it is NP-complete.
%
At the same time, \glspl{DAG} are not
expressive enough to allow the proper modeling of all aspects featured in the
\glspl{program} and \glspl{instruction}.
%
For example, statements such as
\mbox{\techTerm{for} loops} incur \glspl{loop} in the \gls{graph} representing
the \gls{program}, restricting \gls{DAG covering} to the scope of \glspl{block}
and excluding the modeling of \glspl{inter-block instruction}.
%
Another
disadvantage is that optimization opportunities for storing \gls{program}
variables and \glspl{temporary} in different forms and at different locations
across the \gls{function} are forfeited.

In the next chapter we will discuss the last and most general \gls{principle} of
\gls{instruction selection}, which addresses some of the aforementioned
deficiencies of \gls{DAG covering}.
